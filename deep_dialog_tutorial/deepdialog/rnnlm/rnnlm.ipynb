{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# RNNLM\n",
    "Recurrent Neural Network Language Model\n",
    "RNN による言語モデルです。\n",
    "文章の集団を学習させることで、それっぽい文章を生成できます。\n",
    "\n",
    "これが発展して Seq2Seq のデコーダー部分になっていきます。"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "current dir: /home/harumitsu.nobuta/git/deep_dialog_tutorial\n"
     ]
    }
   ],
   "source": [
    "# カレントディレクトリをリポジトリ直下にするおまじない\n",
    "import os\n",
    "while os.getcwd().split('/')[-1] != 'deep_dialog_tutorial': os.chdir('..')\n",
    "print('current dir:', os.getcwd())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "import tensorflow as tf\n",
    "from tensorflow.python.layers import core as layers_core\n",
    "import numpy as np\n",
    "import os\n",
    "import random\n",
    "import collections\n",
    "from tqdm import tqdm"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Create Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "hidden_dim = 1024\n",
    "embedding_dim = 256\n",
    "vocab_size = 1000"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 入出力部分\n",
    "in_ph = tf.placeholder(tf.int32, shape=[None, None], name='in_ph')\n",
    "out_ph = tf.placeholder(tf.int32, shape=[None, None], name='out_ph')\n",
    "len_ph = tf.placeholder(tf.int32, shape=[None], name='len_ph')\n",
    "gen_start_token_ph = tf.placeholder(tf.int32, shape=[], name='gen_start_token_ph')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "def debug(ops):\n",
    "    '''与えられた計算ノードの値を表示します。'''\n",
    "    with tf.Session() as sess:\n",
    "        sess.run(tf.global_variables_initializer())\n",
    "        result = sess.run(ops, {\n",
    "            in_ph: [[30, 40, 50], [160, 170, 180]],\n",
    "            out_ph:[[40, 50, 60], [170, 180, 190]],\n",
    "            len_ph:[3, 3]\n",
    "        })\n",
    "        print('## {}\\nshape: {}'.format(ops.name, ops.shape))\n",
    "        print(result)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "## embedding_lookup/Identity:0\n",
      "shape: (?, ?, 256)\n",
      "[[[-0.20361237 -0.11308474 -0.41880432 ...  2.7252874  -1.0681399\n",
      "    1.3086659 ]\n",
      "  [-0.3352617  -1.0744342   0.9656708  ...  1.1087787   1.8505251\n",
      "    0.02086403]\n",
      "  [-0.48931482  1.2667885   0.58199185 ...  0.02560114  0.500132\n",
      "   -3.2564793 ]]\n",
      "\n",
      " [[-0.12822877 -0.22769526  1.352034   ...  1.9360523  -0.34742078\n",
      "   -0.40487522]\n",
      "  [-2.3179045   0.57485205 -0.754861   ... -1.3065025   1.2339923\n",
      "    0.01515338]\n",
      "  [ 0.6592995  -0.6290501  -0.36402264 ...  0.42282453  0.3705973\n",
      "    0.5638999 ]]]\n"
     ]
    }
   ],
   "source": [
    "# embeddings - 文字の ID から分散表現のベクトルに変換します。\n",
    "# データは [batch_size, sentence_len, embedding_dim] の形になります。\n",
    "embeddings = tf.Variable(tf.random_normal([vocab_size, embedding_dim], stddev=1), name='embeddings', dtype=tf.float32)\n",
    "in_embedded = tf.nn.embedding_lookup(embeddings, in_ph)\n",
    "debug(in_embedded)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "## output_layer/Tensordot:0\n",
      "shape: (?, ?, 1000)\n",
      "[[[ 0.02734021  0.11152687 -0.12108772 ... -0.02212429 -0.09032315\n",
      "    0.03886116]\n",
      "  [ 0.15023798  0.12774038  0.01580057 ...  0.13109946 -0.19433439\n",
      "    0.08362214]\n",
      "  [ 0.11481191  0.05790965 -0.00183003 ...  0.05068074 -0.42197746\n",
      "    0.042301  ]]\n",
      "\n",
      " [[ 0.17572726  0.10455676 -0.05352221 ...  0.10981397 -0.15552926\n",
      "   -0.07315858]\n",
      "  [ 0.07044679 -0.0157798   0.10679971 ...  0.0757293  -0.04247508\n",
      "    0.08267353]\n",
      "  [-0.11602538 -0.07365637 -0.05860609 ...  0.08297199 -0.10835387\n",
      "    0.24690573]]]\n"
     ]
    }
   ],
   "source": [
    "# RNN 部分\n",
    "cell = tf.nn.rnn_cell.GRUCell(hidden_dim, kernel_initializer=tf.orthogonal_initializer)\n",
    "rnn_out, final_state = tf.nn.dynamic_rnn(\n",
    "    cell=cell,\n",
    "    inputs=in_embedded,\n",
    "    sequence_length=len_ph,\n",
    "    dtype=tf.float32,\n",
    "    scope='rnn',\n",
    ")\n",
    "# 隠れ層から全結合をかませて、各単語の生成確率っぽい値にする。\n",
    "# （i番目のニューロンの出力が id: i の単語の生成確率っぽいものになる）\n",
    "output_layer = layers_core.Dense(vocab_size, use_bias=False, name='output_layer')\n",
    "onehot_logits = output_layer.apply(rnn_out)\n",
    "debug(onehot_logits)\n",
    "output_ids_op = tf.argmax(onehot_logits, -1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "## loss:0\n",
      "shape: ()\n",
      "6.907025\n"
     ]
    }
   ],
   "source": [
    "cross_entropy = tf.nn.sparse_softmax_cross_entropy_with_logits(\n",
    "    labels=out_ph,\n",
    "    logits=onehot_logits,\n",
    ")\n",
    "loss_op = tf.reduce_mean(cross_entropy, name='loss')\n",
    "debug(loss_op)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 生成時用の RNN\n",
    "beam_width = 20\n",
    "gen_max_len = 500\n",
    "start_tokens = tf.ones([1], tf.int32) * gen_start_token_ph  # 生成時の batch_size は1\n",
    "\n",
    "decoder = tf.contrib.seq2seq.BeamSearchDecoder(\n",
    "    cell=cell,\n",
    "    embedding=embeddings,\n",
    "    start_tokens=start_tokens,  \n",
    "    end_token=0,  # dummy\n",
    "    initial_state=cell.zero_state(beam_width, tf.float32),\n",
    "    beam_width=beam_width,\n",
    "    output_layer=output_layer,\n",
    ")\n",
    "\n",
    "beam_decoder_output = tf.contrib.seq2seq.dynamic_decode(\n",
    "    decoder=decoder,\n",
    "    maximum_iterations=500,\n",
    "    scope='generator_decode'\n",
    ")[0]\n",
    "generate_op = beam_decoder_output.predicted_ids"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Load and Convert Data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "sentence_len = 50\n",
    "batch_size = 512\n",
    "data_path = 'data/natsume.txt'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Tokenizer:\n",
    "    def __init__(self, vocab):\n",
    "        self.vocab = vocab\n",
    "        self.rev_dict = {c: i for i, c in enumerate(vocab)}\n",
    "        self.pad = 0\n",
    "        self.bos = 1\n",
    "        self.eos = 2\n",
    "        self.unk = 3\n",
    "    \n",
    "    @classmethod\n",
    "    def from_text(cls, text):\n",
    "        char_freq_tuples = collections.Counter(text).most_common(vocab_size - 4)\n",
    "        vocab, _ = zip(*char_freq_tuples)\n",
    "        vocab = ['<pad>', '<bos>', '<eos>', '<unk>'] + list(vocab)\n",
    "        return cls(vocab)\n",
    "\n",
    "    @property\n",
    "    def vocab_size(self):\n",
    "        return len(self.vocab_size)\n",
    "    \n",
    "    def text2id(self, text):\n",
    "        return [self.rev_dict[c] if c in self.rev_dict else self.unk for c in text]\n",
    "\n",
    "    def id2text(self, ids):\n",
    "        return ''.join(self.vocab[i] for i in ids)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "with open(data_path) as f:\n",
    "    text = f.read().replace('\\n', '')\n",
    "\n",
    "tokenizer = Tokenizer.from_text(text)\n",
    "ids = tokenizer.text2id(text)\n",
    "\n",
    "def split_ndlist(ndlist, size):\n",
    "    return [np.array(ndlist[i - size:i]) for i in range(size, len(ndlist) + 1, size)]\n",
    "\n",
    "# (1文字目, 2文字目), (2文字目, 3文字目), ... というペアを作る\n",
    "# ある時刻の入力に対しその次時刻の出力を学習させるため\n",
    "in_sequence_list = split_ndlist(ids[:-1], size=sentence_len)\n",
    "out_sequence_list = split_ndlist(ids[1:], size=sentence_len)\n",
    "\n",
    "in_batch_list = split_ndlist(in_sequence_list, batch_size)\n",
    "out_batch_list = split_ndlist(out_sequence_list, batch_size)\n",
    "\n",
    "# batch_size 個ごとに切り分け\n",
    "batch_list = [\n",
    "    {\n",
    "        'in': in_batch,\n",
    "        'out': out_batch,\n",
    "        'len': np.array([len(seq) for seq in in_batch]),\n",
    "    }\n",
    "    for in_batch, out_batch\n",
    "    in zip(in_batch_list, out_batch_list)\n",
    "]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[33, 27, 8, 51, 14, 3]\n",
      "こんにちは<unk>\n",
      "batch list num: 129\n",
      "{'in': array([[  3,  77,   8, ...,  17, 224,  38],\n",
      "       [ 12,  16,  55, ...,   4, 317,  14],\n",
      "       [491,   3, 120, ...,  27,  25,  18],\n",
      "       ...,\n",
      "       [ 19,  25,  12, ..., 190, 255, 165],\n",
      "       [ 11,  23,   4, ...,  10,  49, 266],\n",
      "       [ 30,  12,  15, ...,   4,  14,  55]]), 'out': array([[ 77,   8,   3, ..., 224,  38,  12],\n",
      "       [ 16,  55,  46, ..., 317,  14, 491],\n",
      "       [  3, 120,   3, ...,  25,  18,   7],\n",
      "       ...,\n",
      "       [ 25,  12,  10, ..., 255, 165,  11],\n",
      "       [ 23,   4,  19, ...,  49, 266,  30],\n",
      "       [ 12,  15,  13, ...,  14,  55, 109]]), 'len': array([50, 50, 50, 50, 50, 50, 50, 50, 50, 50, 50, 50, 50, 50, 50, 50, 50,\n",
      "       50, 50, 50, 50, 50, 50, 50, 50, 50, 50, 50, 50, 50, 50, 50, 50, 50,\n",
      "       50, 50, 50, 50, 50, 50, 50, 50, 50, 50, 50, 50, 50, 50, 50, 50, 50,\n",
      "       50, 50, 50, 50, 50, 50, 50, 50, 50, 50, 50, 50, 50, 50, 50, 50, 50,\n",
      "       50, 50, 50, 50, 50, 50, 50, 50, 50, 50, 50, 50, 50, 50, 50, 50, 50,\n",
      "       50, 50, 50, 50, 50, 50, 50, 50, 50, 50, 50, 50, 50, 50, 50, 50, 50,\n",
      "       50, 50, 50, 50, 50, 50, 50, 50, 50, 50, 50, 50, 50, 50, 50, 50, 50,\n",
      "       50, 50, 50, 50, 50, 50, 50, 50, 50, 50, 50, 50, 50, 50, 50, 50, 50,\n",
      "       50, 50, 50, 50, 50, 50, 50, 50, 50, 50, 50, 50, 50, 50, 50, 50, 50,\n",
      "       50, 50, 50, 50, 50, 50, 50, 50, 50, 50, 50, 50, 50, 50, 50, 50, 50,\n",
      "       50, 50, 50, 50, 50, 50, 50, 50, 50, 50, 50, 50, 50, 50, 50, 50, 50,\n",
      "       50, 50, 50, 50, 50, 50, 50, 50, 50, 50, 50, 50, 50, 50, 50, 50, 50,\n",
      "       50, 50, 50, 50, 50, 50, 50, 50, 50, 50, 50, 50, 50, 50, 50, 50, 50,\n",
      "       50, 50, 50, 50, 50, 50, 50, 50, 50, 50, 50, 50, 50, 50, 50, 50, 50,\n",
      "       50, 50, 50, 50, 50, 50, 50, 50, 50, 50, 50, 50, 50, 50, 50, 50, 50,\n",
      "       50, 50, 50, 50, 50, 50, 50, 50, 50, 50, 50, 50, 50, 50, 50, 50, 50,\n",
      "       50, 50, 50, 50, 50, 50, 50, 50, 50, 50, 50, 50, 50, 50, 50, 50, 50,\n",
      "       50, 50, 50, 50, 50, 50, 50, 50, 50, 50, 50, 50, 50, 50, 50, 50, 50,\n",
      "       50, 50, 50, 50, 50, 50, 50, 50, 50, 50, 50, 50, 50, 50, 50, 50, 50,\n",
      "       50, 50, 50, 50, 50, 50, 50, 50, 50, 50, 50, 50, 50, 50, 50, 50, 50,\n",
      "       50, 50, 50, 50, 50, 50, 50, 50, 50, 50, 50, 50, 50, 50, 50, 50, 50,\n",
      "       50, 50, 50, 50, 50, 50, 50, 50, 50, 50, 50, 50, 50, 50, 50, 50, 50,\n",
      "       50, 50, 50, 50, 50, 50, 50, 50, 50, 50, 50, 50, 50, 50, 50, 50, 50,\n",
      "       50, 50, 50, 50, 50, 50, 50, 50, 50, 50, 50, 50, 50, 50, 50, 50, 50,\n",
      "       50, 50, 50, 50, 50, 50, 50, 50, 50, 50, 50, 50, 50, 50, 50, 50, 50,\n",
      "       50, 50, 50, 50, 50, 50, 50, 50, 50, 50, 50, 50, 50, 50, 50, 50, 50,\n",
      "       50, 50, 50, 50, 50, 50, 50, 50, 50, 50, 50, 50, 50, 50, 50, 50, 50,\n",
      "       50, 50, 50, 50, 50, 50, 50, 50, 50, 50, 50, 50, 50, 50, 50, 50, 50,\n",
      "       50, 50, 50, 50, 50, 50, 50, 50, 50, 50, 50, 50, 50, 50, 50, 50, 50,\n",
      "       50, 50, 50, 50, 50, 50, 50, 50, 50, 50, 50, 50, 50, 50, 50, 50, 50,\n",
      "       50, 50])}\n"
     ]
    }
   ],
   "source": [
    "print(tokenizer.text2id('こんにちは😁'))\n",
    "print(tokenizer.id2text([33, 27, 8, 51, 14, 3]))\n",
    "print('batch list num: {}'.format(len(batch_list)))\n",
    "print(batch_list[0])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Training"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "max_epoch = 50\n",
    "save_path = 'tmp/rnnlm/model.ckpt'\n",
    "log_dir = 'tmp/rnnlm/log/'\n",
    "learning_rate = 0.001"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "if not os.path.isdir(os.path.dirname(save_path)):\n",
    "    os.makedirs(os.path.dirname(save_path))\n",
    "if not os.path.isdir(log_dir):\n",
    "    os.makedirs(log_dir)\n",
    "\n",
    "global_step = tf.Variable(0, name='global_step', trainable=False)\n",
    "optimizer = tf.train.AdamOptimizer(learning_rate)\n",
    "train_op = optimizer.minimize(loss_op, global_step=global_step)\n",
    "tf.summary.scalar('loss', loss_op)\n",
    "summary_op = tf.summary.merge_all()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "min_loss = 100000.0\n",
    "sess = tf.Session()\n",
    "summary_writer = tf.summary.FileWriter(log_dir, sess.graph)\n",
    "saver = tf.train.Saver()\n",
    "\n",
    "sess.run(tf.global_variables_initializer())\n",
    "for epoch in range(max_epoch):\n",
    "    random.shuffle(batch_list)\n",
    "    for batch in tqdm(batch_list):\n",
    "        feed_dict = {\n",
    "            in_ph: batch['in'],\n",
    "            out_ph: batch['out'],\n",
    "            len_ph: batch['len'],\n",
    "        }\n",
    "        _, loss, summary, step = sess.run([train_op, loss_op, summary_op, global_step], feed_dict)\n",
    "        summary_writer.add_summary(summary, step)\n",
    "        if loss < min_loss:\n",
    "            saver.save(sess, save_path)\n",
    "            min_loss = loss\n",
    "    print('epoch {}/{} - loss: {}'.format(epoch, max_epoch, loss))\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Restore"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "load_path = 'learned_model/rnnlm/model.ckpt'\n",
    "sess = tf.Session()\n",
    "sess.run(tf.global_variables_initializer())\n",
    "saver = tf.train.Saver()\n",
    "saver.restore(sess, load_path)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Generate"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "start_char = '私'\n",
    "generated_ids = sess.run(generate_op, {\n",
    "    gen_start_token_ph:  tokenizer.text2id(start_char)[0]\n",
    "})[0, :, 0]\n",
    "generated_text = start_char + tokenizer.id2text(generated_ids)\n",
    "print(generated_text)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
