More than 3 years have passed since last update.皆さんはビッグデータを扱うときどのような形式で保存していますか？ここでいうビッグデータとは数GB～数十GB（笑）のJSONです。MongoDBのようなNoSQLなデータベース使う？素晴らしいと思います。PostgreSQLでJSONを使う？とても良いと思います。
ここでは、データベースという枠組みから外れて、「ファイルシステム」を中心に手軽にお安く（ここポイント）ビッグデータを扱うことを考えます。なので、この方法は最速ではありませんし、個人がちょっと遊んでみようというときに気楽にできる”チープ”な物です1。企業でやるならちゃんとしたデータベースを使うべきです。その前提で読んでみてください（ちょっと長いです）。ファイルシステムは、テキストファイルやZipアーカイブといったただのファイルです。ただのファイルなので、データベースが得意なインデックスも効きませんし、検索や結合も弱いですし、同時接続やトランザクションの管理という器用なことはできません。ただ、頭から最後まで全部読むのにはそこそこ強いです。以下の記事は、MessagePackを使ってデータ分析をしている素晴らしい内容です。背景となっている状況が結構被っているのでぜひ読むことをおすすめします。MessagePackを使ったデータ分析のすすめ
https://qiita.com/m_mizutani/items/c40295549c3368a4257dこの投稿では、ビッグデータ（）の例として某仮想通貨の公開データを用います。自分が書いた記事で恐縮ですが、以前書いた記事の続きです。JSON1個あたりこのようになっています（4～5KB）。このJSONからlast_price.priceとtimstampを抜き出す操作をしてパフォーマンスを検証します。

このようなJSONが1日あたり6万件程度送られてきます。23日分のデータをUTF-8エンコードの無圧縮テキストで記録すると次のようになります。あまりにファイルアクセスが遅すぎたのでSSDに移しました。

ファイル数：1,398,345個、合計サイズ5.59GBです。1日あたり無圧縮で250MB程度です。フォルダは日単位→10分単位でデータを区切るように設定しています。
このデータを実際分析するとき、データをメモリに格納できればそれが一番速いですが、全てで5.6GBなので、丸ごとメモリにインポートするのはきつくなっていきます。HDDにすら載せきれないTB、PB単位の本物のビッグデータはもはや個人で扱うものではないので関知しません。データベースを使わない場合、とにかくファイルシステムのオーバーヘッドと戦うことになります。本稿ではまず、ファイルシステムを使った例として、無圧縮のテキストとZipアーカイブで固めた場合を比較し、読み込みのオーバーヘッドがどの程度変わるのかを体感します。次に、DBを使った例としてPostgreSQLとMongoDBからロードして速さを体感します。最後にファイルシステムに戻り、圧縮・シリアル化のフォーマットを変えていって、DBに近い速度で読み込ませることを目標とする結構無謀な企画です。基本的にIOがボトルネックになるため、ストレージの転送速度を上げることが重要です。HDDではなくSSDを使うのはとてもメリットが多いです。書き込み先は全てSSDで、CrystalDiskMarkでのベンチマークは以下の通りです。

基本的にC#で行いますが、Linux環境でも読み書きできることを保ちたいのでPythonでも実行できることを目指します。ファイル圧縮を1から独自のフォーマットで書いていくようなことはしません。環境：.NET Framework 4.5.2、PostgreSQL 10.2、MongoDB v3.6.2(x86_64)C#ライブラリ：Utf8Json v1.3.7(2018/1/30)、MessagePack v1.7.3.4(2018/1/30)、IonKiwi.lz4.net v1.0.10(2017/12/6)、Npgsql v3.2.6(2017/12/3)、MongoDB.Driver v2.5.0(2017/12/13)、SharpCompress v0.19.2(2017/12/16)テキストファイルを全部読み込むだけの簡単なお仕事。容量：5.59GB（サイズ）、10.6GB（ディスク上のサイズ）：エクスプローラーで確認
読み込み時間：01:44:01.2668862すごく重いし、すごく遅い。やってられない。Zipで固めるとちょっとマシになります。System.IO.Compressionを（ダメだったらSystem.IO.Compression.FileSystemも）参照に追加しておきます。Plain Textほど遅くないのでJSONパースを入れてみます。JSONパースはUtf8Jsonを使います。容量：1.08GB（エクスプローラー上でのサイズ）
読み込み時間：00:02:46.2418295（JSONパースなしの場合は00:02:14.1958927）Zipで固めただけでだいぶマシになりました。JSONパースなしの場合はPlain Textの場合と同様に、バイナリを変数に代入して終了させています。Utf8Jsonはやーい！ バイナリまで読んでしまったらあとはJsonパース部分はファイルフォーマットによらないので、以下全部パースする場合で計測します。だいたい30秒マイナスすれば素の解凍時間になります。以下の定義でデータテーブルを作ります。PostgreSQLの日付の扱いについては拙作の記事でグダグダと書いたのでよろしければ。コンソールからサーバーを起動して、データベースを作っておきます。データ挿入。.msg.lz4というのは後で出てくるMessagePack+LZ4のアーカイブ形式です。Npgsqlを用います。5～6分で挿入完了です。データ数の確認。データテーブルのサイズは以下の通り。データの読み込み。全部読むだけなのであまりDBの恩恵を感じられないかもしれません。サイズ：1.58GB
読み込み時間（中央値）：00:00:19.8031381（他00:00:19.5927425、00:00:19.8031381）さすが本業のDB強い。JSONBでバイナリ対応になって若干サイズも縮んでいます（Zipよりやや圧縮が悪いぐらい）。PostgreSQLのロードマップを見ていると、将来圧縮に対応する予定もあるそうなので期待しましょう。これでインデックススキャンできるのだから大したものです。SQL全般的に設定項目が多いのでチューニングを気にして本末転倒にならないように注意してください。特に設定せずに8GBぐらいのデータテーブルを頭から最後まで読んだら10GBメモリアロケーションされたので、大規模なテーブルを読むときはCPU、IO、メモリ全部に気を遣う必要があります。DBの場合はお片付けも忘れずに。JSONのDBといったらこれです。設定ファイルは以下のようにしました。Snappyで圧縮をかけます。最新のMongoDBだとデフォルトで圧縮かかっているみたいですね。MongoDBはinit_dbしなくていいのが楽です。サーバーを起動します。確認用に別のコンソールでクライアントも起動しておきます。挿入用コード。MongoDB.NET Driverを用います。挿入終わるまで20分ぐらい。JSON→BSONへの変換に時間かかってここがボトルネックになってしまうのがもうちょっと上手く行けばと思いました（BSONがJSONのバイナリだからと言って読み込んだUTF8のバイナリをそのままぶち込むのはダメだし）。容量を調べると1.54GB。PostgreSQLとあんまり変わりません。読み込みはこうです。ネスト構造まではうまく消せなかったので、読み込むときに絞ってあとアプリ側でいじる作戦で。サイズ：1.54GB
読み込み時間（中央値）：00:00:15.8827782（00:00:16.1628182、00:00:14.9827487）インデックスを張っていない違いはあれど、PostgreSQLより若干速い。MongoDBはクエリ実行時ではなく、サーバーが動いている限りメモリを食った状態で常駐しているので気になるといえば気になるかもしれません。DBのお片付けもちゃんとしましょう。クライアントのコンソール側から。もういらねえやこの作業ファイル！でエクスプローラーから消しても、ちゃんとDBを初期化してくれるのいいところだと思います。SQLは下手に消すと動かなくなるので。結論としてはDBほどの速度はでませんが、DBよりは容量は縮みます。Zipの読み込み時間が2分46秒なので、1分切りを目指します。Linuxでよく用いられているアーカイブ形式です。Pythonの場合はtarfileというモジュールが組み込みでついてくるので、Pythonが使えればルート権限は不要です。コマンドからもできます。Pythonからの使い方は公式ドキュメントを参照Zipファイルから一時ファイルに書き出さずにメモリを中継して圧縮させる方法です。C#ではSharpCompressを使います。オンザフライでの書き込みはストリームの数が多くなると同期を取るのが結構難しくになり、下手をしなくてもアーカイブが壊れるのでとても注意してください。特にこのライブラリはデリケートです（そこそこハマった）。SharpZipLibのほうが扱いやすいですがNuGetに上がっているバージョン若干遅いです（解凍でSharpCompressの3～4倍時間がかかる）。
下記のコードのように、いったんメモリ上に読み込んだファイルをバッファリングしながら丁寧に書いていくと壊れません。5分で圧縮が完了しました。メモリ上に逐次解凍して読み込みます。サイズ：211MB（ディスク上は217MB）
読み込み時間：00:01:39.3375536（3回やった中間値：1:39～1:47）Zipよりちょっと速くなり、ファイルサイズは結構縮みました。サイズ面では既にDBに勝っています。圧縮率重視のフォーマットとしてTar.xzがあります。これは7zipに似たLZMA系統の圧縮で、7zipと違いLinuxでも結構簡単に扱えます。Pythonでは組み込みモジュールで読み書きできます（C#もtarアーカイブぐらい組み込みで用意してもいいと思うんですけどね…）。Tar.xzのC#の読み書きはこちらでも書きました。圧縮。SharpCompressにはXZがないので、TarのみSharpCompressで行い、XZはXZ.NETで行います。他にLZMA SDKが必要です。Pythonでは特にインストールは必要ありません。圧縮は20分ちょいでした。サイズを縮めているだけあって遅いです。解凍。サイズ：135MB（ディスク上は141MB）
読み込み時間：00:01:41.1453396（3回の中央値、1:40～1：44）LZMA系なので読み込みも遅いかなと思いましたが、まさかのgzip並の読み込み速度が出てびっくり。圧縮率がすごく良い（Plain Textでは5.6GBもあったことを思い出しましょう）ので、圧縮速度がいらない場合は結構ありかもしれません。同じLZMA系にLZIPもありますが、今回は省略します。暇な方はやってみてください。neuecc先生の傑作ライブラリ、MessagePack for C#があります。これにLZ4フォーマット（速度特化のフォーマットです）がついていたので、これを参考にしました。ただし、MessagePack for C#のライブラリのLZ4フォーマットはFrame Formatではないので（2018年1月末時点）、LZ4圧縮の部分は相互運用性の観点からFrame Formatが使えるIonKiwi.lz4.netを用います。Frame Formatの問題は以前書いたのでよろしければ参考にしてください。リンクの記事にPythonでの圧縮・解凍方法もあります。したがって、ルート権限があればMessagePack＋LZ4でもPythonと相互運用できます。Pythonと相互運用できればLinux環境でも全然使えます（自分は今VPS環境とやり取りするときにこの方式を使っています）。MessagePackもLZ4もアーカイブ機能はないので、バイナリを配列としてつなぎ合わせて擬似アーカイブにします。ファイル名やファイルの種類とかそういう情報は落ちるのですが、まあそういうのを気にしない用途向けで（格納するデータ側にタイムスタンプがあるとかそういう用途）。もちろんMessagePackもJSONみたいなものなので、定義すればそういった情報を保持できます。前置きが長くなってしまいましたが、C#での圧縮です。MessagePackとIonKiwi.lz4.netをNugetからダウンロードしておきます。圧縮は3分程度で終わりました。はやーい。解凍。サイズ：280MB（ディスク上は286MB）
読み込み時間：00:00:52.6152158（3回の中央値、全て52秒台）ぶっちぎりで速いです。1分切りに成功しました。うち30秒ぐらいはJSONパース部分なので、残りの22秒で展開していることになります。速度面ではDBに勝つことはほぼ不可能ですが、容量はDBの1/6ぐらいになっているので、相当肉迫しているといえると思います。ファイルシステムも捨てたものではないです。
ただチェックサムが全くないので、そこら辺気にする方はMessagePackではなくtarに固めてからのほうがいいかもしれません。無圧縮tarならほとんど速度落ちませんので。結果を表にまとめました2 3 4 5 6。速度面ではファイルシステムで最速だったMessagePack+LZ4がデータベースを超えることはできませんでした。データベースにはこれにインデックスやら同時接続やトランザクションがついてくるので強すぎます。
ただ、データベースにも1つにも苦手なことがあって、インデックスつきでシャッフルされたデータで苦手です。別の記事で調べてみたのですが、PostgreSQLのBtreeインデックスはシャッフルされたか整列されたかどうかでソートが5,6倍かかるようになります。なので、うっかり「あっこの期間のデータ入れるの忘れちゃった」「これより前の期間のデータを追加しよう」というような状況に弱いです。ファイルシステムの場合は、アーカイブ単位で時系列を整理しておけば、事実上パス文字列のソートになり、アーカイブ単位のソートで結構お手軽になります。
ファイルシステムの強みは、容量の少なさ、手軽さ、必要スペックの少なさなどで、DBが得意とするような弱みを差し引いても結構いい勝負ではないかと思います。特に個人向きかなと思います。普段はアーカイブとして保存しておいて、実際に使うときにローカルでデータベースに格納するのもありなので、やりたいことに合わせてうまく使い分けると快適になれるのではないかなと思います。余談ですが、アーカイバー同士の比較はFenrirのページでも紹介されていました。圧縮率と時間のトレードオフ関係がわかりやすいです。参考：Zip より Tar より QuickLZ が(サイズが大きいときは)便利です
https://blog.fenrir-inc.com/jp/2012/07/quicklz.htmlもちろん自分も5000兆円あったらAWSでもAzureでもガンガン利用します ↩サイズは1GB=1024MBとして計算 ↩ファイルのサイズはエクスプローラーのサイズより ↩Plain Text以外のファイルシステムは読み込み時間にJSONパースを含んでいる ↩要インストールとは実行する環境で新たにインストールが必要かどうか。ライブラリのインストールは含まれない ↩Linuxのルート権限はPythonの組み込みモジュールで実行できるかどうか ↩


