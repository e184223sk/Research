More than 3 years have passed since last update.linq.jsやZeroFormatterなどステッキーなライブラリを作っているneuecc先生の新作JSONシリアライザーのUtf8Jsonを試してみました。Utf8Json - Fast JSON Serializer for C#
https://github.com/neuecc/Utf8Json”Definitely Fastest”（明らかに最速）とのことなので、最速の恩恵を受けそうな例で実践します。拙作で恐縮ですが、C#で仮想通貨取引所のリアルタイムレートを記録するプログラムを作ってみたを1週間実行したところ、合計1GBを軽く超えるJSONファイルが生成できました。それをデシリアライズするプログラムを作ります。そして、既存のJSONシリアライザーとの比較を行います。環境：VisualStudio2017 Community / .NET 4.6.0ライブラリのバージョン関連記事：
.NETのJSONシリアライザの速度比較
https://qiita.com/t_takahari/items/6855dfe78071bb5eaef6仮想通貨取引所のストリーミングAPIから配信される、ビットコイン円（btc-jpy）の取引（板情報＋トランザクションデータ）。おおよそ1秒に1回～2回以下のようなJSONが送られてきます。JSON Viewerで見ると、次のような構造であることがわかります。

1つのJSONを1つのテキストファイルとして保存します。1つのテキストファイルはおよそ4KBです。last_price.priceとtimestampを抽出するプログラムを作ります。これらのテキストファイルは一定時間（設定では1分、5分）で1つのZipアーカイブに保存されています。途中でこの設定値変えてしまったので1日あたりのアーカイブ数はいい加減です。1/18～1/23の6日分のデータ用意します。エクスプローラーで見ると次のようになります。
Zipをすべて展開たところ、次のようになりました。
35.5万個のJSONが無圧縮で1.43GB（ディスク上は2.70GB）です。32bitプロセスのメモリ制限があるので、まずこれをList&lt;string&gt;としてすべてメモリに落とし込んでみます展開後のtxtファイルではなく、展開前のZipから直接読ませていますが、こっちのほうが速いからです（後述）。実行結果は次のようになります。メモリ使用量は3.0GBです。6日ではなく、7日分読み込ませると「System.OutOfMemoryException」が出ます。なので、これが32bitプロセスの限界とみなしてもよいと思います。64bitでビルドすればほぼ際限なくいけますが、JSONデシリアライズのパフォーマンスを図るには32bitのMAXで十分でしょう。たかだか1～2GBでビッグデータと言い張る気は毛頭ないですが、ビッグデータに片足の親指の爪先ぐらい突っ込んだぐらいのサイズはあります。また、一気にtxtListに読み込ませるのではなく、Zipアーカイブ単位で読み込ませ、その都度デシリアライズしていくと（実践的にはこちらを使います）、メモリ効率はいいですが、Zipの展開やファイルの読み込みにかかる処理時間と、JSONをパースするのにかかる処理時間を切り離しできなくなるので、ここでは一気にメモリを読ませる方法を使います。
txtListから読み込ませても、正確には、txtListから文字列を読み込ませる処理時間を切り離していないのですが、同じメモリ内での操作なので無視できるぐらい小さいとします。Zip→txtListではなく、Zip→Txt→txtListと読み込ませると、Zip→Txtの処理を事前に行っておいてもZip→txtListの場合より遅くなります。試してみます。decompressフォルダにはbtc_jpyフォルダ以下のZipアーカイブがそのままの形で展開されているものとします。50秒対26分で、明らかに遅いです。CPUパワーは全然使わないんですけどね。ファイルのオーバーヘッドぱない。とりあえずZipを使いましたが、この手の使い方をする場合、圧縮はパワー要しても解凍が軽いタイプの圧縮アルゴリズムを選ぶとよさそうな気がします。以下のJSONシリアライザーを比較します。1番目以外はdynamic型が使えるので、型を指定する場合、型を指定せずdynamicでデシリアライズする場合をそれぞれを見ます。
dynamicの場合、型を指定する場合と同様の返し方をすると少し不公平なので（キャストが多い分当然遅くなる）、必要なtimestampとlast_price.priceのみに絞ってTupleで返すようにします。今回の例では、dynamicでデシリアライズする変数の数が、型を指定する場合とそこまで変わらないので、基本的に型を指定したほうが速いです。ただし、文字と数字が混在した配列など不定形なJSONをデシリアライズする場合は、dynamicを使ったほうが都合がいいことがあるので、必要に応じて使い分けるのがいいかと思います。型を指定する場合のクラスをDataとして次のようにします。ライブラリの数が多いので、ライブラリごとに型を指定する場合、dynamicの場合とラッパーメソッドを作ります（普段はこんなことしなくていいです）。typeofで指定してStreamからしか読めないというのはちょっとなーという感一番有名なライブラリ。dynamicの場合が面白い書き方dynamicの場合、内部で文字列になるのが少しトリッキー。(double)とかキャストすると例外になる。直感的でわかりやすい"Dynamic"JSONなので、型指定の場合はdynamicの変数をキャストしてデシリアライズするというピーキーな仕様。プロジェクトにDynamicJSON.csが追加されるのが特徴。個人的には昔だいぶお世話になりました。dynamicの場合はインデクサを使います（.変数名は例外発生）。今後のアップデートで変わるかもしれません。以下のコードで比較します。Zip→txtListの続きからです。本当はtimestampとlast_priceを記録するはずでしたが、純粋にパフォーマンスを見たかったので端折っています。これを5回繰り返します。結果は次の通り。この例ではUtf8Jsonが最速にはなりませんでした。型を指定した場合、ServiceStackが6秒弱で最速、Jilも同じくらい速く7秒弱、そこから倍の6秒程度遅れてUtf8Jsonが13.6秒、そこから30秒程度とだいぶ遅れてJson.Netが43秒。.Net標準のDataContractJsonSerializerはJson.Netからさらに30秒程度遅れて1分14秒。DynamicJSONは一番遅く、2分オーバー。ただ、他のライブラリがdynamicでデシリアライズしたときに軒並み遅くなっているのに対し（おそらく全部のパラメーターを内部で一度パースしているのかと）、DynamicJSONはほぼ変わらないどころか、dynamicのほうが速いまでもあるので、型指定のクラス側で全部パースする場合はそこまで見劣りしないのかと思われます。
比較用にZipの読み込み時間を置いておきましたが、JSONシリアライザーの選択によってZipの読み込み1倍～2倍分の処理時間の差が出ます。本当はZip部分を高速化したかったけどそれはまた別な投稿で。
※追記：いろいろ試してみましたが、MessagePack C#を使うのが良さそうです。気力があれば書きます。dynamicの場合はServiceStackの15秒以外はだいたい1分オーバー。かろうじてUtf8Jsonが55秒と1分割っているぐらいです。こう見るとServiceStack一強のように見えますが、ServiceStackの場合はdynamicの場合doubleが文字列で記録されているので、キャストに苦労するかもしれません。ServiceStackがdynamicの場合に明らかに速いのは、利便性を犠牲にして高速化しているからかもしれません。この例の場合はStringの内部エンコードがUTF-16であるため、Stringのまま読み込ませているとUTF-8へ余計なエンコード変換が入るのがUtf8Jsonにとっていけないのかもしれません。そこでUtf8Jsonの場合のみ、stringではなくbyte[]を読ませるように改良してみます。これを3回ほど実行してみました。キャッシュ方式をstring→byte[]に変えたところ、Zipの展開時間が50秒→38秒と12秒ほど短縮され（24%減）、メモリ使用量が3.19Gbytes→1.61Gbytesと半分になり、Utf8Jsonのデシリアライズ時間が13.6秒→8.7秒と5秒ほど短縮され（36%減）ました。これでもまだ、stringの場合のServiceStackとJilのケースを抜くことができなかったのですが、特にメモリ減少の効果が大きそうです。JSONだとUTF-16までエンコードを拡大するメリットが無い（JSONの規則上エスケープされる）ので、UTF-8 byte[]で読み込んでしまうという作戦はかなり有用かもしれません。
ちなみにJilもServiceStackもTextReaderやStreamを引数として渡すことはできるものの、byte[]を直接渡すことができるのはざっと見たところUtf8Jsonだけでした。細かいチューニングを気にする場合、トータルで見たときにUtf8Jsonが優位に立てる可能性は十分にあります。1.のstringの例を並列化してみます。Parallel.ForEachを使うだけです。ただし、並列化する場合は読み込む順番の保証がされなくなるので、結果が時系列で欲しい場合は最終的にソートする、スレッドセーフなコレクションを使って記録するなどの工夫が必要になります。Utf8Jsonの場合はこうします（他も同様なので省略します）。MaxDegreeOfParallelismを指定しないときりがなく並列していって死ぬので、スレッド数は4に制限しました。結果は次のようになりました。試行間でそこまでぶれがないので、逐次処理の平均と並列処理の平均を比較してみます。逐次÷並列を時間比、逐次－並列を時間差とします。比のほうは2.45～3.0の範囲内です。4並列にしても額面通り速度が4倍になることはないので、妥当といえるでしょう。差が大きいケースほど比も大きくなることが多く、これは極端に速いケースでは、JSONデシリアライズ以外の関係ない処理（foreachや文字列の読み込み）が全体に占めるウェイトが高くなるからだと思われます。逆に差が大きいのに比でそこまで伸びていないケースは、もともとの計算負荷が高いゆえに並列化してもCPUリソース食い尽くしやすく、あまり好ましくないと言えるかもしれません。

ここで回帰直線から大きく上に飛び出いている例は、DataContractJsonSerializerの2例と、DynamicJSONの2例です。この2つは単純に処理時間を見ても選びづらいです。今までデシリアライズの際にstringやbyte[]を引数として渡していましたが、Zipアーカイブの各エントリーのStreamを渡す場合を考えます。したがってtxtListは必要なくなり、エントリーからダイレクトにデシリアライズするので、メモリ効率は遥かに良くなります。より実践的な方法です。これまで特に良好だった、ServiceStack, Jil, Utf8Jsonの3つについて比較をします。各5回やった場合の結果は以下の通りです。この例では、Utf8Jsonが（無事？）最速になりました。ほとんど処理時間にブレがないので、引数をstringにする場合とstreamにする場合で処理時間の平均値を比較します。「zip load」はStringとしてZipを読み込んだ場合のZipをすべて読み込む時間の平均値50.754秒を足しています。「zip load + string - stream」はこれにstringを引数にJSONをデシリアライズしたときの平均処理時間を足し、streamを引数にJSONをデシリアライズした平均処理時間を引いたものです。ここでの処理の差はstream→stringの変換をするかどうか、txtListにstringを格納するかどうか（zip load, stringの場合は格納しているが、streamを直接渡した場合は格納する必要がない）、txtListをforeachで回すかどうか、とすべて定数項で表されそうな処理ですが、ライブラリによってこの差が一定ではないので、引数がstringの場合、streamの場合でそれぞれデシリアライズに最適化が入っていると思われます。Jilはstringに強く、Utf8Jsonはstreamに強いことがわかります。ServiceStackは傾向としてはUtf8Jsonに近そうです。※追記（2/2）
entry.Open()で直接Stream読ませればいいところをStreamReaderをかませるという間抜けをしてしまったので、ダイレクトにEntryから読み込ませて再実験しました。JilについてはTextReader（StreamReaderが継承している抽象クラス）が引数なので除外し、ServiceStackとUtf8Jsonの比較を行います。初回にライブラリのロードが入ってるので、逆に遅くなっているかもしれません。基本的にStreamReaderを噛ませたほうが速くなるということはないはずです。この場合は0.1～0.5秒程度、entry.Open()したほうが速いといえるでしょう。（byte[]を変数で取る目的でもなく、StreamReader噛ませてたのがタダの間抜けなだけ）コメントで指摘してくださった方、ありがとうございました。以上、ギガバイトクラスのJSONを読み込んでデシリアライズのパフォーマンスを計測しました。この例ではUtf8Jsonがぶっちぎりで強いという結論にはなりませんでしたが、特定の状況下では最速になりました。次のような結論になります。今後のアップデートで大きく変わる可能性があるので、期待しましょう。すっかり忘れていましたが、これが作りたかったビットコインの2018/1/18～1/24の6日分のティックチャートです。csvで出力してExcelでグラフ化しました。



