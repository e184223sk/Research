More than 5 years have passed since last update.未来のゲームに Microsoft のテクノロジーはどういう風に貢献できるのだろうかと妄想を膨らませてみたところ、人間の認知機能を機械に持たせることが出来る Cognitive Services の API を使えば、ゲームはもっとインタラクティブに、より Real と Virtual の間の壁を壊してくれるのではないかと考え、「インタラクティブいちゃいちゃギャルゲー」というものを実際に実装し、国内最大規模のゲーム開発者向け技術カンファレンスの CEDEC のこちらのセッションでデモさせていただきました。
この記事では、そのデモの要素技術となる Emotion API や、妄想の産物「インタラクティブいちゃいちゃギャルゲー」とは何たるや、そしてどのようにして実装したのかをご紹介できればと思います。
この記事を通して、たった数行のコードでゲーム上のキャラに感情を理解する力を宿すことができるんだということが伝わり、ぜひ実際のゲームに取り入れていただければと思います。Microsoft が出している、REST で画像を投げると JSON 形式で画像を分析した結果を返してくれる API サービスのことです。
画像に写る人の顔位置を特定し、その表情に含まれる喜び、驚き、悲しみ、怒り、恐れ、嫌悪、軽蔑、中立の 8 つの感情を認識し、合計を 1 としてどの感情がどれだけ含まれているかを割合で返してくれます。
(公式ページ参照)こちらの例では 2 人の顔の位置がしっかり特定され、右側の人の感情が Surprise 99.6%として推定されていることが分かります。
(公式ページ参照)続いての例では右の人が Happiness 92.8% として推定されています。(左の人は Happniess 99.98% でした)
この結果から人間の直感に合った感情推定が精度よく行えていることが分かります。説明しよう。どんな妄想だったのかを。
ゲーム内のキャラが自分の表情を読み取って、例えば元気のない顔をしていたら「元気ないね、どうしたの？」と話しかけてくれたり、
楽しそうな顔をしていたら「一緒にいると楽しいね」と話しかけてくれたりしたら非常にうれしい気持ちになるんじゃないのか？
画面上での操作で閉じていたギャルゲーがリアルタイムで自分の気持ちを読み取って、インタラクティブに話しあえたらどんなに面白いんだろう？
そんなコンセプトを形にしたものが「インタラクティブいちゃいちゃギャルゲー」です。
構成図はこのようになります。
スマホや PC 等のデバイスのカメラからアプリが表情をチェックして、それを Emotion API に投げ、表情分析結果から一番強い感情に対しての反応（声やセリフ、表情）をキャラが返してくれるというものになります。ただ自分は絵心がなさすぎるので、自分の絵には誰も感情移入してくれないだろうと思い、デモ作成の際には、弊社エバンジェリストのちょまどさんにサポートしていただき、はしれ！コード学園の登場人物であるC#ちゃんを派遣していただきました。(詳しくは 漫画家兼エバンジェリストちょまどさんの応援 の項で)そうしてできたデモの完成プレイ画面はこのようになりました。

本来であれば、カメラからの表情を常にキャラがチェックしてくれた方がいいと思いますが、デモ用に自分のタイミングで画像を送るようにしていて、送った画像を右下に表示しています。早速試してみましょう！
笑顔を見せるとこんなセリフと表情を返してくれます。
自分に合った表情（笑顔）を返してくれているところに注目です。
驚いた顔を見せるとこんなセリフと表情を返してくれます。（場面はセリフに合うように別で切り替えています）
今度は場面を切り替え家の中へ、
怒った顔を見せるとこんな反応をくれます。一体裏側にどんなストーリーがあったのでしょうか。
ただ、笑顔を見せるとこうなります。

ざるですね。（自分の実装がワンパターンなだけ）ちなみに中立（無表情）を見せると、こういう風に返すように作っています。
Unity なので、C# による実装です。
Python2 系, Python3系, Java, JavaScript, Objective-C, PHP, Rubyの実装サンプルはこちらの下部に載っています。自分の画像から表情を読み取りそれに合わせた表情とセリフをセットしているのを以下のコードで実装しています。
以下のコードからたった6行のコードで表情からの感情推定が完了していることが分かります。返ってくる JSON は以下になります。
複数の顔が画像に移っている場合は複数人分の感情解析結果が返ってくるようになっています。今までにはなかったようなユーザーエクスペリエンスを提供できるサービスが、非常に短いコード行で実現できてプロダクトに載せることができるということがお分かりいただけたのではないかと思います。Cognitive Services の利用の際には Subscription Key が必要になります。無料試用が可能なのでこちらからご登録ください。Microsoft Account でのログインが必要となります。
登録して得られた Subscription Key をサンプルコード上の Ocp-Apim-Subscription-Key で指定してください。自分でコードを書く前にまずはどんなものか試してみたいという場合は、公式解説ページを見てもらうと準備されている画像を選ぶ or 自分の画像をアップロードして確認できるようになっているのでおすすめです。無料のプランだけではなくビジネスユースでの使用も可能である Basic, Standard のプランも用意されています。
今回詳しくは説明していませんが、Emotion API には動画上の感情推定機能もあり、現時点(2016.8.26)では無料で提供されています。
Standard Planでの使用だと1000枚の画像を分析して2.5 ドルとなります。こちらからダイレクトに Standard Plan のアカウントを立てることができます。
Standard Plan をお使いいただく場合は、Microsoft Azure の Subscription が必要となるので、もしお持ちでない方はこちらから 1か月 ￥20.500 の無料試用アカウント等もぜひお試しください。今回の記事から実際にプロダクトに組み込んでみたいとなった場合はサポートできることがあるかもしれないので、コメントでも個人的にでも連絡でもいただければ！あまりに絵心のない自分に対して弊社の漫画家兼エバンジェリストとして活動されているちょまどさんがキャラを貸し出してくれました。
Twitter アカウントはこちらで、
はしれ！コード学園というのを連載されているのでぜひ興味を持たれた方は読んでみてください。
C#ちゃんは、はしれ！コード学園の登場人物で、キャラの性格設定が C# らしい性格になっていて、他のキャラ（言語）もなるほどなとなる面白い性格設定になっているので、エンジニアの人には楽しんでいただけると思います。
さらにCEDECでデモをさせていただいた際は、ちょまどさんには声の出演（ C# ちゃんの声優）もしていただきました。
声の方は会場限定公開なので、ファンで会場にいらっしゃった方は非常に幸運だったと思います。
本当にありがとうございました！自分はまだまだ C#, Unity 共に駆け出しなので、こういった書き方の方がいいよというのがあればぜひご指摘いただけるとありがたいです。（後程Gitにすべてのコードを公開する予定なので特にそれに関していただければ幸いです。Githubに上げ次第更新通知送りますね）Emotion API は、Microsoft の Cognitive Services というクラウドベースの API サービス群の1つで
下記5つのカテゴリのうちの Vision カテゴリに属する API です。Vision カテゴリ内の API は下記のようになっています。Emotion API では画像認識を含んだ以下の機能が提供されています。


