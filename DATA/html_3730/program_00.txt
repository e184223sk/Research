using System;
using System.Threading.Tasks;
using Microsoft.CognitiveServices.Speech;
using Microsoft.CognitiveServices.Speech.Audio;

namespace AzureCognitiveServicesAPI
{
    class Program
    {
        static string subscriptionKey = "YourSubscriptionKey";
        static string serviceRegion = "YourServiceRegion";

        static string speechRecognitionLanguage = "ja-JP";
        static string speechName = "ja-JP-NanamiNeural";

        static async Task Main(string[] args)
        {
            var speechConfig = SpeechConfig.FromSubscription(subscriptionKey, serviceRegion);
            speechConfig.SpeechRecognitionLanguage = speechRecognitionLanguage;

            var audioConfig = AudioConfig.FromDefaultMicrophoneInput();
            var recognizer = new SpeechRecognizer(speechConfig, audioConfig);
            var synthesizer = new SpeechSynthesizer(speechConfig);

            var speakText = "";

            while (speakText != "終了。")
            {
                speakText = await SpeechToText(recognizer);
                await TextToSpeech(synthesizer, speakText);
            }

            await TextToSpeech(synthesizer, "プログラムを終了させますね。");
        }

        static async Task&lt;string&gt; SpeechToText(SpeechRecognizer recognizer)
        {
            var result = await recognizer.RecognizeOnceAsync();
            Console.WriteLine($"話した内容： {result.Text}");
            return result.Text;
        }

        static async Task TextToSpeech(SpeechSynthesizer synthesizer, string speakText)
        {
            await synthesizer.SpeakSsmlAsync(CreateTextReadOut(speakText));
        }

        static string CreateTextReadOut(string text)
        {
            return $"&lt;speak version='1.0' xmlns='https://www.w3.org/2001/10/synthesis' xml:lang='{speechRecognitionLanguage}'&gt;&lt;voice name='{speechName}'&gt;{text}&lt;/voice&gt;&lt;/speak&gt;";
        }
    }
}

