More than 3 years have passed since last update.　自動微分(Automatic Differentiation, AD)とは、その名の通り自動で微分する技術のことです。プログラム上での微分手法には、$\frac{f(x+h)-f(x)}{h}$みたいな計算をして求める数値微分や、数式処理によって偏導関数を求める数式微分などがありますが、自動微分ではどちらとも違う手法で微分値を求めることができます。　自動微分にはボトムアップ型の手法とトップダウン型の手法があり、以下のページではボトムアップ型の実装を紹介しています。
- 二重数で自動微分する
- ボトムアップ型自動微分の実験　今ブームのニューラルネットワークで用いられている誤差逆伝播法はトップダウン型自動微分の一種ということなので、ここではトップダウン型の実装に挑戦してみようと思います。言語はC#。　自動微分では、合成関数の微分の考え方を利用します。　自動微分では、対象となる関数を対象となる関数を単純な関数に分解して計算グラフで考えます。たとえば、$y=a\exp(x)\times(\exp(x)+b)$から$\frac{\partial y}{\partial x}$を求めることを考えます。$y$を初等関数や四則演算に分解して、各演算に変数名を振ると、となり、以下のような計算グラフで表せます。各ノードが中間変数、各エッジが演算の入出力関係を示しています。
　各演算は単純なものなので、微分値を計算することができます。この微分値と各エッジを対応付けて考えます。
　グラフを見てわかるように$c$以外は直接$x$の関数ではないので、合成関数の考え方から、$\frac{\partial y}{\partial x}=\frac{\partial y}{\partial c}\frac{\partial c}{\partial x}$となります。また同様に、$\frac{\partial y}{\partial c}=\frac{\partial x}{\partial d}\frac{\partial d}{\partial c}+\frac{\partial x}{\partial e}\frac{\partial e}{\partial c}$となります。ですので、上から順にと計算することで$\frac{\partial y}{\partial x}$を求めることができます。
　この考えを、プログラムで実装していきたいと思います。　まず、計算グラフを構築できるような自動微分型クラスを考えます。一つのインスタンスが一つの演算や変数を表すとして、演算の出力値、入力値、入力変数による偏導関数値をメンバとして持つように実装するとこんな感じでしょうか。　演算を表すコンストラクタは外から直接呼ぶことはないのでprivateにしてあります。四則演算はオペレータオーバーロードで、初等関数はstaticなメンバ関数として実装します。　AddInputは、指定したインデックスのInputsとDifferentialsに入力変数と対応する偏微分値を格納する関数です。$\frac{\partial}{\partial x}(x+y)=1,\frac{\partial}{\partial y}(x+y)=1$なので、+演算子のオーバーロードの中でxを格納したInputsに対応するDifferentialsに1を入れ、yに対応するDifferentialsにも1を入れています。同様に、$\frac{\partial}{\partial x}(x\cdot y)=y,\frac{\partial}{\partial y}(x\cdot y)=x$なので、*演算子のオーバーロードの中で、x,yに対応するDifferentialsにそれぞれy,xの出力値を入れています。またExp関数に関しては、$\frac{\partial}{\partial x}\exp(x)=\exp(x)$で、出力値と等しくなるので、Differentialsに自身の出力値を入れています。
　このように実装することで、自然に計算グラフを構築できるクラスを実現できます。上記計算グラフは、たとえば等とすることによって構築できます。微分を行うメンバ関数GetDifferentialを実装します。この関数は、上記AD型変数yに対してy.GetDifferential()とすると、$\frac{\partial y}{\partial a},\frac{\partial y}{\partial b},\ldots,\frac{\partial y}{\partial x}$を取得できるような関数です。以下のような考え方でこの関数を実装します。この考えでGetDifferentialを実装すると、　ValとDifは変数値や偏微分値を取得するためのプロパティです。y.GetDifferential()とやると、x.Difが$\frac{\partial y}{\partial x}$を表す値になります。
　基本的には上で書いた考え方の通りに実装していますが、いくつか実装上の都合で変わっている部分を説明します。
　計算グラフの頂点からグラフをたどる場合、グラフの形によっては同じノードを複数回たどることになります。重複してノードをたどってしまうと、UsedNumの数がずれたり、偏微分値の足しこみが重複して正しい結果を計算できなくなります。InPreparationというメンバ変数を用意して、ノードの状態を管理することで、重複計算を回避しています。
　また、CalcListというキューをstaticで用意して、計算待機リストとして使っています。prepare関数やcalculate関数の中で入力ノードをCalcListに追加して、GetDifferential関数の中でwhileを回してCalcListに追加したノードでprepareやcalculateすることで、計算グラフをたどりながら計算することを実現しています。関数の再帰等の手法でも実現できますが、複雑な演算を微分する場合、再帰だとスタックオーバーフローする可能性があるのでこのような手法を使いました。
　for文のところはLINQとか使った方がかっこいいかなとも思いましたが、実行速度を優先してこんな風になりました。　簡単のため和積とExp関数しか実装しませんでしたので、他の四則演算やよく使う関数も実装します。　あまり使わないような関数もついでに実装してみました。AbsやMax等の関数も（無理矢理）微分できるのが自動微分の特徴です。SigmoidやReLUはニューラルネットワーク等で頻繁に使用される関数なので実装してみました。また、平均や合計や内積関数を見てわかるように、入力変数が3つ以上になる関数も実装可能です。　実際に、試してみました。　$x=1,a=2,b=3$のとき、となるので、微分計算できていることがわかります。ループを使うような計算や、最急降下法等への応用もできているようです。　最急降下法で注意が必要なのは、xの更新で x -= 0.1 * x.Dif とせずに x = x.Val - 0.1 * x.Dif としている点です。右辺でdouble型にして代入の際に暗黙の型変換で自動微分型に戻しています。一時的にdouble型にすることで、自動微分型の計算グラフを断ち切っています。この程度の計算なら対して問題にはなりませんが、この書き方をしないと、更新前のxが計算グラフに延々と連なってしまいメモリや実行速度の上で不利になります。　次はADクラスを使ってニューラルネットっぽいものを作ってみたいと思います。　実装にあたって、以下のページを参考にしました。
- 自動微分で遊ぼう


