　今回はいよいよニューラルネットワークの基本とも言える、Linear層を実装したいと思います。他の実装と比べて特殊な点は、インスタンスごとにParamデータを格納しておき、学習時にそれらの更新ができるようになっているという点です。　全結合層(LinearまたはAffine, Fully Connected Layerなど)で行われているのは以下のような操作です。これらは基本的に今まで実装した関数で実装できると思いましたが、バイアスの足し算の際に次元を補完するためにRepeat関数を追加します。　これは実装上では以下のように表されます。$*$は0またはそれより大きい数の次元が入ります。　また、学習の際にパラメータを操作するために、ParamersプロパティをIFunctionに設定します。　Repeatクラスの実装は集合演算のクラスと似ていて、対象となる次元と繰り返し回数を否定することで指定した次元方向に複製したTensorを出力します。　学習対象となるパラメータを持った関数からパラメータを取り出すためにIFunctionインターフェースにParametersプロパティとBaseFunctionにParamsメンバを追加しておきます。　これでIFunction.Parametersにアクセスできるようになりました。　これでLinearクラスに必要なものが揃いましたので実装していきます。ここで問題となるのが、Linearがどのクラスを継承するのかです。LinearクラスのForwardでは、DotとBiasの加算が行われますが、これはすでに実装しており、Linearクラスはこれら二つの演算と二つのパラメータを格納するクラスとなっています。そのためBaseFunctionのForward処理をオーバーライトする必要があります。　今回はLinear関数の実装をしました。やっとパラメータを持った関数を実装することができました。ここで改めて小目標として設定した「DQNを実装する」ですが、現時点でこれに必要なのはの四つぐらいですかね。上三つに関してはこれまでの延長線上で出来そうな気がするのですが、パラメータの保存、読み込み辺りはどういう形式で保存すれば良いか分からないです。Pytorchとかではどう実装してるんでしょうか。　そこら辺は追々考えることにして次回はdetachメソッド、ロス関数あたりを実装していきます。


