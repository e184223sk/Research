More than 1 year has passed since last update.　バーチャルYouTuberさんについて広く知りたいけど、いろんなチャンネル、Twitter見て新着動画探すの大変だよー。　となったのでバーチャルYouTuberが投稿した動画、公開した生放送を自動で再生リストに登録してツイートするボットを作りました。紹介します。日刊VTuber動画(@vtuber_movies)さん | Twitter
https://twitter.com/vtuber_movies　こんな感じにツイートされます。再生リスト「2019年7月14日に投稿されたバーチャルYouTuberの動画・生放送」を作成しました。(119件登録済み)https://t.co/pxGURk5J0J　アイコン絶賛募集中です。　ざっくり下記の流れでプログラムが動きます。　GitHubで公開済みです。kokeiro001/YouTubeNotifier
https://github.com/kokeiro001/YouTubeNotifier　だいたいこの辺の技術使って実現しました。　そもそもどんなバーチャルYouTuberさんが動画投稿、生放送を実施しているか知る必要があります。自動で。　今回はVtuber Insightさんのデータをクロールして利用させていただくことにしました。Vtuber Insight
https://vtuberinsight.com/　さて、このサイト。チャンネル名やチャンネルのIDを取得できるのですがWebSocketを用いて実現しています。普段Webページからのデータ取得、クロールには適当なHttpClientを用いてGetやらPostで取得しているのですがWebSocketではそうも行きません。　そこで、今回はヘッドレスモードのChrome＋Seleniumを用いてざっくり下記のようにデータを取得しました。　Chromeの起動時オプションでいろいろな機能を無効にしてます。このプログラムはConoHaのメモリ512MBのVPSで動かしてるのですが、少なくとも画像読み込まないようにしないとメモリ使い潰してしまいます。多分headlessにした時点で無効になってるパラメータもあって、重複して無効にしようとしているオプションあるんだろうなとは思います。が、動いてるのでおｋ．気が向いたらしっかりドキュメント読みつつ検証します。　DockerイメージにChrome/Seleniumをインストールするにあたっては下記リポジトリを参考にさせていただきました。こっちのDockerファイルもあまり理解しないままほぼ丸コピしてるだけなので気が向いたらしっかり読みます。　次はVtuber Insightから取得したYouTubeのチャンネルIDを用いて、それぞれのチャンネルにアクセスして新着動画を取得していきます。　新着動画を取得するにあたって、実現方法は大きく２パターンを想定していました。　はじめは素直に1で実装していました。任意の期間に投稿された動画をAPIリクエストに入れられたのでそもそも実現も楽そうでしたし、何より「普通こっちだよなぁ」みたいな直感があったので。　が、YouTubeのAPIのクォータを消費してしまうことがわかったため辞めました。YouTube Data API の概要  |  YouTube Data API (v3)  |  Google Developers
https://developers.google.com/youtube/v3/getting-started?hl=ja#quotaYouTube Data API では、デベロッパーが目的どおりにサービスを使用できることと、不当にサービス品質を低下させたり他のリソースへのアクセスを制限したりしてしまうアプリケーションの作成を防ぐことを目的としてクォータを使用します。　動画取得のためにAPIを叩いてしまうと、このプログラムの本命である「再生リストに動画を登録する」APIを叩いてる際にクォータを消費しきってしまい、新着動画を登録しきれなくなることが判明しました。(この利用上限を開放するためには英語による手続き、審査が必要なので面倒臭い。。)　そこで、クォータを消費しないRSSを用いて新着動画を取得することにしました。下記のようなURLでそれぞれのチャンネルのRSSにアクセスできます。　取得後、自分でRSSをパースする手間が増えますがこれでいくらチャンネル情報を取得してもクォータ利用制限で死ななくなります。　自分用のコードはガンガンハードコードするマン。　ちょっとクォータ使用してないことに抜け道感はありますが、おそらくYouTubeのサーバー的にもRSSへのアクセスのが低負荷でそんなに悪い子としてないと自分を信じ込ませましょう。現在読み込むチャンネル数は300にしていますが、適当にディレイ挟みながらアクセスしてるのでそこまでしんどくないはず。300って数字は気分で決めました。　素直にnugetから Google.Apis.YouTube.v3 をインストールしてAPI叩いていきます。この辺のAPI使うにはGoogleの開発者登録必要です。必要に応じて登録してください。　今回作ったプログラムは一日に一回cronで起動し「昨日の再生リスト」を生成することを想定していますが、作ろうとしている名前と同名の再生リストをがあったら新規作成しない、既に登録済みの動画があった場合は登録しない、という制御を自前で入れています。自分で制御しないと重複した名前の再生リスト作ったり、同じ再生リストに同じ動画が複数入ったりするので注意が必要です。デバッグ中は何度も実行するので、この辺の制御入れとかないと手動で消す羽目になります(n敗)。　また、しっかり受け取るパラメータをFieldプロパティで明示して制限しないとあっという間にクォータ上限いっちゃって次の日までデバッグできないのも注意です(n敗)。　余談ですが小規模な趣味開発では「GetOrInsertPlaylist」みたいな複数の関心事をモリモリやらせてるんだろうなぁ！みたいなメソッドを躊躇せず作ります。小規模で誰にも引き継がないプロジェクトであれば結局そっちのが実装が早くかつメンテナンスしやすいと思ってます。　こちらも素直にnugetから CoreTweet をインストールしてAPI叩きます。こっちもTwitterでの開発者登録が必要です。ググってください。　バーチャルYouTuberさんたちって日々こんなに動画あげてるのかー、生放送やってるのかーってのを日次で知ることが出来るようになりました。大きな収穫です。めちゃめちゃ多い。なるほど手動じゃ追いきれんわな。　また、ヘッドレスモードのChromeを用いたクロールやらYouTubeのRSSの取得、再生リストの作成、動画の登録などなど、今まで触ったことのなかった技術やAPIにたくさん触れることが出来て楽しかったです。メモリ512MBのLinux上でヘッドレスモードのChrome動かせるってのが分かったのも収穫。今後のクロール手札が増えました。　実はもともと非コンテナアプリとしてAzure Functions向けに作ってたのですが、ヘッドレスブラウザを利用する必要が出てきたため断念してコンテナ化。コンテナをAzureで動かすにはまだランニングコスト10円/月とかは難しいって認識で、以前つから使ってたConoHaのVPSに統合する形となりました。Azure Storageを利用しているのはその頃の名残です。知らないAzureのコンテナ動かすサービスありそうだなぁ。。　ちなみに推しはポン子です。


