More than 3 years have passed since last update.Microsoftサポートに確認し、新たに判明した仕様などを更新。「（2017/06/02追記）」を赤字で入れました。Transrator Speech APIはMaicrosoft社のAzureのCognitive Serviceの1APIとして提供されています。音声送れば音声を解析し、その結果を翻訳してテキスト、合成音声で結果を返してくれます。
なかなか楽しい機能なのですが、気軽に試そうとしていろいろ壁に当たりました。まだTransrator Speech APIについての情報があまり見つからない（探すのが下手なのかもしれないですが。。。）ので自分が四苦八苦したあたりを備忘録に残します。最終的にはHoloLensに搭載のやり方も含めて書きます。Microsoft AzureのCognitive Serviceの1APIでTransrator Speech APIとして提供されています。おそらく個人ではF0,S0のいずれかになると思うんですが、S0使う場合青天井で課金されますので注意してください。課金のルールですが、Microsoftサポートに確認したところWebsocketのオープンからクローズの時間に対して課金されます。現時点では送ったデータ量ではないとの回答をもらっています（2017/06/02追記）。公式にある仕様は以下のサイトに書かれています。
http://docs.microsofttranslator.com/speech-translate.html
以下はサイトの情報と、試行錯誤した部分を説明しています。Translator Speech APIの機能の中に対応している言語の情報として以下のものをHTTP通信で取得可能です。
資料はこのサイトになります。
http://docs.microsofttranslator.com/languages.html
* Speech-to-text: 音声からテキスト翻訳できる言語の種類(speech)
* Text translation: テキストの翻訳できる言語の種類(text)
* Text-to-speech: テキストから音声に翻訳できる言語の種類(tts)
カッコ内文字列は通信時のパラメータ「scope」で渡す値になります。これによりAPIがサポートする言語が返ります。組み合わせは、翻訳元の言語と翻訳先の言語を取得した言語で任意に構成できます。このAPIはWebSocketでストリーミングサービスとして提供されています。基本的な処理方法としては以下の通りです。
1. Translator Speech APIと接続を確立する
2. 翻訳したい音声データをストリーミングで送信する
3. 送信した音声データを文章単位で非同期にレスポンスが返ってくる
ここではまったのが2.の手順です。どうもリアルタイム翻訳を前提にしたAPIだからなのか一度に大量のデータを送信すると処理してくれません（しかも送ったデータ分は課金対象かもしれない。。。）。
ストリーミングだから当たり前といえばそれまでなのですが、例えば録音済みのデータを一気に渡すというやり方はうまく動かないです。録音済みの場合でも小出しに送る必要があります。音声はwaveを使います。マイクからの音声入力考えるとwaveの方が扱いやすいという理由です。接続確立してからサービスにデータを送るのですが、最初にwaveのHeaderデータを送信します。以降は音声をサンプリングしたデータをひたすら送ります。
Headerの仕様は一般的なwaveファイルとほぼ同じなのですがストリーミングで送信するためデータサイズは不定になります。
このため音声データサイズに関する部分はサイズ0として送信します。
データのサンプリングレートは以下で設定してください。
* サンプリングレート:16kHz
* 量子化ビット:16bit
* チャネル:1
でおそらくこのTranslator Speech APIはwaveに関しては上記のデータしかうけつけない可能性があります（これ以外のレートではうまくやり取りできない）。
Microsoftサポートに確認したところ現時点では16Khz,16bit,1channelしかうけられない仕様との事です（2017/06/02追記）。基本的には文単位になります。文章の切れ目は一定時間の無音で判断するようです。リアルタイム翻訳であれば、間が開けば勝手に解析始めるという感じです。データの受信については翻訳が完了したデータ（文単位）から順次送られてきます。デフォルト設定のままであれば、翻訳結果は文単位で一括で送信されます。翻訳途中を一定間隔で返させることも可能です（いるのかちょっと不明）。
データは以下の順で返信されます。
1. 翻訳後のテキストデータ(JSON)
2. 翻訳後の音声データ(バイナリ)
2.の音声データについてはテキスト→音声、または音声→音声の組み合わせでの翻訳の場合に送られてくるはずなのですが、なぜか要求しなくても返ってきてるような気がします。不要であればJSON以外は捨てれば一応処理はできます。サンプルコード交えて音声を入力して翻訳後のテキストと音声を返す説明します。
基本的な作業の流れは以下のサイトに書かれている通りです。
http://docs.microsofttranslator.com/speech-translate.htmlソースコードは以下のgithubに公開しています。最終的にはHoloLensのアプリもここに公開する予定です。今回は、HoloLensに使うライブラリとしての整備もかねているので、以下の環境で実行しています。各ライブラリはすべてNugetで取得可能です。HoloLensで使用する場合はUWP版のプロジェクトをUnityからビルド後に参照して利用します1。
なお、もともとはUnity上で実装するつもりで.NET 3.5版を作成していたのですが、Unityのビルド時点でUWPに変換できなったです。結局のところUnityでビルドが通る場合でもUWPの変換に失敗するシチュエーションがあるようでもう少し整理が必要といった感じです。せっかくなのでコードは残しています。処理の流れは以下になります。以降の説明はUWP版のソースコードで説明します。.NET版についても利用するライブラリが少し違うだけで行っていることは同じですのでこの説明を見ながらコード眺めれば処理は理解できると思います。最初にCognitive Serviceの認証トークンを取得します。認証トークンは予めAzureで追加したTranslator Speech APIのSubscriptionKeyを認証サービスに送信します。サービスの接続先はhttps://api.cognitive.microsoft.com/sts/v1.0/issueTokenになります。クエリパラメータ以下の通りです。リクエストを送信すれば、レスポンスとしてトークンを取得できます。Translator speech APIがサポートしている言語の種類などを取得します。ここで取得した情報からTranslator speech APIで変換する言語の組み合わせを指定します。Translator speech APIでは、３つのパラメータ「to/from」,「voice」を指定します。
とりあえず使うのであれば、以下の設定を利用してみてください。この設定を使うことで英語→日本語の翻訳は可能です。サポート言語については以下のサービスに問い合わせます。なおこの機能はトークン
不要です。こちらもHTTP通信で取得します。サービスの接続先はhttps://dev.microsofttranslator.com/languagesになります。クエリパラメータは以下の通りです。サンプルコードではそれぞれのscopeに分けて取得しています。以下のコードはspeechのデータをとるためのロジックです。UWPであれば、HttpClientクラスでPostAsyncメソッドを使用します。データはJSON形式の文字列表現となるので、解析してデータに格納しています。サンプルコードではUWP標準のJSON部品を使用しています。今回使っているJsonObjectは型不定の場合に自分で解析して処理するためのものです。構造わかってるならJSONシリアライザやほかのOSSのJSONライブラリでもいいと思います。
所々で出てくるasync - awaitのキーワードですが、これはC# 5.0から導入された非同期関連の機能です2。scopeは複数設定可能でカンマ区切りで送ればまとめてJSON形式で結果が返ります。形式は以下のようなものです。必要な情報が集まったら、translator speech APIに接続を行います。UWPでWebSocket用に提供されているクラスはStreamWebSocketとMessageWebSocketの２種類ありますが、今回はMessageWebSocketクラスを使用します。
使い方はすごく簡単で、インスタンス化の後初期設定を行い接続すれば完了します。最初に引数なしでインスタンス化を行います。そのあとHeader情報に認証情報を付与します。これは先ほど取得した"Bearer "+認証トークン文字列をAuthorizationヘッダに付与するだけです。あとはAPIの接続先としてwss://dev.microsofttranslator.com/speech/translateと翻訳する言語の設定として以下のクエリを設定します。なお、ヘッダ情報を書き換えできないライブラリを利用する場合は、Authヘッダの代わりにクエリとしてaccess_tokenで認証トークンを渡せば正しく処理されます（詳細は公式資料を参照）。音声データの送信については、Waveファイルを同じ形式でデータを送信します。まず最初にwaveのヘッダー情報を送信し、続いて音声データを送ります。
ヘッダについてはMicrosoft社のドキュメントに書いている通りです。ロジックで表現すると次のような形になります。先にも書きましたが、16000Hz,16bit,1channelでヘッダはセットしてください（送るデータもこのレートになるようにします）。WebSocketにデータを送信する場合はMessageWebSocketクラスのOutputStreamプロパティにデータを書き込めば非同期で送信されます。書き込む毎にデータを送信するため非効率なので、定期的にこのStreamに書き込むためにバッファリングできるラッパークラスを利用します。UWPではDataWriterクラスがそれです。このクラスはWriteメソッドでデータをバッファリングし、StoreAsyncメソッドで元のStreamクラスに対してバッファリングされたデータの書き込みを実施するという便利なものです。このStoreAsyncメソッドを一定間隔で呼び出すようにタイマーで制御しておき、音声データについてはDataWriterのWriteメソッドで随時書き込めば非同期でリアルタイムにデータを送信する仕組みが実現します。DataWriterのラッピングについてはMessageWebSocketのOutputStreamを引数に渡すだけで実現できます。定期的にバッファリングしたデータをOutputStreamに書き出すために、System.Threading.Timerクラスを使用します。このクラスは指定されたメソッドを一定時間後に一定時間間隔で実行することができます。上記の実装では250ms後に１回だけ実行するように設定しています。250ms後にs=&gt;{}内の処理が呼び出されるのですが、この最後のところで、250ms後に１度実行するようにTimerクラスの再設定を行うことで250ms毎に繰り返し実行するようにしています。サンプルコードには何かのWaveファイルを読み込んでサービスに書き込むロジックも記載しています。waveファイルを扱いやすくするためにNaudioライブラリを使用しています。後述の受信データの再生にもNaudioを使用しています。
waveのRAWデータを取得して、量子化ビットに合わせてサンプリング値に変換しサーバに送信しています。データは小出しに送るために適当な間隔で少しずつ送るロジックを記載しています。データの送信が問題なく行われて解析が実行されると処理結果が返ってきます。合成音声での翻訳結果を受け取る場合は２回レスポンスがあります。
１回目は送信データの解析結果とその翻訳後の情報がJSON形式で取得できます。２回目のデータはバイナリのwave形式の音声データとなっています。サンプルは音声データをNAudioに渡すことで音声出力しています。
データの振り分けについてはUWPのライブラリはかなり簡単です。メッセージ受信に発生するイベントのMessageWebSocketMessageReceivedEventArgsクラスにはMessageType プロパティというメッセージの種類がわかるものがあります。これでバイナリかどうか判断すればテキスト形式のデータか音声データか判別可能です。なお、ほかのWebSocket部品を利用する場合は最初の４バイトが"RIFF"のデータになっているかを確認すればOKです。もしRIFFなら音声データが受信できているので音声処理を行います。テキストデータについては以下のような形式でデータが返ってきます。必要な部分を取り出して利用する形です。サンプルコードは特に加工せずにデータを出力します。音声データはwaveフォーマットに従って送られてきます。ですので例えばファイルで保存すればそのまま再生できます。今回のサンプルではファイル出力なしにNaudio使って再生しています。Translator Speech APIの仕組みがまだあまり情報としてないので、分量は多くなりましたが何かの参考になれば幸いです。基本的にはこのUWPのライブラリを使いHoloLensのマイクからリアルタイムで集音した音声データをTranslator Speech APIに送信し、結果を文字でHoloLensに表示することで今回のものは実現しています。また、ライブラリ自体は使いまわせるはずなのでいろいろなアプリにも組み込めると思います。
その2ではHoloLensからリアルタイムで音声を集音する方法を書きたいと思います。
ほんとはその2でHoloLensで実装にするつもりでしたが、おそらく音声のリアルタイム集音はそれだけで何かに使える可能性が高いので別途整理したいなと思った次第です。Visual Studio 2015 update 3 Community EditionでUWPのプロジェクトを作成するとMicrosoft.NETCore.UniversalWindowsPlatformのバージョンは5.2.3です。 ↩asyncはメソッドの修飾子で「非同期処理を伴っている」メソッドであることを示すものになります。この場合のメソッドも呼び出し元では非同期で処理され、戻り値はTaskクラス（戻り値なし）またはTask&lt;戻り値の型&gt;クラスで定義します。呼び出す場合は普通のメソッドと同じ呼び出し方をするだけで非同期で実行されます。このメソッドを呼ぶときに結果を待つ必要がある場合は呼び出し時にawait を付けます。awaitを付けると不思議なことにジェネリックで指定した型が返ってきます。注意点としてawaitを付けた処理を書いたメソッドはasync修飾子が必要になります。awaitを使えない事情がある場合（たいていUnityと連携したときに起きる）はMethodAsync().GetWaiter().GetResult()と書くと同期的に動作します。ただし、同期的に書いたときはスレッドが完全に待ち状態になりこれが仇になるパターンが多いです。非同期処理を一切使っていない自分のライブラリを非同期用に提供するようなパターンでは大丈夫そうですが、.NETの提供する非同期メソッドで同期処理書くとデッドロックっぽい動きしてる気が。。。（よく遭遇するので、同期にして異常に待ちが発生するのであれば同期処理はやめてフラグで待つなど原始的な方法に逃げたほうが幸せになるかもです） ↩voice指定時のみ ↩


