この記事は、OpenVINOをC#で使うための記事です。
内容としては、過去に書いたC++で実行するOepnVinoによる手書き文字認識(MNIST)の内容を、最新バージョン(2021.2)向けに更新したものになっています。
また、それだけだとただの焼き直しになるので、C++とPythonのインターフェースしか用意されていないOpenVINOをC#から使えるようにしてみました。
説明はしていませんが、dllをC++から使うアプリも一緒に公開していますので、必要があればそちらも見てください。この記事は以下5つのパートに分かれます。学習済みモデル(識別器)の生成
Kerasというフレームワークを使って学習済みモデルを生成します。
内容としてはC++で実行するOepnVinoによる手書き文字認識(MNIST)に沿っていますが、フレームワークをChainerからKerasに変更しています。学習済みモデルをOpenVINOが扱える形式に変換
Kerasで作成した学習済みモデルをOpenNINO付属のModel Optimizerで変換します。
内容としてはC++で実行するOepnVinoによる手書き文字認識(MNIST)に沿っています。OpenVINOを使って推論するdll(C++)を作成
OpenVINOを扱うクラスを実装します。
内容としてはC++で作成したDLLでAPIではなくクラスを提供する方法に沿っています。SwigでC#向けのインターフェースを追加
3.で作ったdllにSwigを使いC#向けのインターフェースを追加します。
内容としてはC++で作成したdllにSwigを使ってC#向けのインターフェースを追加してC#から呼び出す方法に沿っています。SwigでC#向けインターフェースを追加したdllを使って推論するアプリ(C#)を作成
C#で作ったdllを使うアプリを作ります。
内容としてはC++で作成したdllにSwigを使ってC#向けのインターフェースを追加してC#から呼び出す方法に沿っています。このエントリを書いている私の環境は以下です。CUDAとcuDNNはお使いのtensorflow-GPUのバージョンで変えてください。
ここに対応表があります。近年、KerasやPyTorchなどのDeep Learningフレームワークが登場したことにより、手軽に個人でDeep Learning(学習フェーズと推論フェーズ)を実行できるようになりつつありますが、個人では以下の理由により製品適用レベルの学習済みモデルを作成することはまだハードルが高いです。また、学習済みモデルさえ用意できれば推論フェーズは個人で十分に実行可能ではあるのですが、推論自体も処理負荷が軽いわけではないために非力なデバイスで実行すると思った通りの速度がでない、もしくはCPU負荷が100%付近に張り付きユーザービリティの低下を招くという状況になってしまいます。
そのため、解決策の一つして今回はIntel社のOpenVINOを使用します。
OpenVINOを使用することで、以下が可能になります。
- CPU以外のデバイス(iGPU, VPU, FPGA)での推論
- CPUで実施する推論処理の高速化具体的には、CPUが非力で推論を実行するとCPU負荷が100%になってしまう場合、VPU(Movidius Neural Compute Stick)を接続してVPUで推論することによりCPUには負荷をかけずに推論というようなことが可能になります。なお、OpenVINOはIntelが出しているため、上で記載しているCPU、iGPU、VPU、FPGAとはIntel製のものがターゲットになります。今回はMNISTという手書き数字(0～9)を学習させて、入力された数字が0～9のどれなのか当てるという分類問題とします。
データ自体はAPI一発でダウンロードできるため、簡単に利用することが可能です。
学習・検証には公開されているデータを使い、テストには私がペイントで作ったデータを使うこととします。ニューラルネットワークは、入力層と隠れ層(中間層)、出力層を決める必要がありますが、その前にNCHWの説明をします。
NCHWとはそれぞれ以下を示します。NCHW：1x3x224x224という感じで使われる用語なので覚えておいてください。
さて入力層ですがMNISTは手書き数字のデータセットで、1枚は28x28のグレースケールですので素直に作るとNx1x28x28となります。
が、今回はわざわざカラー画像として入力したいと思いますのでNx3x28x28となります。
Nは実際に学習させる時にメモリに載るサイズまで増やしていきましょう。
隠れ層は、各自自由に作ってみて精度が出なければ層を深くするなり、畳み込みをするなり、工夫してみてください。
最後の出力層ですが、最終的に入力した画像が0～9のどれなのかを識別したいので、Nx10となります。今回はJupyter Notebook上で試していますのでそれぞれのセルごとに説明していきます。
なおMNIST-Keras.ipynbというファイル名でここに格納しています。Kerasなど必要なものをimportしています。
ここでエラーになる場合は必要なパッケージが足りていませんので、エラーに従って適宜入れてください。GPUが接続されているかチェックしています。
GPUの接続が確認できればisGPUをTrueと、確認できなければFalseとしています。ここでは3つのポイントがあります。
- チャネルを1チャネルから3チャネルへ
　　これはテスト用画像をペイントで作る時にグレースケール画像(1チャネルの意味)を作れないからです。
- isGPU==FalseならNCHWからNHWCへ
　　CPUではNCHWに対応していないため。
　　なお、GPUはNHWCにも対応していますが、速度が遅くなります。
- 画像の取りうる値を[0,255]から[0.0, 1.0]へ
　　精度を上げるため。ネットワーク全体は上記としました。
上記ネットワークはCPUで実行すると時間がかかりますので、GPUがない方はGoogle Colabを使うなり、時間がかからないようにネットワークを変更するなりしてください。
最適化関数はAdamとしています。Epochを進めて検証用データでのlossとaccuracyが改善していることを確認してください。実際にペイントで作った3x28x28の画像を入力し、それぞれどの数字と予測したのかを表示しています。
今学習したモデルで正解しているかを確認してください。まず、普通にKerasの機能で学習済みモデルを保存(mnist.h5)しています。
ただしh5形式の学習済みモデルはOpenVINOでは使えないため、ONNX形式かpb形式に一旦した後にIR形式というOpenVINO専用の形式にする必要があります。
なお、NHWCをONNX形式で保存してもうまくIR形式に変換できなかったので、NHWCの場合はpb形式で保存するようにしています。これで学習済みモデルを作成する部分は完了です。OpenVinoはONNX形式の学習済みモデルを動かすことはできず、IR形式と呼ばれる専用の形式で記述された学習済みモデルでしか動作させることができません。そのため、ONNX形式をIR形式に変換する必要があります。ここが皆さんが遭遇する最初の山かもしれません。公式ページに沿ってやっていきましょう。公式ページにそのものずばりがありますので割愛します。
公式ページ以外ですと、以下2つを紹介しておきます。
- OpenVINO 2021.1 環境構築（Windows 10編）
- OpenVINO (2019.R1) Windows10版のインストールとサンプルのテスト公式ページによると、"setupvars.bat"を実行する必要があると記載されいるので、さっそく実行します。次に変換に必要なパッケージをインストールしますが、ONNX形式をIR形式にするのか、それともpb形式をIR形式にするのかで必要となるパッケージが異なります。ONNX形式の場合は以下を実行してください。pb形式の場合ですが、今回はKeras(Tensorflow2)でpb形式の学習済みモデルを作ったので、install_prerequisites_tf2.batを実行することになります。ようやくIR形式に変換していきます。
このページを参照しながら変換していきましょう。上記ページをみると、以下のコマンドで変換ができると記載があります。また必要なオプションも記載してありますので各自試してみてください。
最終的に私が点けたオプションは以下の通りです。--data_type FP16は1つのモデルでCPU、GPU、VPUでの推論を可能にするためです。
各デバイスで対応しているdata_typeは以下ページ内のSupported Model Formatsセクションに記載されています。
Supported Devices--scale_values [255,255,255]は入力された画像の値をコード上で255で割る手間を省くためです。ONNX形式時に実行したコマンドは以下の通りです。上記を実行するとC:\Work\に以下のファイルが作成されていることが確認できます。これでModel Optimaizerによる変換は完了です。
今までのコマンドを自動で実行するバッチをconvertModel.batというファイル名で置いていますので面倒な人はそっちを使っても良いです。ようやくIR形式の学習済みモデルが作成できましたので、推論をする部分を作っていきます。
推論用のコードはC++かPythonで書くことができます。
Pythonで説明しているQiitaのエントリは我ら(また勝手に"我らが"とか言ってすみません)がPINTOさんを筆頭に良質なエントリがありますし、私は最終的にC#を使って推論をしたいため、今回はC++推論用のdllを作り、それをC#で使うことを目指します。なにはともあれ、Inference Engineの説明ページを見てみます。
上記ページによると以下の流れで推論を実施することになります。順にコードに落としていきます。
今回はMyOpenVINOImplというクラスを定義して、そこに推論機能を実装してGetInstance()というAPIでMyOpenVINOImplクラスのインスタンスを取得して使うことを想定しています。
dll自体の作り方はC++で作成したDLLでAPIではなくクラスを提供する方法に沿っていますのでそちらを参照してください。
ここではOpenVINOのAPIを使う部分にフォーカスして説明します。
なお、不明な個所があったら、Inference Engineの説明ページを参照してください。ただの宣言ですね。ここではモデルの読み込みを実施しています。引数のmodelNameには.xmlファイルのパスを渡します。
そのため、学習済みモデルとして必要なものは.xmlファイルと.binファイルの2つです。
IR形式に変換する際に.mappingファイルが作成されていますが、不要です。
また、実はONNX形式もそのまま読み込めるようなので実装したんですが、未試行なので動くか不明です。ここでは入力と出力の形状(NCHWとか)とデータ型(FP32とか)を設定します。Pythonで学習させた時に形状とデータ型をどうしていたのか思い出して、それを指定するだけです。実行可能なネットワークを作成します。
特に何も考えずにこのまま実行しましょう。ここでは推論を実行するオブジェクトを作成します。
ここも特に何も考えずにこのまま実行しましょう。matU8ToBlob()でデータをセットしています。推論を実行します。
同期の場合はInferSync()を呼び出すだけで、推論が終了するまで待たされて結果が返ってきます。
非同期の場合はInferASync()を呼び出すとすぐ制御が返りますが、推論の実態はInferASyncLocal()でされて結果はコールバックで渡されます。
なお、事前に結果を返すコールバックを登録しておく必要があります。Visual Studioでビルドするのですが、そのままではビルド出来ないため以下を設定します。また、出力ディレクトリを以下に変更しています。C#側がdllを使う際に作成したdll(MyOpenVINO.dll)をコピーするのですが、dllが置かれている場所が異なるとコピーできませんの注意してください。ビルドに成功したのでdllが作成されましたが、どのようなAPIが使えるのか確認してみましょう。
Visual Studioに付属しているDeveloper Command Prompt for VS2019を起動してみます。
(お使いの環境によっては開発者コマンドプロンプトという名称かもしれません)Developer Command Prompt for VS2019を起動して、さきほど作成したdllが置いてある場所に移動してください。
今回はC:\Workに置いてあるとしています。
dumpbinというコマンドでdllが公開しているAPIを確認することが出来ます。
赤枠部分が公開しているAPIですが、GetInstanceという名前のAPIだけが公開されていることが分かります。Swigを使ってC#向けのインターフェースを追加する方法はC++で作成したdllにSwigを使ってC#向けのインターフェースを追加してC#から呼び出す方法を確認してください。
最終的なインターフェースファイル(.i)は以下となります。次に以下のコマンドを実行します。
なお、作ったインターフェースファイル、先ほどdllを作る際に作成したヘッダファイル(MyOpenVINO.h)が同じ場所にある前提です。これで同じ場所に以下のファイルが作成されます。作成された上記ファイルのうち、Swig_wrap.cppをMyOpenVINOプロジェクトに加えてビルドします。
なお、Swig_wrap.hがSwig_wrap.cppと同じ場所にあるのであれば、Swig_wrap.hを加える必要はありませんが、同じ場所にないのであればSwig_wrap.hも加えてください。
ちなみにexecSwig.batというファイル名でSwigを実行するバッチを作っていますので、それを実行しても良いです。Swigで作成したファイルを加えてビルドしましたので、dllが公開しているAPIがどう変わったのか確認してみましょう。
赤枠部分が公開しているAPIですが、ものすごくAPIが追加されていることが分かりますね。SwigでC#向けインターフェースを追加したdllを使って推論するアプリ(C#)を作成する方法はC++で作成したdllにSwigを使ってC#向けのインターフェースを追加してC#から呼び出す方法を確認してください。やることを簡単に書くと以下になります。なお今回作成するアプリはWPF App(.NET)としていますが、コンソールでも良いので必要あれば適宜変更してください。プロジェクトに以下のファイルを追加してください。以下となります。InferCallBack()でUI上に配置してあるTextBoxに推論結果を表示しています。Click_InferSync()はInferSyncボタンクリック時に実行されるAPIで、Click_InferASync()はInferASyncボタンクリック時に実行されるAPIです。あとは、UI周りをちょちょっと作成してください。コード自体は完成し、ビルドも出来るようになったのですがdllを動かすために必要なファイルがありますので、ビルド後のイベントを使ってコピーします。
以下のコマンドをビルド後イベントのコマンドラインに記載してください。これで動くようになったはずなので、実行してみてください。
以下のような画面が表示されます。
UI上にラジオボタンが表示されていますが、MYRIADはMovidius Neural Compute Stick 2が接続されていると選択できるようになります。OpenVinoを使って推論を実行してみました。
OpenVINOをC#から使っている記事はなさそうですので、同じことをしたい人の助けになれば幸いです。
誤記や認識間違いあればご指摘をお願いします。


