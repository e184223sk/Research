More than 3 years have passed since last update.Microsoft Cognitive Servicesの視覚関連APIのうちEmotion API（感情認識API）のサンプルになります。現時点ではまだPreviewなので将来的に変更があるかもしれません。
検出できる感情は怒り、軽蔑、嫌悪感、恐怖、喜び、中立、悲しみ、驚きとなっています。
APIの構成は簡単で画像データをPOSTすると顔の位置情報と各感情にマッチする値を返すようになっています。サンプルではキャプチャーした画像を使って今どのような表情なのかを判定します。感情の種類によって顔の位置を示す枠の色を変えてみました。最初にEmotion APIを使用するためAzureに登録します。・Azureダッシュボードから「新規」-「AI + Cognitive Services」-「Emotion API」を選択します。見つからない場合には「すべて表示」をクリックしてください。
・「作成」をクリックします。
・「名前」、「場所」、「価格レベル」、「リソースグループ」等を入力して「作成」ボタンを押します。現在のところ「場所」は「米国西部」のみになります。正式リリース時にはもう少し増えるでしょう。
・「アクセスキーを表示...」をクリックします。・キー１もしくはキー２をメモしておきます。
以上でAzureでの設定は終わりです。・VisualStudio2017でUWPを作成します。「新規プロジェクト」-「空白のアプリ（ユニバーサルWindows）」として開きます。・「ソリューションエクスプローラー」-「参照」を右クリックして「Nugetパッケージの管理」を選びます。
・検索ボックスに「emotion」と入力します。検索結果の「Microsoft.ProjectOxford.Emotion」をインストールします。
・次に「MainPage.xaml」にCaptureElementを1つ、Imageを1つ、Canvasを1つ、TextBoxを1つ、AppBarButtonを1つを配置します。
・クライアントの宣言
サブスクリプションキーを引数としてクライアントオブジェクトを作成します。・クライアントオブジェクトに画像データを渡すと結果が返ってきます。以上　Emotion APIのサンプルプログラムになります。ソースコードはGitHubにアップロードしています。ダウンロードして試してみてください。


