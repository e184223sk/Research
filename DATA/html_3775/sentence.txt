ディープニューラルネットワークを使用して、話し言葉のアクセントとイントネーションに関する従来の音声合成の限界を克服します。韻律予測と音声合成が同時に行われるため、より滑らかで自然な音声出力が得られます。ニューラル音声を使用すると、チャットボットや音声アシスタントとの対話をより自然で魅力的なものにすることができます。[ 引用 ] https://docs.microsoft.com/ja-jp/azure/cognitive-services/speech-service/text-to-speech#core-featuresこの記事では、日本語で女性のニューラル音声であるNanamiがテキストを読み上げてくれます。
以下のMicrosoftドキュメントを参考に作成してみました。Cognitive Servicesのリソースを作成する際に、リージョン選択に注意してください。
かなり重要なことですが、ニューラル音声を使用できるリージョンが限られています。この記事を日本人が見る場合が多いと思うので、おそらく東日本もしくは西日本のリージョンを選択すると思いますが、なんとニューラル音声がサポートされていません！！！
ニューラル音声がサポートされているリージョンに関してはMSドキュメントで確認してください。
私は米国東部のリージョンを選択して、リソースを作成しています。フィールド変数のsubscriptionKeyとserviceRegionに前提で取得しておいたキーと場所に書き換えてください。
実行すると、”こんにちは”とNanamiが話してくれるかと思います。以上です。ありがとうございました。


