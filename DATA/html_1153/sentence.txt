まずはデモです。
やってることは見たまんまの位置同期ですが、
ハンドトラッキングの実装はOculusIntegration内に存在するOVR系のコンポーネントを
理解する必要があり、私のレベルでは非常に面倒でした。しかし、一度理解してしまえば使い回すだけなので、
同じ苦労をする人が一人でも減るようにメモしておきます。Unity2019.3.10f1
Oculus Integration 1.49
PUN2　Version 2.19.1まずはOculusIntegration内に存在するOVR系のコンポーネントがどのような役割を持ち、
どのような流れでハンドトラッキングを行っているかを理解していきます。簡単に言うと下記です。
①手を認識
②ボーンとなるオブジェクトを生成
③手のメッシュを作成
④手のメッシュにボーンを設定
⑤生成したボーンを認識した手の関節の座標に合わせるおおざっぱに理解したにすぎないので、間違いがあったらコメントください。OVRHandが手を認識しているというのは詳細を言うと少し誤った表現です。もう少し正確に言うと、
OVRHandが認識した手のデータを受け取って様々なクラスに
インタフェース経由でデータを渡しているという説明になるかと思います。もっと辿っていくとOVRPluginというクラスが存在しており、
デバイスが手を認識した際のデータを
C#で利用できるようにするラッパークラスとしての役割を担っています。OVRSkeletonのコードを見ていくと手のボーンを生成するコードを見つけました。この処理によって、動的にボーンとなるオブジェクトが生成されます。
このBonesの子階層にあるオブジェクトはEditor上で確認すると
手の動きに追従して回転しているのが確認できます。
(InspectorでShould Update Boneにチェックを入れた場合)【参考リンク】：【Unity】Oculus Link使ってEditor上でデバッグOVRMeshのコード内で手のメッシュを生成しています。
BoneWeightの設定も行っています。OVRMeshは生成したメッシュをSkinnedMeshrendererに設定します。
PlayModeを押すとMeshが動的に生成、設定されているのがわかります。
先ほど生成したMeshにボーンを設定します。
正確に言うと、SkinnedMeshrendererの持つMeshのボーン情報に設定します。このコード内ではメッシュのデフォルトの位置となるbindposesも設定しています。
Meshの持つボーンのデフォルトの位置を設定することで、
ボーンの移動した値とデフォルト値の差分から計算が可能になるそうです。最後に再度OVRSkeletonの登場です。自身で生成したBonesを認識した手の関節情報にそれぞれ追従させます。ハンドトラッキングの一連の流れが明らかになったので、
いよいよ同期処理を考えていきます。オブジェクト同期と呼ばれる手法を用います。
流れとしては下記イメージです。おおざっぱではありますが、こんな感じです。ここまでの理解でもかなり骨が折れましたが、本当に大変なのはここからでした。一連の流れを見ればわかりますが、
手の見た目のみの役割を果たす同期用オブジェクトを
用意する必要があります。やり方としては、2つの選択肢があります。
1つは、事前にボーンとなるオブジェクト及び、手のメッシュを用意することです。実際に下記の海外勢のサンプルでは、この手法を用いていました。
【参考リンク】：SpeakGeek-Normcore-Quest-Hand-Tracking(サンプルではPUN2とは別のNormcoreというライブラリを使用して同期の実装を行っています)下記GIFのように、あらかじめ同期するオブジェクトの中に
Bone及びBindPoseのオブジェクトがびっしりと用意されています。
ボーンの役割を担うオブジェクトはBindPoseも合わせると片手だけで全部で48個あります。
しかも、それぞれの座標が生成時に(0,0,0)ではないデフォルト値を持つので
自前で事前に用意するとなると、
生成時のすべての値を48×2回メモして一つずつ手打ち、、、もしくは
プレイモードで生成されたオブジェクトをそのまま保存できるスクリプトを用意する、、、
などなかなかの手間となります。さらに、プロジェクトを跨いで利用する際には
毎回オブジェクトをインポートする必要があるので少々効率が悪いです。そこで、もう一つのやり方として、
OVR系コンポーネントと同様に手の見た目のオブジェクトを動的に生成する方法を用います。しかし、既存のOVR系コンポーネントは切り離すことが困難な密結合な状態になっています。
ですので、手の見た目の役割を果たすオブジェクトを生成する処理を
OVR系コンポーネントから拝借して自前で用意する必要がありました。ここから実装の核心となるコードの説明です。
先ほどの同期実装の流れと合わせて見ていきます。この処理に関しては完全にOVR系コンポーネントに担ってもらいます。
OVRHandとOVRSkeletonをアタッチしたオブジェクトを各手のAnchorの子階層に配置します。
この処理に関しては長くなるので
・手の見た目のみの役割を持つオブジェクトを用意
・各自の手の位置情報に追従
の二つに分けて説明していきます。メッシュの生成に関してはOVRMeshをそのまま利用します。
生成するオブジェクトにSkinnedMeshrendererと共にアタッチしておきます。
ボーンの役割を担うオブジェクトの生成に関してはOVRSkeletonでの処理をほぼ丸パクリです。次にMeshの生成を行います。
ついでにMesh、SkinnedMeshRendererにBindPose、Boneの登録もそれぞれ行います。Bonesの子階層、すなわち指のボーンとなるオブジェクトから謎のリストを作成している理由は
次の 各自の手の位置情報に追従 で説明します。先ほど作成した謎の順番整理を行ったリストですが、各自の手の位置情報に追従させる際に
利用する上で都合が良いです。というのも、IOVRSkeletonDataProviderから渡ってきたボーン情報の順番が少し複雑だからです。
"Tip"と名の付く指先以外のボーンの位置情報が親指から順に列挙して送られてきたのち、
"Tip"と名の付く指先の情報が親指から順に送られてきます。下記コードで見るとより理解しやすいと思います。順番を整理したおかげで、for文を利用したボーンの情報をリストに順番通り取得してくる処理
が容易になっています。ここまでの処理で
手の見た目のオブジェクト
だけを同期オブジェクトとして実装することが可能となりました。この処理に関しては非常に簡単です。PhotonNetwork.Instantiateを使えばPUN2が自動で生成してくれます。コードに落とし込むと下記です。_avatarはPrefabをアタッチする必要があり、そのPrefabはAssets/Photon/PhotonUnityNetworking/Resources
に配置する必要があります。最後に同期オブジェクトの位置情報を共有する実装です。PUN2のIPunObservableを経由して送受信します。これでようやく同期が完了しました。ここまでの理解ですらかなりの時間を要しましたが、
最適化やUI/UXの面からまだまだ課題は多いです。今後も引き続きハンドトラッキング含め、同期に関して
調査しようと思っています。2021/03/22
プロジェクト公開しました。お役に立てば幸いです。→HandSync SkinnedMeshとBoneWeightについてメモ
PUN2で始めるオンラインゲーム開発入門【その５】


