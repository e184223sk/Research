More than 1 year has passed since last update.PFNよりIntel-MKLベースのONNXモデル推論ライブラリが提供されています。
本体はC言語で実装されており、それに各種言語のラッパークラスが幅広く用意されています。WindowsMLと機能が被りますが、WindowsMLはバグ抱えているし、
最近登場してきた(C#好きな私的には)大本命のMicrosoft製ONNX Runtimeもバージョンが0.1.5(2018/12/13時点)で当分は正式リリースされなさそうだしということで、Menohを活用して「実用的な推論器の実装」を勉強してみました。C# + .NetFramework4.5.1でVGG19による推論を実施するところを目標にします。
まんまexampleのコードなので困難なポイントはそこまでないと思います。
ただ、モデルの準備の話やコードの解説があまり見かけないので、その辺も含めて解説していきます。
あと私だけかもしれませんが、本家Githubのサンプルが何故か走らなかったので、同じ状況に陥った人たちへのフォローにもなれば幸いです。フォームアプリも良いですが手軽さ重視で、今回はコンソールアプリとして作成していきます。
作成したらまず初めにプロジェクトのプロパティを開いて、
ビルド＞プラットフォームターゲットを「x64」にします。32bitでは走りません。
あと、アンセーフコードの実行を許可しておきます。
次にnugetでMenohSharpを入手します。
（プロジェクトをダウンロードしてビルド＆関連ライブラリを自分で集めるでも可能です）
今回はModel ZooからVGG19を持ってきます。
他のモデルも使えるとは思いますが、Menohが対応していない演算が組み込まれたモデルは走らないので注意です。
もちろん、自分で作ったモデルでもOKです。
最近はSonyのNeualNetworkConsoleがONNX出力に対応したので誰でもコーディング無しで簡単に作れてしまいます。少し前までONNXモデルの中を確認するのが面倒でしたが、現在はNetronという便利なツールがあるのでそれを利用します。まずReleaseページからWindows用のバイナリを入手してインストールして、onnxファイルをNetronで開きます。
（関連付けされていればダブルクリック、そうでない場合はドラッグ＆ドロップで起動）以下のような表示が出ると思います。ここでモデルのプロパティを確認します。
ここで必要な情報は入出力の情報です。
したがって、Inputs
id: data_0
type: float32[1,3,224,224]Outputs
id: prob_1
type: float32[1,1000]この部分の情報を控えておきます。
MenohSharpではポインタを扱うことになるので、型もしっかり押さえておきます。この部分は特別なものは無いので本家のサンプルコードを見てもらうのが良いです。
サンプルコードでは以下の3つのことを実施しています。こんな感じのコードで推論できます。
所々で先ほどNetronにて確認した情報を入力していきます。ビルドして実行すればターミナルに結果が並びます。かなり簡単に推論を実行することができました。動作も早いです。
様々な言語から利用できることもあり、もっと広がっても良い気がします。
VGGに対応していたら実用上は多くのタスクに対応できると思いますし。現状はIntel CPUにしか対応していませんが、
今後のバージョンアップでどんどん使いやすくなる気がします。
シングルボードコンピュータではLattePandaあたりが使いやすそうですかね。個人的にはラズパイで使いたいので次はARM CPUに対応してくれると嬉しいです。
(2019.5.12追記)
ARMで動作するbranchがあるとのアドバイスをいただきました。
https://github.com/pfnet-research/menoh/tree/armnn/menoh/armnn
これを求めてました。今度トライしてみようと思います。暇があったら次は、Intel CPUもARM CPUもnVIDIA GPUにも対応しているMicrosoft製ONNX Runtimeあたりでも確認してみようと思います。
(2019.5.12追記)
ONNX Runtimeにトライしてみましたが、ラズパイ向けにビルド(Dockerコンテナを使ったクロスコンパイル)を行おうとしても上手く通らないので当分保留です。。。


