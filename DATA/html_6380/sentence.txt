　今回はロス関数とTensorに対する操作(detach, reshapeなど)を実装したいと思います。　実装でロス関数をBackward処理を行う際に　と呼び出したいのですが、そのままだとBackward処理が連鎖していかない(起点となる処理が必要)ので、Tensorクラスで現在Backward関数としている関数をBackwardChainに変更し新たにBackwardを追加します。　それではいくつか主となるLoss関数を実装していきます。基本的にはLossの関数をLambdaで計算した後SumやMeanを計算することになります　これは以下のような関数ですF.cs内部に直接LambdaFunctionとして実装していきます。HuberLossは以下のような計算を行います。　こちらも同様にF.csに加えていきます。　次はいくつかTensorの構造に作用する関数を実装していきたいと思います。Tensorの操作を行うメソッドでは基本的にShapeに作用するためDataの中身を変えないため、入力したTensorと同じインスタンスが出力されることとなります。　これはTensorの依存関係を切り離し、勾配の伝播を止める操作です。要は学習はさせないがネットワークの出力だけ欲しいという時に使う関数です。これをTensorの関数として実装したいのですが、一つ問題があります。
 例えば以下のような形式で使用するとします。　ここでTensor yは独立したBackFuncを持たないTensorとなるのですが、network内部ではxが入力された時に計算グラフが作られ保存されているので、これらの関係を解消するためには一々yからグラフを遡る必要が出てきます。
　そのため、残念ながらTensorの操作としてのDetach操作は断念せざるを得ません。
　そこで、代わりにBaseFunctionに「勾配情報を保存しないForward」を定義します。これをPredictとします。　まずIFunctionに対してPredictを追加します。　IFunctionに追加した関数の詳細をBaseFunctionで定義します。　これを使用することで、学習時に勾配を計算させないようにすることができます。PytorchのようにDetachをTensorの操作として呼び出したいなら、計算グラフの実装方法を変える必要があるようです。　SqueezeはTensorのある軸方向のサイズが1の時にその軸を消し次元を減らす操作で、
　Unsqueezeは逆に次元を増やす操作です。これらも同様に関数クラスとして実装しTensorから呼び出せるようにしておきます。　ReshapeでもSqueezeと同様にTensorのデータは変えずにShapeのみを入れ替えることになります。　ここまで実装したクラスのForwardをTensorから実行できるようにしておきます。　これでTensor側でいつでも操作できるようになりました。　今回は、ロス関数とTensorの操作関数を定義しました。ロス関数は他にもクロスエントロピーとかがよく使うと思いますが、現時点では使わなさそうなので必要になったら実装しようと思います。
　次はOptimizerの実装を行います。


