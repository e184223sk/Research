More than 3 years have passed since last update.前回、前々回で紹介した方法を使い、HoloLensでリアルタイム翻訳を実施します。構築したアプリケーションとしては、以下の機能を有するものです。以上のような簡単なものです。ただ、実をいうと音声出力はよくないと思います。というのも、スピーカーから出すとマイクで集音されてしまうのでただの雑音になり精度を下げる要因になります。お試しで使う形になるかと思います。動作させるとこのような形で動きます。とりあえずの最終版のHoloLensリアルタイム翻訳アプリの動作状況。現状のTranslator Speech APIが使えると思われる設定だと音声品質の問題で認識率が落ちてる気がします。#HoloLensJP pic.twitter.com/xBww80Ch6N環境については以下の通りです。前回、前々回でライブラリ化しているので、実態としてはそれをそのまま利用する形になります。ただ、音声については多少の加工を行っているのでそのあたりを紹介したいと思います。事前作業としてAzureのCognitive Serviceとして「Translator Speech API」の追加を行ってください。追加後SubscriptionKeyの控をお忘れなく。UnityのUIとしては3つのパラメータの設定、翻訳のスタート＆ストップです。
パラメータについてはそれぞれ以下の情報を設定します。合成音声を返さない場合は未設定とします。それぞれのパラメータは前々回の「2. サポート言語の取得」でえられるデータのIDに相当する値となります。
今回は適当にこんなUIにしてみました。
また、翻訳結果をテロップ表示するためのテキストも用意します。のちに作るMicrophoneInput.csの中で利用するものでサンプルでは「TranslateText」としてprefabにしています。表示後少ししてから上に移動しながらフェードアウトするように処理を入れています。上記はいずれもnGUIでつくりました。Canvasの設定については操作系はTagalongを利用しました。テロップはカメラ追従で実装しています。次にマイクからの集音処理部分の実装です。
UnityのHierarchyで「create」-「Audio」-「Audio Source」を選択し「Audio Source」を追加します。あと、Projectで「create」-「Audio Mixer」を選択し「Audio Mixer」も追加します。
まず、「Audio Mixer」-「Master」を選択し、Inspectorの設定項目「Volume」を-80dbに設定します。
次にHierarchyで「Audio Source」を選択し、Inspectorの設定項目「output」に先ほどの「Master」をドラッグ＆ドロップします。これはスピーカーから自分のマイク音声を出力しないための設定です。なお「Audio Clip」については後でプログラム側で設定するのでそのままにしておきます。
次に「Audio Source」の処理を書きます。Project内にC#Scpirtを作成します。
Startメソッドでは、前々回のUWPのサンプルコードを参考にTranslator Speech APIのサービスへの接続処理を書きます。
UWPで作ったプロジェクト（Com.Reseul.Apis.Services.CognitiveService.Translators.UWPプロジェクト）をそのまま利用するのが楽ですが、この場合一旦Unityでビルドを行いUWPに変換してから追記する必要があります。翻訳結果の表示については前々回のUWPのサンプルコードを参考にサービスの翻訳結果のうちJSONの内容を加工し「TranslateText」をインスタンス化して
画面に表示します。次にマイクからの集音開始と終了及びデータの送信部分を作成します。マイクの集音開始については以下のように実装します。このメソッドは先ほど作ったUIのStartボタンで呼び出されるように実装します。
処理内容としては前回と同じ実装内容になります。今回はHolotoolkit-Unityは使用せずにUnityのみで実現しています。マイクの集音終了はMicrophoneInput.ServiceStopメソッドで実装します。このメソッドは先ほど作ったUIのStopボタンで呼び出されるように実装しています。
こちらの処理内容も前回と同じ実装内容になります。次に集音データの送信を行います。処理はMicrophoneInput.OnAudioFilterReadメソッドで実装します。こちらも基本的な部分は前回と同じ実装内容になります。
異なるのは、以下の２点です。リサンプリングについては、HoloLensで集音すると48Khz,2chの品質でサンプリングされます。ですが、経験上（確認中ですが）Translator Speech APIは16Khz,1ch,16bitで送るのがよさそうなので、リサンプリングを行っています。リサンプリングは単純にデータを間引く対応を行っています。間引き方は単純で48/16*2=6個のデータ毎に平均をとって配列を再構成します。waveデータはfloatの配列で取得できるのですが、ステレオの場合、この配列の偶数が右チャネル、奇数が左チャネルとなります。今回はステレオからモノラルの変換も兼ねているので単純にまとめて処理しています。リサンプリングしたデータは16bitで量子化したうえでTranslator Speech APIに送信します。
なお、この辺りの処理が単純すぎるので認識率の低下につながってる可能性もあります（データ劣化を起こしている）。全てUnity上でできれば丸く収まるのですが、今回作成したものはUWPのライブラリですので、
UWP側で残りの実装を行います。前々回のUWPのライブラリを利用した場合以下の６つの実装を行います。ライブラリの初期化をTranslator Speech APIの接続を開始します。
この処理は録音開始のボタンの押下時に実行するように実装します。
ConnectメソッドにはUIで設定したfrom,to,Voiceを渡します。コンストラクタの引数にはAzureのサブスクリプションキーを渡します。
2,3行目は送信した翻訳データを受信した際に呼び出すメソッドを割り当てる処理となります。接続の完了後音声データを送信する処理は以下の通りです。Unityの方で音声データを集音した際にはデータは16bitのサンプリングデータに変換します。サービスへの送信についてはこれをByteの配列に変換します。データはリトルエンディアンになりますので注意してください。Translator Speech APIでサポートしている合成音声の情報を取得します。これはUnity上のUIで値を選択するための情報として利用します。ライブラリではメソッドを１つ呼ぶだけで実施可能です。なお、ライブラリのGetSpeechTtsInfoは非同期のメソッドとして定義していますので、
レスポンスを同期的に処理するためにはWaitメソッドを用いて待機します。Translator Speech APIでサポートしている言語の情報を取得します。これはUnity上のUIで値を選択するための情報として利用します。ここでは、翻訳後のデータが入ったJSONデータを解析してUnity側に描画します。JSONデータには音声から解析された原文と翻訳後の情報があります。今回は両方のデータを利用できるように文字列を加工しています。Unityへの描画についてはコメントの部分に記載を行います。ここでは、翻訳後の音声データとしてバイナリデータが送られた場合、スピーカーから発話します。音声周りはNAudioを利用して再生しています。
なお、発話するとスピーカーからの音声をマイクが集音することで雑音が増える結果になります。以上がHoloLensに組み込む際のポイントを残してみました。もう少しここはどうなってる？等ありましたら、コメントしていただければ返したいと思います。
なお、2017年5月からTranslator Speech APIが大幅値下げされています。お試しで使うとしてもかなりお得感があるので色々試してください。
私自身は、この翻訳アプリをDe:code 2017の基調講演で試行してみたいと思ってます。ただ、事前に映画見ながらも使っていたのですが「１．音声データの品質」「２．翻訳精度」の２つの問題から思ったようには翻訳されないことが多いです。かなり条件そろわないと難しい印象です。この辺り、他のTranslatorも少し使ってみようと思います。


