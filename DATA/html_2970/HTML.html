<!DOCTYPE html><html><head><meta charset="utf-8" /><title>Unityでディープラーニングする方法調べた - Qiita</title><meta content="width=device-width,initial-scale=1,shrink-to-fit=no,viewport-fit=cover" name="viewport" /><meta content="#55c500" name="theme-color" /><meta content="XWpkTG32-_C4joZoJ_UsmDUi-zaH-hcrjF6ZC_FoFbk" name="google-site-verification" /><meta content="telephone=no" name="format-detection" /><link rel="canonical" href="https://qiita.com/highno_RQ/items/7a33e7eaa6d77c93968f" /><link href="/manifest.json" rel="manifest" /><link href="/opensearch.xml" rel="search" title="Qiita" type="application/opensearchdescription+xml" /><link as="script" href="https://www.googletagservices.com/tag/js/gpt.js" rel="preload" /><link href="https://securepubads.g.doubleclick.net" rel="preconnect" /><script async="" src="https://www.googletagservices.com/tag/js/gpt.js"></script><meta name="csrf-param" content="authenticity_token" />
<meta name="csrf-token" content="KEexP1wsP+h56LQkyiU3OJgm+k+vwKdVCYATc9H9nOjPVXySILFEZJsz+M2/hzOtszPrzMVSD+k6j/Cy4HntoQ==" /><link rel="shortcut icon" type="image/x-icon" href="https://cdn.qiita.com/assets/favicons/public/production-c620d3e403342b1022967ba5e3db1aaa.ico" /><link rel="apple-touch-icon" type="image/png" href="https://cdn.qiita.com/assets/favicons/public/apple-touch-icon-ec5ba42a24ae923f16825592efdc356f.png" /><link rel="stylesheet" media="all" href="https://cdn.qiita.com/assets/public/article-2eaa7dbedc42a8ea65c722cda46d0ebb.min.css" /><script src="https://cdn.qiita.com/assets/public/v3-article-bundle-63de2d91fef827269d3f6b958db2335b.min.js" defer="defer"></script><meta name="twitter:card" content="summary_large_image"><meta content="@Qiita" name="twitter:site" /><meta content="@highno_RQ" name="twitter:creator" /><meta property="og:type" content="article"><meta property="og:title" content="Unityでディープラーニングする方法調べた - Qiita"><meta property="og:image" content="https://qiita-user-contents.imgix.net/https%3A%2F%2Fcdn.qiita.com%2Fassets%2Fpublic%2Farticle-ogp-background-1150d8b18a7c15795b701a55ae908f94.png?ixlib=rb-4.0.0&amp;w=1200&amp;mark64=aHR0cHM6Ly9xaWl0YS11c2VyLWNvbnRlbnRzLmltZ2l4Lm5ldC9-dGV4dD9peGxpYj1yYi00LjAuMCZ3PTg0MCZoPTM4MCZ0eHQ2ND1WVzVwZEhuamdhZmpnNGZqZ3FQamc3empnNWZqZzZuamc3empnNHZqZzdQamdyRGpnWm5qZ292bWxybm1zNVhvcXJfamdibmpnWjgmdHh0LWNvbG9yPSUyMzMzMyZ0eHQtZm9udD1IaXJhZ2lubyUyMFNhbnMlMjBXNiZ0eHQtc2l6ZT01NCZ0eHQtY2xpcD1lbGxpcHNpcyZ0eHQtYWxpZ249Y2VudGVyJTJDbWlkZGxlJnM9M2FkYjA3MTQwMjExMzM4MzQ4NjM2M2UxMDU0MWE2YmM&amp;mark-align=center%2Cmiddle&amp;blend64=aHR0cHM6Ly9xaWl0YS11c2VyLWNvbnRlbnRzLmltZ2l4Lm5ldC9-dGV4dD9peGxpYj1yYi00LjAuMCZ3PTg0MCZoPTUwMCZ0eHQ2ND1RR2hwWjJodWIxOVNVUSZ0eHQtY29sb3I9JTIzMzMzJnR4dC1mb250PUhpcmFnaW5vJTIwU2FucyUyMFc2JnR4dC1zaXplPTQ1JnR4dC1hbGlnbj1yaWdodCUyQ2JvdHRvbSZzPWFjNzNiYzYyNTNhZjViZjcyNDEyNzUzZTFiN2ExYjkw&amp;blend-align=center%2Cmiddle&amp;blend-mode=normal&amp;s=57c9cfe0ae60e4a553c9a47e09d75b27"><meta property="og:description" content="

2020/7/2追記

Barracudaの記事を書きました
この記事の手法より圧倒的に簡単なので、こっち利用推奨です！！
Unity Technologies製推論エンジン Barracudaがスゴイという話


2020/5/..."><meta content="https://qiita.com/highno_RQ/items/7a33e7eaa6d77c93968f" property="og:url" /><meta content="Qiita" property="og:site_name" /><meta content="564524038" property="fb:admins" /><meta content="C#,Unity,DeepLearning,TensorFlow,ONNX" name="keywords" /><script>!function(f,b,e,v,n,t,s)
{if(f.fbq)return;n=f.fbq=function(){n.callMethod?
n.callMethod.apply(n,arguments):n.queue.push(arguments)};
if(!f._fbq)f._fbq=n;n.push=n;n.loaded=!0;n.version='2.0';
n.queue=[];t=b.createElement(e);t.async=!0;
t.src=v;s=b.getElementsByTagName(e)[0];
s.parentNode.insertBefore(t,s)}(window, document,'script',
'https://connect.facebook.net/en_US/fbevents.js');
fbq('init', '668972150489891');
fbq('track', 'PageView');</script><style data-emotion-css="17jxvjw 11t2ec1 1dvr2p8 18lkoru 1g4cku8 1iupg5d ijvq0v 15cocm3 12rp90f 115f4t 1b8uj5v 79elbk 16hhh7b fcbn8c 1gj7nt 154zy0m yikrym 1jqivyb 1ode1bp le4d8r 1hbd3g7 1nzh4zz 38fzdi helsa7 8qb8m4 2imjyh he5w1s 70qvj9 3ojehk 100alwu 1dtnjt5 10ougpm 1ay9vb9 m19uds cgzq40 1wa99t2 1l3zk9f 4czcte 1yzj1fm 1uv1qiv 109dbrr 5jpx49 mnxgyc 1vlpknv fsjkhv 1b17vb0 7i7f4d"}>.css-17jxvjw{display:grid;display:-ms-grid;grid-template-columns:80px minmax(0,1fr) 300px;-ms-grid-columns:80px minmax(0,1fr) 300px;grid-template-rows:minmax(270px,auto) 1fr;-ms-grid-rows:minmax(270px,auto) 1fr;max-width:1280px;margin-right:auto;margin-left:auto;padding-top:24px;padding-right:24px;padding-left:24px;}@media (max-width:1200px){.css-17jxvjw{padding-bottom:0;padding-left:0;padding-right:0;}}@media (max-width:992px){.css-17jxvjw{grid-template-columns:80px 452px 300px;-ms-grid-columns:80px 452px 300px;-webkit-box-pack:center;-webkit-justify-content:center;-ms-flex-pack:center;justify-content:center;}}@media (max-width:770px){.css-17jxvjw{display:block;}}@media (max-width:480px){.css-17jxvjw{padding-top:0;}}.css-11t2ec1{-webkit-align-items:center;-webkit-box-align:center;-ms-flex-align:center;align-items:center;z-index:5;position:-webkit-sticky;position:sticky;top:calc(56px + 24px + 16px + 32px - 16px);display:-webkit-box;display:-webkit-flex;display:-ms-flexbox;display:flex;-webkit-flex-direction:column;-ms-flex-direction:column;flex-direction:column;padding:16px;width:80px;}.css-11t2ec1:after{content:'';display:table;}.css-11t2ec1:before{content:'';display:table;}@media (max-width:770px){.css-11t2ec1{display:none;}}.css-1dvr2p8{-webkit-align-items:center;-webkit-box-align:center;-ms-flex-align:center;align-items:center;-webkit-appearance:none;-moz-appearance:none;appearance:none;cursor:pointer;display:-webkit-box;display:-webkit-flex;display:-ms-flexbox;display:flex;-webkit-flex-direction:column;-ms-flex-direction:column;flex-direction:column;-webkit-box-pack:center;-webkit-justify-content:center;-ms-flex-pack:center;justify-content:center;outline:none;margin-bottom:16px;}.css-18lkoru{-webkit-align-items:center;-webkit-box-align:center;-ms-flex-align:center;align-items:center;border:2px solid #55C500;border-radius:50%;cursor:pointer;display:-webkit-box;display:-webkit-flex;display:-ms-flexbox;display:flex;height:40px;-webkit-box-pack:center;-webkit-justify-content:center;-ms-flex-pack:center;justify-content:center;outline:none;padding:0;width:40px;background-color:#FFFFFF;}.css-1g4cku8{display:inline-block;vertical-align:middle;height:20px;width:20px;fill:#55C500;}.css-1iupg5d{color:#55C500;cursor:pointer;font-size:14px;font-weight:bold;}.css-ijvq0v{-webkit-align-items:center;-webkit-box-align:center;-ms-flex-align:center;align-items:center;-webkit-appearance:none;-moz-appearance:none;appearance:none;display:-webkit-box;display:-webkit-flex;display:-ms-flexbox;display:flex;-webkit-flex-direction:column;-ms-flex-direction:column;flex-direction:column;-webkit-box-pack:center;-webkit-justify-content:center;-ms-flex-pack:center;justify-content:center;outline:none;margin-bottom:16px;}.css-15cocm3{-webkit-align-items:center;-webkit-box-align:center;-ms-flex-align:center;align-items:center;-webkit-appearance:none;-moz-appearance:none;appearance:none;background-color:#FFFFFF;border:2px solid #6E6F70;border-radius:50%;cursor:pointer;display:-webkit-box;display:-webkit-flex;display:-ms-flexbox;display:flex;height:40px;-webkit-box-pack:center;-webkit-justify-content:center;-ms-flex-pack:center;justify-content:center;outline:none;padding:2px 0px 0px;width:40px;}.css-12rp90f{display:inline-block;vertical-align:bottom;height:17.77777777777778px;width:16px;fill:#6E6F70;}.css-115f4t{color:#6E6F70;font-size:14px;font-weight:bold;}.css-1b8uj5v{display:-webkit-box;display:-webkit-flex;display:-ms-flexbox;display:flex;-webkit-align-items:center;-webkit-box-align:center;-ms-flex-align:center;align-items:center;-webkit-box-pack:center;-webkit-justify-content:center;-ms-flex-pack:center;justify-content:center;-webkit-appearance:none;-moz-appearance:none;appearance:none;background-color:transparent;border:none;color:#6E6F70;cursor:pointer;font-size:20px;height:32px;outline:none;width:32px;margin-bottom:16px;padding:0;}.css-79elbk{position:relative;}.css-16hhh7b{display:-webkit-box;display:-webkit-flex;display:-ms-flexbox;display:flex;-webkit-align-items:center;-webkit-box-align:center;-ms-flex-align:center;align-items:center;-webkit-box-pack:center;-webkit-justify-content:center;-ms-flex-pack:center;justify-content:center;-webkit-appearance:none;-moz-appearance:none;appearance:none;background-color:transparent;border:none;color:#6E6F70;cursor:pointer;font-size:20px;height:32px;outline:none;width:32px;padding:0;}.css-fcbn8c{display:none;bottom:initial;left:initial;right:initial;top:initial;left:100%;top:calc(-8px - (14px * 1.8) - 16px - 4px);}.css-1gj7nt{color:rgba(0,0,0,0.6);font-size:14px;font-weight:bold;line-height:1.8;padding:8px 16px;}.css-154zy0m{display:-webkit-box;display:-webkit-flex;display:-ms-flexbox;display:flex;-webkit-align-items:center;-webkit-box-align:center;-ms-flex-align:center;align-items:center;color:rgba(0,0,0,0.87);cursor:pointer;font-size:16px;font-weight:normal;line-height:1.8;padding:4px 16px;}.css-154zy0m:hover{-webkit-text-decoration:none;text-decoration:none;background-color:#F2F2F2;}.css-yikrym{width:20px;}.css-1jqivyb{color:rgba(0,0,0,0.6);font-size:13px;}.css-1ode1bp{background-color:rgba(0,0,0,0.12);height:1px;width:100%;margin:8px 0;}.css-le4d8r{display:inline-block;vertical-align:middle;height:13px;width:13px;fill:rgba(0,0,0,0.6);}.css-1hbd3g7{height:250px;}.css-1nzh4zz{display:-webkit-box;display:-webkit-flex;display:-ms-flexbox;display:flex;-webkit-align-items:center;-webkit-box-align:center;-ms-flex-align:center;align-items:center;padding:16px 32px;background-color:#FBE69E;color:rgba(0,0,0,0.87);line-height:1.5;font-weight:600;}@media (max-width:770px){.css-1nzh4zz{padding:16px;}}.css-38fzdi{color:#CA832A;margin-right:4px;}.css-helsa7{background-color:#FFFFFF;padding:32px;margin-bottom:24px;}@media (max-width:992px){.css-helsa7{margin:0 auto 40px;}}@media (max-width:480px){.css-helsa7{margin:0 0 40px;padding:32px 16px;}}.css-8qb8m4{margin-bottom:48px;}.css-2imjyh{display:-webkit-box;display:-webkit-flex;display:-ms-flexbox;display:flex;-webkit-flex-wrap:wrap;-ms-flex-wrap:wrap;flex-wrap:wrap;}.css-he5w1s{display:-webkit-box;display:-webkit-flex;display:-ms-flexbox;display:flex;-webkit-align-items:center;-webkit-box-align:center;-ms-flex-align:center;align-items:center;-webkit-box-pack:justify;-webkit-justify-content:space-between;-ms-flex-pack:justify;justify-content:space-between;width:100%;}@media (max-width:770px){.css-he5w1s{-webkit-flex-direction:column;-ms-flex-direction:column;flex-direction:column;-webkit-align-items:flex-start;-webkit-box-align:flex-start;-ms-flex-align:flex-start;align-items:flex-start;}}.css-70qvj9{display:-webkit-box;display:-webkit-flex;display:-ms-flexbox;display:flex;-webkit-align-items:center;-webkit-box-align:center;-ms-flex-align:center;align-items:center;}.css-3ojehk{display:-webkit-box;display:-webkit-flex;display:-ms-flexbox;display:flex;-webkit-flex-shrink:0;-ms-flex-negative:0;flex-shrink:0;margin-right:4px;}.css-100alwu{display:inline-block;border-radius:50%;line-height:1;overflow:hidden;vertical-align:middle;}.css-1dtnjt5{display:-webkit-box;display:-webkit-flex;display:-ms-flexbox;display:flex;-webkit-align-items:center;-webkit-box-align:center;-ms-flex-align:center;align-items:center;-webkit-flex-wrap:wrap;-ms-flex-wrap:wrap;flex-wrap:wrap;}.css-10ougpm{color:rgba(0,0,0,0.87);font-size:14px;font-weight:600;line-height:1.8;margin-right:4px;text-wrap:break-word;word-break:break-all;}.css-1ay9vb9{margin-right:16px;}.css-m19uds{color:rgba(0,0,0,0.6);font-size:14px;line-height:1.8;}.css-cgzq40{color:rgba(0,0,0,0.87);font-size:32px;font-weight:bold;line-height:1.4;margin-top:8px;text-wrap:break-word;word-break:break-all;}.css-1wa99t2{-webkit-align-items:center;-webkit-box-align:center;-ms-flex-align:center;align-items:center;color:rgba(0,0,0,0.6);display:-webkit-box;display:-webkit-flex;display:-ms-flexbox;display:flex;-webkit-flex-wrap:wrap;-ms-flex-wrap:wrap;flex-wrap:wrap;margin-top:8px;}.css-1l3zk9f{color:rgba(0,0,0,0.6);font-size:20px;margin-right:8px;}.css-4czcte{margin-right:4px;color:inherit;font-size:14px;line-height:1.8;}.css-4czcte:not(:last-child)::after{content:',';margin-right:4px;}.css-1yzj1fm{-webkit-align-items:center;-webkit-box-align:center;-ms-flex-align:center;align-items:center;display:-webkit-box;display:-webkit-flex;display:-ms-flexbox;display:flex;-webkit-flex-wrap:wrap;-ms-flex-wrap:wrap;flex-wrap:wrap;margin-top:32px;}.css-1uv1qiv{-webkit-align-items:center;-webkit-box-align:center;-ms-flex-align:center;align-items:center;-webkit-appearance:none;-moz-appearance:none;appearance:none;background-color:transparent;border:none;color:#6E6F70;cursor:pointer;display:-webkit-box;display:-webkit-flex;display:-ms-flexbox;display:flex;font-size:20px;height:32px;-webkit-box-pack:center;-webkit-justify-content:center;-ms-flex-pack:center;justify-content:center;margin-right:16px;outline:none;padding:0;width:32px;}.css-109dbrr{background-color:#F9F9F9;border-top:1px solid rgba(0,0,0,0.12);bottom:0;box-shadow:0px 1px 4px rgba(0,0,0,0.14);display:none;height:calc(env(safe-area-inset-bottom,0px) + 56px);-webkit-box-pack:center;-webkit-justify-content:center;-ms-flex-pack:center;justify-content:center;padding-bottom:env(safe-area-inset-bottom);position:-webkit-sticky;position:sticky;width:100%;z-index:2000;}@media (max-width:770px){.css-109dbrr{display:-webkit-box;display:-webkit-flex;display:-ms-flexbox;display:flex;}}.css-5jpx49{-webkit-align-items:center;-webkit-box-align:center;-ms-flex-align:center;align-items:center;display:-webkit-box;display:-webkit-flex;display:-ms-flexbox;display:flex;-webkit-box-pack:space-evenly;-webkit-justify-content:space-evenly;-ms-flex-pack:space-evenly;justify-content:space-evenly;width:100%;}.css-mnxgyc{-webkit-align-items:center;-webkit-box-align:center;-ms-flex-align:center;align-items:center;-webkit-appearance:none;-moz-appearance:none;appearance:none;cursor:pointer;display:-webkit-box;display:-webkit-flex;display:-ms-flexbox;display:flex;-webkit-flex-direction:row;-ms-flex-direction:row;flex-direction:row;-webkit-box-pack:center;-webkit-justify-content:center;-ms-flex-pack:center;justify-content:center;outline:none;}.css-1vlpknv{-webkit-align-items:center;-webkit-box-align:center;-ms-flex-align:center;align-items:center;border:2px solid #55C500;border-radius:50%;cursor:pointer;display:-webkit-box;display:-webkit-flex;display:-ms-flexbox;display:flex;height:40px;-webkit-box-pack:center;-webkit-justify-content:center;-ms-flex-pack:center;justify-content:center;outline:none;padding:0;width:40px;margin-right:4px;background-color:#FFFFFF;}.css-fsjkhv{-webkit-align-items:center;-webkit-box-align:center;-ms-flex-align:center;align-items:center;-webkit-appearance:none;-moz-appearance:none;appearance:none;display:-webkit-box;display:-webkit-flex;display:-ms-flexbox;display:flex;-webkit-flex-direction:row;-ms-flex-direction:row;flex-direction:row;-webkit-box-pack:center;-webkit-justify-content:center;-ms-flex-pack:center;justify-content:center;outline:none;}.css-1b17vb0{color:#6E6F70;font-size:14px;font-weight:bold;margin-left:4px;}.css-7i7f4d{display:none;bottom:initial;left:initial;right:initial;top:initial;bottom:32px;right:0;}</style></head><body><div class="allWrapper"><div><div id="GlobalHeader-react-component-01f55ef9-be80-4d35-af8c-3b8837bbfb6d"><div class="st-Header"><div class="st-Header_container"><div class="st-Header_start"><a href="/" class="st-Header_logo"><svg xmlns="http://www.w3.org/2000/svg" viewBox="0 0 426.57 130"><circle cx="167.08" cy="21.4" r="12.28"></circle><path d="M250.81 29.66h23.48v18.9h-23.48z"></path><path d="M300.76 105.26a22.23 22.23 0 01-6.26-.86 12.68 12.68 0 01-5.17-3 14.41 14.41 0 01-3.56-5.76 28 28 0 01-1.3-9.22V48.56h29.61v-18.9h-29.52V3.29h-20.17v83.34q0 11.16 2.83 18.27a27.71 27.71 0 007.7 11.2 26.86 26.86 0 0011.43 5.62 47.56 47.56 0 0012.34 1.53h15.16v-18zM0 61.7a58.6 58.6 0 015-24.21A62.26 62.26 0 0118.73 17.9 63.72 63.72 0 0139 4.78 64.93 64.93 0 0164 0a65 65 0 0124.85 4.78 64.24 64.24 0 0120.38 13.12A62 62 0 01123 37.49a58.6 58.6 0 015 24.21 58.34 58.34 0 01-4 21.46 62.8 62.8 0 01-10.91 18.16l11.1 11.1a10.3 10.3 0 010 14.52 10.29 10.29 0 01-14.64 0l-12.22-12.41a65 65 0 01-15.78 6.65 66.32 66.32 0 01-17.55 2.3 64.63 64.63 0 01-45.23-18A62.82 62.82 0 015 85.81 58.3 58.3 0 010 61.7zm21.64.08a43.13 43.13 0 0012.42 30.63 42.23 42.23 0 0013.43 9.09A41.31 41.31 0 0064 104.8a42 42 0 0030-12.39 42.37 42.37 0 009-13.64 43.43 43.43 0 003.3-17 43.77 43.77 0 00-3.3-17A41.7 41.7 0 0080.55 22 41.78 41.78 0 0064 18.68 41.31 41.31 0 0047.49 22a42.37 42.37 0 00-13.43 9.08 43.37 43.37 0 00-12.42 30.7zM331.89 78a47.59 47.59 0 013.3-17.73 43.22 43.22 0 019.34-14.47A44.25 44.25 0 01359 36a47.82 47.82 0 0118.81-3.58 42.72 42.72 0 019.26 1 46.5 46.5 0 018.22 2.58 40 40 0 017 3.84 44.39 44.39 0 015.71 4.63l1.22-9.47h17.35v85.83h-17.35l-1.17-9.42a42.54 42.54 0 01-5.84 4.67 43.11 43.11 0 01-7 3.79 44.86 44.86 0 01-8.17 2.59 43 43 0 01-9.22 1A47.94 47.94 0 01359 119.9a43.3 43.3 0 01-14.47-9.71 44.17 44.17 0 01-9.34-14.47 47 47 0 01-3.3-17.72zm20.27-.08a29.16 29.16 0 002.17 11.34 27 27 0 005.92 8.88 26.69 26.69 0 008.76 5.76 29.19 29.19 0 0021.44 0 26.11 26.11 0 008.72-5.76 27.57 27.57 0 005.88-8.84 29 29 0 002.16-11.38 28.62 28.62 0 00-2.16-11.22 26.57 26.57 0 00-5.93-8.8 27.68 27.68 0 00-19.51-7.9 28.29 28.29 0 00-10.77 2.05 26.19 26.19 0 00-8.71 5.75 27.08 27.08 0 00-5.84 8.8 28.94 28.94 0 00-2.13 11.31zm-194.97-30.5h19.78v73.54h-19.78zm49.25 0h19.78v73.54h-19.78z"></path><circle cx="216.33" cy="21.4" r="12.28"></circle></svg></a><div class="st-Header_communitySelector" tabindex="0"><span class="fa fa-caret-down"></span></div><div class="st-Header_dropdown"><div class="st-Header_dropdownHeading">Qiita Teams that are logged in</div><div class="st-Header_dropdownItemNote">You are not logged in to any team</div><hr class="st-Header_dropdownDivider st-Header_dropdownDivider-shrink"/><a href="https://teams-center.qiita.com/find_team" class="st-Header_dropdownItem"><span class="fa fa-fw fa-sign-in st-Header_dropdownItemIcon"></span><div>Log in to Qiita Team</div></a><div class="st-Header_dropdownDivider"></div><div class="st-Header_dropdownHeading">Community</div><a href="/organizations" class="st-Header_dropdownItem">Organization</a><a href="/official-events/open" class="st-Header_dropdownItem">Event</a><a href="/advent-calendar" class="st-Header_dropdownItem">Advent Calendar</a><a href="https://qiitadon.com/" class="st-Header_dropdownItem" target="_blank">Qiitadon (β)</a><div class="st-Header_dropdownDivider"></div><div class="st-Header_dropdownHeading">Service</div><a href="https://jobs.qiita.com/?utm_source=qiita&amp;utm_medium=referral&amp;utm_content=header" class="st-Header_dropdownItem" target="_blank">Qiita Jobs</a><a href="https://zine.qiita.com/?utm_source=qiita&amp;utm_medium=referral&amp;utm_content=header" class="st-Header_dropdownItem" target="_blank">Qiita Zine</a><a href="https://blog.qiita.com/?utm_source=qiita&amp;utm_medium=referral&amp;utm_content=header" class="st-Header_dropdownItem" target="_blank">Qiita Blog</a></div><form class="st-Header_search" action="/search" method="get"><span class="fa fa-search st-Header_searchIcon"></span><input type="search" class="st-Header_searchInput" autoComplete="off" placeholder="Search" value="" name="q" required=""/></form><form class="st-Header_searchModal" action="/search" method="get"><input type="text" class="st-Header_searchModalInput" autoComplete="off" placeholder="Search" value="" name="q" required=""/></form></div><div class="st-Header_end"><div class="st-Header_searchButton"><span class="fa fa-search"></span></div><a class="st-Header_signupButton" href="/signup?redirect_to=%2Fhighno_RQ%2Fitems%2F7a33e7eaa6d77c93968f">Signup</a><a class="st-Header_loginLink" href="/login?redirect_to=%2Fhighno_RQ%2Fitems%2F7a33e7eaa6d77c93968f">Login</a></div><div class="st-Header_overlay"></div></div></div></div>
      <script type="application/json" class="js-react-on-rails-component" data-component-name="GlobalHeader" data-dom-id="GlobalHeader-react-component-01f55ef9-be80-4d35-af8c-3b8837bbfb6d">{"unreadNotificationsCount":null,"realms":[{"humanName":"Qiita","isCurrentRealm":true,"isQiita":true,"isQiitaTeam":false,"loggedInUser":null,"teamId":null,"url":"https://qiita.com/"}],"teamFindUrl":"https://teams-center.qiita.com/find_team","isTeamOnlyUser":null,"currentUser":null}</script>
      
</div><div class="st-HeaderAlert st-HeaderAlert-warning"><div class="st-HeaderAlert_body"></div></div><script type="application/ld+json">{"@context":"http://schema.org","@type":"BreadcrumbList","itemListElement":[{"@type":"ListItem","position":1,"item":{"@id":"/","name":"Qiita"}},{"@type":"ListItem","position":2,"item":{"@id":"/tags/csharp","name":"C#"}}]}</script><script type="application/ld+json">{"@context":"http://schema.org","@type":"Article","datePublished":"2019-08-09T08:19:35.000+09:00","dateModified":"2020-07-02T21:29:40.000+09:00","headline":"Unityでディープラーニングする方法調べた","image":"https://qiita-user-contents.imgix.net/https%3A%2F%2Fcdn.qiita.com%2Fassets%2Fpublic%2Farticle-ogp-background-1150d8b18a7c15795b701a55ae908f94.png?ixlib=rb-4.0.0\u0026w=1200\u0026mark64=aHR0cHM6Ly9xaWl0YS11c2VyLWNvbnRlbnRzLmltZ2l4Lm5ldC9-dGV4dD9peGxpYj1yYi00LjAuMCZ3PTg0MCZoPTM4MCZ0eHQ2ND1WVzVwZEhuamdhZmpnNGZqZ3FQamc3empnNWZqZzZuamc3empnNHZqZzdQamdyRGpnWm5qZ292bWxybm1zNVhvcXJfamdibmpnWjgmdHh0LWNvbG9yPSUyMzMzMyZ0eHQtZm9udD1IaXJhZ2lubyUyMFNhbnMlMjBXNiZ0eHQtc2l6ZT01NCZ0eHQtY2xpcD1lbGxpcHNpcyZ0eHQtYWxpZ249Y2VudGVyJTJDbWlkZGxlJnM9M2FkYjA3MTQwMjExMzM4MzQ4NjM2M2UxMDU0MWE2YmM\u0026mark-align=center%2Cmiddle\u0026blend64=aHR0cHM6Ly9xaWl0YS11c2VyLWNvbnRlbnRzLmltZ2l4Lm5ldC9-dGV4dD9peGxpYj1yYi00LjAuMCZ3PTg0MCZoPTUwMCZ0eHQ2ND1RR2hwWjJodWIxOVNVUSZ0eHQtY29sb3I9JTIzMzMzJnR4dC1mb250PUhpcmFnaW5vJTIwU2FucyUyMFc2JnR4dC1zaXplPTQ1JnR4dC1hbGlnbj1yaWdodCUyQ2JvdHRvbSZzPWFjNzNiYzYyNTNhZjViZjcyNDEyNzUzZTFiN2ExYjkw\u0026blend-align=center%2Cmiddle\u0026blend-mode=normal\u0026s=57c9cfe0ae60e4a553c9a47e09d75b27","mainEntityOfPage":"https://qiita.com/highno_RQ/items/7a33e7eaa6d77c93968f","author":{"@type":"Person","address":"","email":null,"identifier":"highno_RQ","name":"highno_RQ","image":"https://qiita-user-profile-images.imgix.net/https%3A%2F%2Fqiita-image-store.s3.ap-northeast-1.amazonaws.com%2F0%2F440309%2Fprofile-images%2F1565306650?ixlib=rb-4.0.0\u0026auto=compress%2Cformat\u0026lossless=0\u0026w=75\u0026s=fa0d9a94a3e3518975d82d7562d24329","url":"https://qiita.com/highno_RQ","description":"1の方です。\r\nこのサイトの掲載内容は私個人の見解であり、\r\n必ずしも私が所属する会社、組織、団体の立場、戦略、意見を代表するものではありません。","memberOf":[]},"publisher":{"@type":"Organization","name":"Qiita","logo":{"@type":"ImageObject","url":"//cdn.qiita.com/assets/public/qiita-logo-c39ded593afa388e2e1ba435b110554e.png"}}}</script><style type="text/css">.wb-CampaignLink {
  background-color: #333333;
  width: 100%;
}

.wb-CampaignLink_container {
  display: flex;
  flex-wrap: wrap;
  justify-content: space-between;
  max-width: 1100px;
  margin: 0 auto;
  font-size: 13px;
  padding: 0.8em;
}
.wb-CampaignLink_container > a {
  color: #fff;
}

.wb-CampaignLink_container > a:hover {
  text-decoration: underline;
}</style><div class="wb-CampaignLink"><div class="wb-CampaignLink_container"><a target="_blank" id="header_text_message_1" href="https://increments.connpass.com/event/211948/">【9/9(木)開催】Qiitaの過去と未来についてお話しします！Qiita エンジニアフェスタ 2021 Online Meetup</a><a target="_blank" id="header_text_message_2" href="https://increments.connpass.com/event/211948/">詳しくはこちら</a></div><script>td.trackEvent(
  'front_events',
  {"query_parameters":{},"path_parameters":{"controller":"public/items","action":"show","user_id":"highno_RQ","type":"items","id":"7a33e7eaa6d77c93968f"},"user_id":null,"event_category":"Header Text Ads Banner","event_action":"Show","data":{"message":"【9/9(木)開催】Qiitaの過去と未来についてお話しします！Qiita エンジニアフェスタ 2021 Online Meetup","url":"https://increments.connpass.com/event/211948/","sub_message":"詳しくはこちら"}}
)</script><script>document.getElementById('header_text_message_1').addEventListener('click', function() {
  td.trackEvent(
    'front_events',
    {"query_parameters":{},"path_parameters":{"controller":"public/items","action":"show","user_id":"highno_RQ","type":"items","id":"7a33e7eaa6d77c93968f"},"user_id":null,"event_category":"Header Text Ads Banner","event_action":"Click","data":{"index":2,"pos_id":"header_text_message_1","message":"【9/9(木)開催】Qiitaの過去と未来についてお話しします！Qiita エンジニアフェスタ 2021 Online Meetup","url":"https://increments.connpass.com/event/211948/","sub_message":"詳しくはこちら"}}
  )
})</script><script>document.getElementById('header_text_message_2').addEventListener('click', function() {
  td.trackEvent(
    'front_events',
    {"query_parameters":{},"path_parameters":{"controller":"public/items","action":"show","user_id":"highno_RQ","type":"items","id":"7a33e7eaa6d77c93968f"},"user_id":null,"event_category":"Header Text Ads Banner","event_action":"Click","data":{"index":2,"pos_id":"header_text_message_2","message":"【9/9(木)開催】Qiitaの過去と未来についてお話しします！Qiita エンジニアフェスタ 2021 Online Meetup","url":"https://increments.connpass.com/event/211948/","sub_message":"詳しくはこちら"}}
  )
})</script></div><script type="application/json" id="js-react-on-rails-context">{"railsEnv":"production","inMailer":false,"i18nLocale":"en","i18nDefaultLocale":"en","href":"https://qiita.com/highno_RQ/items/7a33e7eaa6d77c93968f","location":"/highno_RQ/items/7a33e7eaa6d77c93968f","scheme":"https","host":"qiita.com","port":null,"pathname":"/highno_RQ/items/7a33e7eaa6d77c93968f","search":null,"httpAcceptLanguage":null,"actionPath":"public/items#show","settings":{"analyticsTrackingId":"UA-24675221-12","assetsMap":{},"csrfToken":"ic2hbJcQ593LvThY1+1addVDAn5hbCS66AgBBuPdG89u32zB642cUSlmdLGiT17g/lYT/Qv+jAbbB+LH0llqhg==","locale":"en"},"currentUser":null,"isLoggedIn":false,"recaptchaSiteKey":"6LfNkiQTAAAAAM3UGnSquBy2akTITGNMO_QDxMw6","serverSide":false}</script>
<div id="PersonalArticlePage-react-component-39c5d909-6029-48a7-bc6a-c577e131fed2"><div class="p-items_wrapper"><div class=" css-17jxvjw"><div class="css-11t2ec1"><div class="css-1dvr2p8"><button class=" css-18lkoru"><svg size="20" xmlns="http://www.w3.org/2000/svg" viewBox="0 0 392.81 429" color="#55C500" class="css-1g4cku8 e31pr5q0"><path d="M14.19 5.4h53.86v149.45h90.05v44.87H14.19zM288.4 93.77h100.79q1.29 25-5.66 45.39a96.79 96.79 0 01-20.33 34.89 92 92 0 01-32.13 22.45 104 104 0 01-40.95 7.93 109.71 109.71 0 01-76-29.92 104.05 104.05 0 01-23-32.56 95.46 95.46 0 01-8.47-39.88 94.78 94.78 0 018.47-39.87 104.38 104.38 0 0123-32.42A107.71 107.71 0 01248.23 8a110.79 110.79 0 01118.48 22.49l-35.07 35.08a51.25 51.25 0 00-17.75-15 52.83 52.83 0 00-44.67-1.23 52.92 52.92 0 00-17 12 57.07 57.07 0 00-11.45 18.11 60 60 0 00-4.23 22.77 60 60 0 004.23 22.68 56.57 56.57 0 0011.45 18.19 52.62 52.62 0 0017 12 50.5 50.5 0 0020.9 4.36q20.19 0 31.07-7.51a35.75 35.75 0 0014.46-20.55h-47.39zM51.29 279.55H0v-44.86h156v44.86h-51.13V429H51.29zM283.36 381.71l-41.72-62V429h-53.86V234.69h47.47L290 312l55.9-77.29h46.9V429h-53.86V320.27l-42.43 61.44z"></path></svg></button><a href="/highno_RQ/items/7a33e7eaa6d77c93968f/likers" class="css-1iupg5d">102</a></div><div class="css-ijvq0v"><button class=" css-15cocm3"><svg size="16" xmlns="http://www.w3.org/2000/svg" viewBox="0 0 353.02 398" color="#6E6F70" class="css-12rp90f e11v00bf0"><path d="M176.72 398c-67.52 0-130.16-29-171.84-79.69l-4.46-5.42V78.05H353v234.84l-4.45 5.42C306.88 369 244.24 398 176.72 398zm-137.2-99.34c34.17 38.37 83.78 60.25 137.2 60.25s103-21.88 137.21-60.25V117.14H39.52zM0 0h351.12v40.94H0z"></path></svg></button><span class="css-115f4t">94</span></div><div class="css-1b8uj5v"><span class="fa fa-twitter"></span></div><div class="css-1b8uj5v"><span class="fa fa-facebook"></span></div><div class="css-79elbk"><button class="css-16hhh7b"><i class="fa fa-ellipsis-h"></i></button><div class="css-fcbn8c"><div class="css-1gj7nt">Improve article</div><a href="/drafts/7a33e7eaa6d77c93968f/edit" class="css-154zy0m"><span class="css-yikrym"><i class="fa fa-code-fork css-1jqivyb" aria-hidden="true"></i></span>Send edit request</a><div class="css-1ode1bp"></div><div class="css-1gj7nt">Article information</div><a href="/highno_RQ/items/7a33e7eaa6d77c93968f/revisions" class="css-154zy0m"><span class="css-yikrym"><i class="fa fa-history css-1jqivyb" aria-hidden="true"></i></span>Revisions</a><a href="/highno_RQ/items/7a33e7eaa6d77c93968f/patches" class="css-154zy0m"><span class="css-yikrym"><i class="fa fa-inbox css-1jqivyb" aria-hidden="true"></i></span>Edit Requests</a><a href="/highno_RQ/items/7a33e7eaa6d77c93968f/likers" class="css-154zy0m"><span class="css-yikrym"><svg size="13" xmlns="http://www.w3.org/2000/svg" viewBox="0 0 392.81 429" color="rgba(0, 0, 0, 0.6)" class="css-le4d8r e31pr5q0"><path d="M14.19 5.4h53.86v149.45h90.05v44.87H14.19zM288.4 93.77h100.79q1.29 25-5.66 45.39a96.79 96.79 0 01-20.33 34.89 92 92 0 01-32.13 22.45 104 104 0 01-40.95 7.93 109.71 109.71 0 01-76-29.92 104.05 104.05 0 01-23-32.56 95.46 95.46 0 01-8.47-39.88 94.78 94.78 0 018.47-39.87 104.38 104.38 0 0123-32.42A107.71 107.71 0 01248.23 8a110.79 110.79 0 01118.48 22.49l-35.07 35.08a51.25 51.25 0 00-17.75-15 52.83 52.83 0 00-44.67-1.23 52.92 52.92 0 00-17 12 57.07 57.07 0 00-11.45 18.11 60 60 0 00-4.23 22.77 60 60 0 004.23 22.68 56.57 56.57 0 0011.45 18.19 52.62 52.62 0 0017 12 50.5 50.5 0 0020.9 4.36q20.19 0 31.07-7.51a35.75 35.75 0 0014.46-20.55h-47.39zM51.29 279.55H0v-44.86h156v44.86h-51.13V429H51.29zM283.36 381.71l-41.72-62V429h-53.86V234.69h47.47L290 312l55.9-77.29h46.9V429h-53.86V320.27l-42.43 61.44z"></path></svg></span>Show all likers</a><a href="/highno_RQ/items/7a33e7eaa6d77c93968f.md" class="css-154zy0m"><span class="css-yikrym"><i class="fa fa-file-text-o css-1jqivyb" aria-hidden="true"></i></span>Show article in Markdown</a><div class="css-1ode1bp"></div><div class="css-154zy0m"><span class="css-yikrym"><i class="fa fa-flag css-1jqivyb" aria-hidden="true"></i></span>Report article</div></div><div class="st-Modal"><div class="st-Modal_backdrop"></div><div class="st-Modal_body"><form><div class="st-Form"><span class="st-Form_label">Help us understand the problem. What is going on with this article?</span></div><div class="st-Form"><label><input type="radio" name="reason" value="illegal" required=""/>It&#x27;s illegal (copyright infringement, privacy infringement, libel, etc.)</label></div><div class="st-Form"><label><input type="radio" name="reason" value="inappropriate_content" required=""/>It&#x27;s socially inappropriate (offensive to public order and morals)</label></div><div class="st-Form"><label><input type="radio" name="reason" value="advertising" required=""/>It&#x27;s advertising</label></div><div class="st-Form"><label><input type="radio" name="reason" value="spam" required=""/>It&#x27;s spam</label></div><div class="st-Form"><label><input type="radio" name="reason" value="guideline_violation" required=""/>Other than the above, but not suitable for the Qiita community (violation of guidelines)</label></div><div class="st-Form st-Form-right"><input type="submit" class="st-Form_submit" value="Submit"/></div></form></div></div></div></div><div class="p-items_options"><div class="mt-2"><div class="css-1hbd3g7"></div></div></div><div class="p-items_toc"><div class="mt-2"><div class="css-1hbd3g7"></div></div></div><div class="p-items_main"><div class="css-1nzh4zz"><span class="fa fa-fw fa-warning css-38fzdi"></span><p>More than 1 year has passed since last update.</p></div><div class="css-helsa7"><div class="css-8qb8m4"><div class="css-2imjyh"><div class="css-he5w1s"><div class="css-70qvj9"><div class="css-3ojehk"><a href="/highno_RQ"><img class="css-100alwu eyfquo10" src="https://qiita-image-store.s3.ap-northeast-1.amazonaws.com/0/440309/profile-images/1565306650" width="24" height="24" loading="lazy"/></a></div><div class="css-1dtnjt5"><a href="/highno_RQ" class="css-10ougpm">@<!-- -->highno_RQ</a><div class="css-1ay9vb9"><span><meta content="2019-08-08T23:19:35Z"/><time dateTime="2020-07-02T12:29:40Z" class="css-m19uds">updated at 2020-07-02</time></span></div></div></div></div></div><h1 class="css-cgzq40">Unityでディープラーニングする方法調べた</h1><div class="css-1wa99t2"><span class="fa fa-tags mr-1of2 css-1l3zk9f" aria-hidden="true"></span><a href="/tags/csharp" class="css-4czcte">C#</a><a href="/tags/unity" class="css-4czcte">Unity</a><a href="/tags/deeplearning" class="css-4czcte">DeepLearning</a><a href="/tags/tensorflow" class="css-4czcte">TensorFlow</a><a href="/tags/onnx" class="css-4czcte">ONNX</a></div></div><section class="it-MdContent"><div id="personal-public-article-body"><div>
<h1>
<span id="202072追記" class="fragment"></span><a href="#202072%E8%BF%BD%E8%A8%98"><i class="fa fa-link"></i></a>2020/7/2追記</h1>

<p>Barracudaの記事を書きました<br>
この記事の手法より圧倒的に簡単なので、こっち利用推奨です！！<br>
<strong><a href="https://qiita.com/highno_RQ/items/478e1145f0eb868c0f2e" id="reference-e9987542498d7c5c7318">Unity Technologies製推論エンジン Barracudaがスゴイという話</a></strong></p>

<h1>
<span id="2020512追記" class="fragment"></span><a href="#2020512%E8%BF%BD%E8%A8%98"><i class="fa fa-link"></i></a>2020/5/12追記</h1>

<p>Unity製推論エンジンのBarracudaがonnxからモデルを直接読み込めるようになっていたようです。<br>
この記事で書いた程度の内容であればこんな面倒な手順を踏まなくてもいけそうです。<br>
<del>Barracudaのテストについては後日実施します(おそらく6月中)</del></p>

<p>※参考<br>
<a href="https://docs.unity3d.com/Packages/com.unity.barracuda@0.7/manual/index.html" rel="nofollow noopener" target="_blank">公式ドキュメント ver0.70</a><br>
<a href="https://note.com/npaka/n/nf7f76871af8f" rel="nofollow noopener" target="_blank">Unity Barracuda - ニューラルネットワーク推論ライブラリ</a></p>

<h1>
<span id="概要" class="fragment"></span><a href="#%E6%A6%82%E8%A6%81"><i class="fa fa-link"></i></a>概要</h1>

<p>ARで深度推定とかするのにml-agentsだけだと足りなかったので<br>
Unityで学習済みモデルを扱う方法を調べて試してみました。</p>

<h1>
<span id="実施内容" class="fragment"></span><a href="#%E5%AE%9F%E6%96%BD%E5%86%85%E5%AE%B9"><i class="fa fa-link"></i></a>実施内容</h1>

<p>学習済みモデルVGG19を利用してUnity内に表示した画像の分類を行う。<br>
<a href="https://camo.qiitausercontent.com/13921305ae9dbdf05abe82482997f2c90f27dd14/68747470733a2f2f71696974612d696d6167652d73746f72652e73332e61702d6e6f727468656173742d312e616d617a6f6e6177732e636f6d2f302f3434303330392f64636565383635322d363832382d626638342d666664322d3863313966313230303730392e706e67" target="_blank" rel="nofollow noopener"><img src="https://qiita-user-contents.imgix.net/https%3A%2F%2Fqiita-image-store.s3.ap-northeast-1.amazonaws.com%2F0%2F440309%2Fdcee8652-6828-bf84-ffd2-8c19f1200709.png?ixlib=rb-4.0.0&amp;auto=format&amp;gif-q=60&amp;q=75&amp;s=f37962d877c4a385893cf0b3aa1069fa" alt="キャプチャ.PNG" data-canonical-src="https://qiita-image-store.s3.ap-northeast-1.amazonaws.com/0/440309/dcee8652-6828-bf84-ffd2-8c19f1200709.png" srcset="https://qiita-user-contents.imgix.net/https%3A%2F%2Fqiita-image-store.s3.ap-northeast-1.amazonaws.com%2F0%2F440309%2Fdcee8652-6828-bf84-ffd2-8c19f1200709.png?ixlib=rb-4.0.0&amp;auto=format&amp;gif-q=60&amp;q=75&amp;w=1400&amp;fit=max&amp;s=7fa5e57312a3518b3d9118e5c5d77d20 1x" loading="lazy"></a></p>

<h2>
<span id="モデルの形式" class="fragment"></span><a href="#%E3%83%A2%E3%83%87%E3%83%AB%E3%81%AE%E5%BD%A2%E5%BC%8F"><i class="fa fa-link"></i></a>モデルの形式</h2>

<table>
<thead>
<tr>
<th style="text-align: left"></th>
<th style="text-align: left"></th>
</tr>
</thead>
<tbody>
<tr>
<td style="text-align: left">モデル名</td>
<td style="text-align: left">VGG-19</td>
</tr>
<tr>
<td style="text-align: left">入力</td>
<td style="text-align: left">224×224×3</td>
</tr>
<tr>
<td style="text-align: left">出力</td>
<td style="text-align: left">1000</td>
</tr>
<tr>
<td style="text-align: left">データセット</td>
<td style="text-align: left">ImageNet (ILSVRCA2012)</td>
</tr>
</tbody>
</table>

<h2>
<span id="前処理" class="fragment"></span><a href="#%E5%89%8D%E5%87%A6%E7%90%86"><i class="fa fa-link"></i></a>前処理</h2>

<ol>
<li>8bitで表された画像を0-1範囲に変換　= pixel / 255</li>
<li>RGBの各値を平均{ 0.485f, 0.456f, 0.406f } 標準偏差　{ 0.229f, 0.224f, 0.225f }で正規化</li>
</ol>

<h1>
<span id="環境" class="fragment"></span><a href="#%E7%92%B0%E5%A2%83"><i class="fa fa-link"></i></a>環境</h1>

<table>
<thead>
<tr>
<th style="text-align: left"></th>
<th style="text-align: left"></th>
</tr>
</thead>
<tbody>
<tr>
<td style="text-align: left">OS</td>
<td style="text-align: left">Windows10 Home</td>
</tr>
<tr>
<td style="text-align: left">CPU</td>
<td style="text-align: left">Intel(R) Core(TM) i7-7700K</td>
</tr>
<tr>
<td style="text-align: left">Unity</td>
<td style="text-align: left">2019.1.0f2(64-bit)</td>
</tr>
<tr>
<td style="text-align: left">tensorflow</td>
<td style="text-align: left">1.14.0</td>
</tr>
</tbody>
</table>

<p>今回、ライブラリの導入に<a href="https://www.nuget.org" rel="nofollow noopener" target="_blank">NuGet</a>を利用しました。<br>
UnityではVisualStudioのNuGetパッケージマネージャーは使えないので以下の2手法を利用します。</p>

<ol>
<li>NugetForUnity Assetの利用
AssetStoreから<a href="https://assetstore.unity.com/packages/tools/utilities/nuget-for-unity-104640" rel="nofollow noopener" target="_blank">Nuget For Unity</a>をインストールして利用します。</li>
<li>公式ドキュメント記載の方法
公式ドキュメント 「<a href="https://docs.microsoft.com/ja-jp/visualstudio/cross-platform/unity-scripting-upgrade?view=vs-2019#add-packages-from-nuget-to-a-unity-project" rel="nofollow noopener" target="_blank">NuGetからUnity プロジェクトにパッケージを追加する</a>」を参照してください。</li>
</ol>

<p>どちらの手法でもパッケージ同士の依存関係は手動で特定する必要があります。</p>

<p>参考：<br>
Unity であらゆる C#(.NET) パッケージを使う（例：opencv）<br>
　<a href="https://qiita.com/kingyo222/items/11100e8f7be396b98453" class="autolink" id="reference-bd62df36b1b8f3bdbcb2">https://qiita.com/kingyo222/items/11100e8f7be396b98453</a></p>

<h1>
<span id="手法一覧" class="fragment"></span><a href="#%E6%89%8B%E6%B3%95%E4%B8%80%E8%A6%A7"><i class="fa fa-link"></i></a>手法一覧</h1>

<ol>
<li>Onnx形式で学習済みモデルを扱う

<ul>
<li>Onnx Runtime</li>
<li>OpenCVSharp</li>
</ul>
</li>
<li>C#から直接tensorflowを扱う

<ul>
<li>TensorFlowSharp</li>
</ul>
</li>
<li>今回実施できなかった手法

<ul>
<li>WindowsML</li>
<li>ML. Net</li>
<li>CoreML</li>
<li>IronPython</li>
</ul>
</li>
<li>おまけ

<ul>
<li>ml-agents</li>
<li>The Unity Inference Engine</li>
</ul>
</li>
</ol>

<h1>
<span id="1-onnx形式で学習済みモデルを扱う" class="fragment"></span><a href="#1-onnx%E5%BD%A2%E5%BC%8F%E3%81%A7%E5%AD%A6%E7%BF%92%E6%B8%88%E3%81%BF%E3%83%A2%E3%83%87%E3%83%AB%E3%82%92%E6%89%B1%E3%81%86"><i class="fa fa-link"></i></a>1. Onnx形式で学習済みモデルを扱う</h1>

<h2>
<span id="概要-1" class="fragment"></span><a href="#%E6%A6%82%E8%A6%81-1"><i class="fa fa-link"></i></a>概要</h2>

<p>機械学習モデルの標準形式であるOnnx(Open Neural Network Exchange)形式を利用します。<br>
この形式に変換することで、TensorflowやPytorch,Chainerといった<br>
主要な深層学習ライブラリで学習したモデルを色々なライブラリで利用できるようになります。</p>

<p>参考：<br>
ONNX Model Zoo(訓練済みモデルの配布場所)<br>
　<a href="https://github.com/onnx/models" class="autolink" rel="nofollow noopener" target="_blank">https://github.com/onnx/models</a></p>

<h2>
<span id="--onnx-runtime" class="fragment"></span><a href="#--onnx-runtime"><i class="fa fa-link"></i></a>- Onnx Runtime</h2>

<p>Onnx形式を扱う事に特化した推論エンジンです。<br>
2018/12/04にOSS(MITライセンス)化されたこともあり、使い所は色々とありそうです。</p>

<div class="code-frame" data-lang="C#"><div class="highlight"><pre class="with-code"><code>    <span class="k">private</span> <span class="n">InferenceSession</span> <span class="n">session</span><span class="p">;</span>
    <span class="k">public</span> <span class="k">void</span> <span class="nf">LoadModel</span> <span class="p">(</span><span class="kt">string</span> <span class="n">model_name</span><span class="p">)</span> <span class="p">{</span>
        <span class="kt">string</span> <span class="n">model_path</span> <span class="p">=</span> <span class="n">Application</span><span class="p">.</span><span class="n">dataPath</span> <span class="p">+</span> <span class="s">@"/MLModel/"</span> <span class="p">+</span> <span class="n">model_name</span> <span class="p">+</span> <span class="s">".onnx"</span><span class="p">;</span>

        <span class="kt">var</span> <span class="n">opitions</span> <span class="p">=</span> <span class="k">new</span> <span class="nf">SessionOptions</span> <span class="p">();</span>
        <span class="n">session</span> <span class="p">=</span> <span class="k">new</span> <span class="nf">InferenceSession</span> <span class="p">(</span><span class="n">model_path</span><span class="p">);</span>
    <span class="p">}</span>
    <span class="k">public</span> <span class="kt">int</span> <span class="nf">Inference</span> <span class="p">(</span><span class="n">Texture2D</span> <span class="n">input</span><span class="p">)</span> <span class="p">{</span>
        <span class="kt">var</span> <span class="n">input_nodes_name</span> <span class="p">=</span> <span class="n">session</span><span class="p">.</span><span class="n">InputMetadata</span><span class="p">.</span><span class="nf">First</span><span class="p">().</span><span class="n">Key</span><span class="p">;</span>
        <span class="kt">var</span> <span class="n">input_nodes_dim</span> <span class="p">=</span> <span class="n">session</span><span class="p">.</span><span class="n">InputMetadata</span><span class="p">.</span><span class="nf">First</span><span class="p">().</span><span class="n">Value</span><span class="p">.</span><span class="n">Dimensions</span><span class="p">;</span>

        <span class="c1">// Texture2Dをモデルの入力に合った形に整形、正規化する。</span>
        <span class="kt">var</span> <span class="n">input_floats</span> <span class="p">=</span> <span class="nf">GetFloatFromTex2DWithFlip</span> <span class="p">(</span><span class="n">input</span><span class="p">);</span>
        <span class="kt">var</span> <span class="n">input_tensor</span> <span class="p">=</span> <span class="k">new</span> <span class="n">DenseTensor</span><span class="p">&lt;</span><span class="kt">float</span><span class="p">&gt;</span> <span class="p">(</span><span class="n">input_floats</span><span class="p">,</span> <span class="n">input_nodes_dim</span><span class="p">);</span>

        <span class="c1">// OnnxRuntimeでの入力形式であるNamedOnnxValueを作成する</span>
        <span class="kt">var</span> <span class="n">input_onnx_values</span> <span class="p">=</span> <span class="k">new</span> <span class="n">List</span><span class="p">&lt;</span><span class="n">NamedOnnxValue</span><span class="p">&gt;</span> <span class="p">{</span>
            <span class="n">NamedOnnxValue</span><span class="p">.</span><span class="nf">CreateFromTensor</span> <span class="p">(</span><span class="n">input_nodes_name</span><span class="p">,</span> <span class="n">input_tensor</span><span class="p">)</span>
        <span class="p">};</span>

        <span class="c1">// 推論を実行</span>
        <span class="kt">var</span> <span class="n">results</span> <span class="p">=</span> <span class="n">session</span><span class="p">.</span><span class="nf">Run</span> <span class="p">(</span><span class="n">input_onnx_values</span><span class="p">);</span>
        <span class="kt">var</span> <span class="n">scores</span> <span class="p">=</span> <span class="n">results</span><span class="p">.</span><span class="nf">First</span><span class="p">().</span><span class="n">AsTensor</span><span class="p">&lt;</span><span class="kt">float</span><span class="p">&gt;().</span><span class="nf">ToArray</span> <span class="p">();</span>

        <span class="k">return</span> <span class="nf">GetBestScorePos</span><span class="p">(</span><span class="n">scores</span><span class="p">);</span>
    <span class="p">}</span>
</code></pre></div></div>

<ul>
<li>利点

<ul>
<li>モデルの扱いが単純</li>
<li>前処理など数値の扱いが容易</li>
</ul>
</li>
<li>欠点

<ul>
<li>依存関係のあるパッケージの特定に時間がかかる</li>
<li>モデルの読み込みが不安定(環境依存？)</li>
</ul>
</li>
</ul>

<p>総評：<br>
入力・モデルの扱い共に非常に簡単です。<br>
モデルの読み込みが最初の数秒不安定ですが、それ以降は特に問題なく実行できます。</p>

<p>参考：<br>
C#でONNXファイルを利用して手書き数字を認識する方法：<br>
　<a href="https://kagasu.hatenablog.com/entry/2019/05/21/162445" class="autolink" rel="nofollow noopener" target="_blank">https://kagasu.hatenablog.com/entry/2019/05/21/162445</a><br>
onnx runtime 公式 github：<br>
　<a href="https://github.com/microsoft/onnxruntime" class="autolink" rel="nofollow noopener" target="_blank">https://github.com/microsoft/onnxruntime</a></p>

<h2>
<span id="--opencvsharp" class="fragment"></span><a href="#--opencvsharp"><i class="fa fa-link"></i></a>- OpenCVSharp</h2>

<p>画像処理ライブラリとして有名ですが、Onnxを読み込む機能がサポートされているので使ってみました。</p>

<div class="code-frame" data-lang="C#"><div class="highlight"><pre class="with-code"><code>    <span class="k">private</span> <span class="n">Net</span> <span class="n">model</span><span class="p">;</span>
    <span class="k">public</span> <span class="k">void</span> <span class="nf">LoadModel</span> <span class="p">(</span><span class="kt">string</span> <span class="n">model_name</span><span class="p">)</span> <span class="p">{</span>
        <span class="kt">string</span> <span class="n">model_path</span> <span class="p">=</span> <span class="n">Application</span><span class="p">.</span><span class="n">dataPath</span> <span class="p">+</span> <span class="s">@"/MLModel/"</span> <span class="p">+</span> <span class="n">model_name</span> <span class="p">+</span> <span class="s">".onnx"</span><span class="p">;</span>
        <span class="n">model</span> <span class="p">=</span> <span class="n">Net</span><span class="p">.</span><span class="nf">ReadNetFromONNX</span> <span class="p">(</span><span class="n">model_path</span><span class="p">);</span>
    <span class="p">}</span>
    <span class="k">public</span> <span class="kt">int</span> <span class="nf">Inference</span> <span class="p">(</span><span class="n">Texture2D</span> <span class="n">input</span><span class="p">)</span> <span class="p">{</span>
        <span class="c1">// Texture2Dをモデルの入力に合った形に整形する</span>
        <span class="n">Mat</span> <span class="n">img</span> <span class="p">=</span> <span class="nf">FixTexture2Input</span> <span class="p">(</span><span class="n">input</span><span class="p">);</span>

        <span class="c1">// OpenCVSharpでの入力形式であるblobを作成、同時に正規化を行う。</span>
        <span class="n">Scalar</span> <span class="n">mean</span> <span class="p">=</span> <span class="k">new</span> <span class="nf">Scalar</span> <span class="p">(</span><span class="m">0.485f</span><span class="p">,</span> <span class="m">0.456f</span><span class="p">,</span> <span class="m">0.406f</span><span class="p">);</span>
        <span class="kt">var</span> <span class="n">blob</span> <span class="p">=</span> <span class="n">CvDnn</span><span class="p">.</span><span class="nf">BlobFromImage</span> <span class="p">(</span><span class="n">img</span><span class="p">,</span> <span class="m">1.0</span> <span class="p">/</span> <span class="m">255.0</span> <span class="p">/</span> <span class="m">0.225f</span><span class="p">,</span> 
                                        <span class="k">new</span> <span class="nf">Size</span> <span class="p">(</span><span class="n">input</span><span class="p">.</span><span class="n">width</span><span class="p">,</span> <span class="n">input</span><span class="p">.</span><span class="n">height</span><span class="p">),</span> 
                                        <span class="n">mean</span><span class="p">,</span> <span class="n">swapRB</span> <span class="p">:</span> <span class="k">true</span><span class="p">,</span> <span class="n">crop</span> <span class="p">:</span> <span class="k">false</span><span class="p">);</span>
        <span class="n">model</span><span class="p">.</span><span class="nf">SetInput</span> <span class="p">(</span><span class="n">blob</span><span class="p">);</span>

        <span class="c1">// 推論を実行</span>
        <span class="n">Mat</span> <span class="n">scores</span> <span class="p">=</span> <span class="n">model</span><span class="p">.</span><span class="nf">Forward</span> <span class="p">();</span>

        <span class="k">return</span> <span class="nf">GetBestScorePos</span><span class="p">(</span><span class="n">scores</span><span class="p">);</span>
    <span class="p">}</span>
</code></pre></div></div>

<ul>
<li>利点

<ul>
<li>モデルの扱いが単純</li>
<li>Nuget For Unityで簡単にプラグインを導入できる</li>
</ul>
</li>
<li>欠点

<ul>
<li>画像をMat形式で扱うため、指定の前処理を行えないことがある</li>
</ul>
</li>
</ul>

<p>OpenCVSharpに関してはこちらの記事で今回の問題点を解決してくれてます。<br>
<a href="https://qiita.com/utibenkei/items/adc7f526d629bdfc41fc" id="reference-b3395fb394c8becba400">Unityでディープラーニング推論をする方法（マルチプラットフォーム対応）</a></p>

<p>総評：<br>
読み込みが早く、動作も安定しています。<br>
画像関係のモデルを扱うならばこれだけで十分そうです。</p>

<p>参考：<br>
3Dの姿勢推定のOnnxのモデルでUnityちゃんを動かしてみた<br>
　<a href="https://qiita.com/yukihiko_a/items/386e3a86a5e523757707" class="autolink" id="reference-9b9e9a1704d1888fc8a7">https://qiita.com/yukihiko_a/items/386e3a86a5e523757707</a></p>

<h1>
<span id="2-unityから直接tensorflowを扱う" class="fragment"></span><a href="#2-unity%E3%81%8B%E3%82%89%E7%9B%B4%E6%8E%A5tensorflow%E3%82%92%E6%89%B1%E3%81%86"><i class="fa fa-link"></i></a>2. Unityから直接tensorflowを扱う</h1>

<h2>
<span id="tensorflowsharp" class="fragment"></span><a href="#tensorflowsharp"><i class="fa fa-link"></i></a>TensorFlowSharp</h2>

<p>C#から直接Tensorflowを利用するためのライブラリです。<br>
といっても学習済みモデルの利用が主な機能なので実質推論環境みたいなものです。</p>

<div class="code-frame" data-lang="C#"><div class="highlight"><pre class="with-code"><code>
    <span class="k">private</span> <span class="n">TFGraph</span> <span class="n">model</span><span class="p">;</span>
    <span class="k">public</span> <span class="k">void</span> <span class="nf">LoadModel</span><span class="p">(</span><span class="kt">string</span> <span class="n">model_name</span><span class="p">)</span>
    <span class="p">{</span>
        <span class="kt">string</span> <span class="n">model_path</span> <span class="p">=</span> <span class="n">Application</span><span class="p">.</span><span class="n">dataPath</span> <span class="p">+</span> <span class="s">@"/MLModel/"</span> <span class="p">+</span> <span class="n">model_name</span> <span class="p">+</span> <span class="s">".pb"</span><span class="p">;</span>

        <span class="kt">var</span> <span class="n">model_input</span> <span class="p">=</span> <span class="n">File</span><span class="p">.</span><span class="nf">ReadAllBytes</span><span class="p">(</span><span class="n">model_path</span><span class="p">);</span>
        <span class="n">model</span> <span class="p">=</span> <span class="k">new</span> <span class="nf">TFGraph</span><span class="p">();</span>
        <span class="n">model</span><span class="p">.</span><span class="nf">Import</span><span class="p">(</span><span class="n">model_input</span><span class="p">);</span>

        <span class="n">loaded</span> <span class="p">=</span> <span class="k">true</span><span class="p">;</span>
    <span class="p">}</span>
    <span class="k">private</span> <span class="kt">bool</span> <span class="n">loaded</span> <span class="p">=</span> <span class="k">false</span><span class="p">;</span>
    <span class="k">public</span> <span class="kt">bool</span> <span class="nf">GetLoaded</span><span class="p">()</span> <span class="p">{</span> <span class="k">return</span> <span class="n">loaded</span><span class="p">;</span> <span class="p">}</span>

    <span class="k">public</span> <span class="kt">int</span> <span class="nf">Inference</span><span class="p">(</span><span class="n">Texture2D</span> <span class="n">input</span><span class="p">)</span>
    <span class="p">{</span>
        <span class="kt">var</span> <span class="n">session</span> <span class="p">=</span> <span class="k">new</span> <span class="nf">TFSession</span><span class="p">(</span><span class="n">model</span><span class="p">);</span>

        <span class="c1">// Texture2Dをモデルの入力に合った形に整形、正規化する。        </span>
        <span class="kt">var</span> <span class="n">float_values</span> <span class="p">=</span> <span class="nf">GetFloatFromTex2DWithFlip</span><span class="p">(</span><span class="n">input</span><span class="p">);</span>

        <span class="c1">// TensorFlowSharpでの入力形式であるTFTensorを作成する</span>
        <span class="kt">var</span> <span class="n">shape</span> <span class="p">=</span> <span class="k">new</span> <span class="nf">TFShape</span><span class="p">(</span><span class="m">1</span><span class="p">,</span> <span class="n">input</span><span class="p">.</span><span class="n">width</span><span class="p">,</span> <span class="n">input</span><span class="p">.</span><span class="n">height</span><span class="p">,</span> <span class="m">3</span><span class="p">);</span>
        <span class="kt">var</span> <span class="n">input_tensor</span> <span class="p">=</span> <span class="n">TFTensor</span><span class="p">.</span><span class="nf">FromBuffer</span><span class="p">(</span><span class="n">shape</span><span class="p">,</span> <span class="n">float_values</span><span class="p">,</span> <span class="m">0</span><span class="p">,</span> <span class="n">float_values</span><span class="p">.</span><span class="n">Length</span><span class="p">);</span>

        <span class="c1">//  データの入力・推論</span>
        <span class="c1">//  input_2:0およびoutput_node0:0はpbファイル作成時につけた入力ノードと出力ノードの名前。</span>
        <span class="kt">var</span> <span class="n">runner</span> <span class="p">=</span> <span class="n">session</span><span class="p">.</span><span class="nf">GetRunner</span><span class="p">();</span>
        <span class="n">runner</span><span class="p">.</span><span class="nf">AddInput</span><span class="p">(</span><span class="n">model</span><span class="p">[</span><span class="s">"input"</span><span class="p">][</span><span class="m">0</span><span class="p">],</span> <span class="n">input_tensor</span><span class="p">);</span>
        <span class="n">runner</span><span class="p">.</span><span class="nf">Fetch</span><span class="p">(</span><span class="n">model</span><span class="p">[</span><span class="s">"content_vgg/prob"</span><span class="p">][</span><span class="m">0</span><span class="p">]);</span>

        <span class="c1">// 推論を実行</span>
        <span class="kt">var</span> <span class="n">output</span> <span class="p">=</span> <span class="n">runner</span><span class="p">.</span><span class="nf">Run</span><span class="p">();</span>
        <span class="kt">var</span> <span class="n">scores</span> <span class="p">=</span> <span class="p">((</span><span class="kt">float</span><span class="p">[][])</span><span class="n">output</span><span class="p">[</span><span class="m">0</span><span class="p">].</span><span class="nf">GetValue</span><span class="p">(</span><span class="k">true</span><span class="p">))[</span><span class="m">0</span><span class="p">];</span>

        <span class="k">return</span> <span class="nf">GetBestScorePos</span><span class="p">(</span><span class="n">scores</span><span class="p">);</span>
    <span class="p">}</span>

</code></pre></div></div>

<ul>
<li>利点

<ul>
<li>TensorFlowでサポートされているモデルを全て実行できる(onnxでは一部のパーツが非対応)</li>
<li>参考になるドキュメントが多い</li>
<li>前処理など数値の扱いが容易</li>
</ul>
</li>
<li>欠点

<ul>
<li>推論が非常に遅い(OnnxRuntime 0.2[sec]に対し16.8[sec]) </li>
<li>Python・TensorFlowが使えることが前提となる</li>
<li>モデルの保存・読み込み方法が複雑</li>
</ul>
</li>
</ul>

<p>総評：<br>
バージョン変更によるAPIの変化やpbファイル(TensorFlowのモデル出力形式)の扱いづらさに加え、<br>
推論速度も今回試した手法の中で圧倒的に遅いです。<br>
Unityでディープラーニングをしたいだけなら他を選んだ方がよいと思います。<br>
TensorFlowを使い慣れていて、複雑なモデルを利用したい場合には使えるかもしれません。</p>

<p>参考：<br>
C#でTensorFlowのCNNを動かす。<br>
　<a href="https://qiita.com/Tama_maru/items/6e50edfd8f8dea184d18" class="autolink" id="reference-5a3a25cc477978cf4e25">https://qiita.com/Tama_maru/items/6e50edfd8f8dea184d18</a> <br>
Unityで学習済みのVGG16による1000クラスの画像識別<br>
　<a href="https://qiita.com/dipmizu/items/e46591bd4ab03b02ad38" class="autolink" id="reference-1382ef96fb9093121d29">https://qiita.com/dipmizu/items/e46591bd4ab03b02ad38</a></p>

<h1>
<span id="3-今回実施できなかった手法" class="fragment"></span><a href="#3-%E4%BB%8A%E5%9B%9E%E5%AE%9F%E6%96%BD%E3%81%A7%E3%81%8D%E3%81%AA%E3%81%8B%E3%81%A3%E3%81%9F%E6%89%8B%E6%B3%95"><i class="fa fa-link"></i></a>3. 今回実施できなかった手法</h1>

<p>自分の能力不足や環境の問題で扱えなかった手法です。</p>

<h2>
<span id="--windowsml" class="fragment"></span><a href="#--windowsml"><i class="fa fa-link"></i></a>- WindowsML</h2>

<p>Windows 10に搭載されている機械学習のための実行環境です。<br>
Onnx形式をUWP(Universal Windows Platform)アプリで使うのに最適化されています。</p>

<ul>
<li>利点

<ul>
<li>パッケージの導入を必要としない</li>
<li>参考となるドキュメントがやや多い</li>
</ul>
</li>
<li>欠点

<ul>
<li>テストの度にビルドしなくてはならない</li>
<li>非同期処理が基本となっており、扱いにある程度知識が必要</li>
</ul>
</li>
</ul>

<p>総評：<br>
一応試してはみましたが、UWPアプリが良くわからなかったので一旦保留にしました。<br>
HoloLens界隈でよく使われているようです。<br>
非同期処理が分かるようになったらまた挑戦します。</p>

<p>参考：<br>
公式GitHub<br>
　<a href="https://github.com/Microsoft/Windows-Machine-Learning/tree/master" class="autolink" rel="nofollow noopener" target="_blank">https://github.com/Microsoft/Windows-Machine-Learning/tree/master</a><br>
HoloLensでWindowsMLを試してみる(Unity利用編)<br>
　<a href="http://akihiro-document.azurewebsites.net/post/hololens_windowsmachinelearningunity/" class="autolink" rel="nofollow noopener" target="_blank">http://akihiro-document.azurewebsites.net/post/hololens_windowsmachinelearningunity/</a></p>

<h2>
<span id="--ml-net" class="fragment"></span><a href="#--ml-net"><i class="fa fa-link"></i></a>- ML. Net</h2>

<p>C#上で機械学習を行うためのフレームワークです。<br>
モデル構築・訓練まで行えるので、Unityでの入力で直接学習を進めたりできそうです。<br>
今回はランタイムエラーがどうしても解決できなかったため未実施です。<br>
ネット上のドキュメントも少ないのでしばらくは保留にすると思います。</p>

<p>参考：<br>
公式ドキュメント・チュートリアル<br>
　<a href="https://docs.microsoft.com/ja-jp/dotnet/machine-learning/" class="autolink" rel="nofollow noopener" target="_blank">https://docs.microsoft.com/ja-jp/dotnet/machine-learning/</a></p>

<h2>
<span id="--coreml" class="fragment"></span><a href="#--coreml"><i class="fa fa-link"></i></a>- CoreML</h2>

<p>WindowsMLを紹介したのでこちらも一応。<br>
iOS向けの推論環境で、こちらはOnnxを専用形式に変換してから利用します。<br>
今回はメイン環境がWindowsだったので未実施です。<br>
一応実施例が一つだけありましたが、中でswiftを使っているみたいなので<br>
素のUnityでCoreMLが使えるのかどうかは不明です。<br>
(少なくともNugetにはCoreMLはありませんでした)</p>

<p>参考：<br>
UnityVision-iOS<br>
　<a href="https://github.com/possiblecee/UnityVision-iOS" class="autolink" rel="nofollow noopener" target="_blank">https://github.com/possiblecee/UnityVision-iOS</a></p>

<h2>
<span id="--ironpython" class="fragment"></span><a href="#--ironpython"><i class="fa fa-link"></i></a>- IronPython</h2>

<p>Unity上でpythonのコードを扱えるようになるライブラリです。<br>
python3.x系がサポートされておらず、自分の環境ではtensorflowが動かないので諦めました。<br>
macやLinuxを使っていて、どうしてもPythonしか書きたくない場合には使えるかもしれません。<br>
(Unityでそんな状況が起こるのかは知りませんが)</p>

<p>参考：<br>
公式ページ<br>
　<a href="https://ironpython.net" class="autolink" rel="nofollow noopener" target="_blank">https://ironpython.net</a></p>

<h1>
<span id="4-おまけ" class="fragment"></span><a href="#4-%E3%81%8A%E3%81%BE%E3%81%91"><i class="fa fa-link"></i></a>4. おまけ</h1>

<h2>
<span id="--ml-agents-toolkit" class="fragment"></span><a href="#--ml-agents-toolkit"><i class="fa fa-link"></i></a>- ML-Agents Toolkit</h2>

<p>Unity Technorogyが公式に提供している機械学習用アセットです。<br>
機械学習用アセットとは言うものの、サポートしている機能は強化学習だけなので<br>
画像や音声、文章を扱うモデルには適用できません。<br>
大変面白い機能ですが、今回の目的とは合わないのでおまけ扱いに。  </p>

<p>参考：<br>
公式ドキュメント<br>
ML-Agents Installation<br>
　<a href="https://github.com/Unity-Technologies/ml-agents/blob/master/docs/Installation.md" class="autolink" rel="nofollow noopener" target="_blank">https://github.com/Unity-Technologies/ml-agents/blob/master/docs/Installation.md</a><br>
Basic Guide<br>
　<a href="https://github.com/Unity-Technologies/ml-agents/blob/master/docs/Basic-Guide.md" class="autolink" rel="nofollow noopener" target="_blank">https://github.com/Unity-Technologies/ml-agents/blob/master/docs/Basic-Guide.md</a></p>

<h2>
<span id="--the-unity-inference-enginebarracuda" class="fragment"></span><a href="#--the-unity-inference-enginebarracuda"><i class="fa fa-link"></i></a>- The Unity Inference Engine(Barracuda)</h2>

<p>ml-agents v0.7から導入されたUnity Labs謹製推論エンジンです。<br>
Unityに対応する全てのプラットフォームで推論できる事を目的に開発されているようですが、<br>
現段階ではml-agentsを経由しないと利用できない様子です。 <br>
(多分そんなことはないのだけれど、コードを読んでもよくわかりませんでした)<br>
将来的には独立したUnityパッケージとして<br>
他の深層学習アプリケーションにも対応する予定らしいので、<br>
リリースされたらまた調べてみようと思います。</p>

<p>参考：<br>
公式ブログの該当記事<br>
　<a href="https://blogs.unity3d.com/jp/2019/03/01/unity-ml-agents-toolkit-v0-7-a-leap-towards-cross-platform-inference/" class="autolink" rel="nofollow noopener" target="_blank">https://blogs.unity3d.com/jp/2019/03/01/unity-ml-agents-toolkit-v0-7-a-leap-towards-cross-platform-inference/</a></p>

<h1>
<span id="おわりに" class="fragment"></span><a href="#%E3%81%8A%E3%82%8F%E3%82%8A%E3%81%AB"><i class="fa fa-link"></i></a>おわりに</h1>

<p>あまり時間をかけすぎるのも効率が悪いので一旦打ち切りましたが、<br>
WindowsMLやCoreMLあたりはベースになる言語を勉強した上で再挑戦したいと思います。</p>

<h1>
<span id="今回の内容githubにあげました" class="fragment"></span><a href="#%E4%BB%8A%E5%9B%9E%E3%81%AE%E5%86%85%E5%AE%B9github%E3%81%AB%E3%81%82%E3%81%92%E3%81%BE%E3%81%97%E3%81%9F"><i class="fa fa-link"></i></a>今回の内容(GitHubにあげました)</h1>

<p>プロジェクトファイル<br>
　<a href="https://github.com/highnoRQ/Unity_ML_Test" class="autolink" rel="nofollow noopener" target="_blank">https://github.com/highnoRQ/Unity_ML_Test</a></p>

<p>Assets/MLModel下に以下のモデルファイルを置いてください<br>
VGG19　tensorflow pbファイル<br>
　<a href="https://drive.google.com/file/d/10XFhmr43zIZD4dPKP55hUg9YxKUZ3cBV/view?usp=sharing" class="autolink" rel="nofollow noopener" target="_blank">https://drive.google.com/file/d/10XFhmr43zIZD4dPKP55hUg9YxKUZ3cBV/view?usp=sharing</a><br>
VGG19 onnxファイル<br>
　<a href="https://github.com/onnx/models/tree/master/vision/classification/vgg" class="autolink" rel="nofollow noopener" target="_blank">https://github.com/onnx/models/tree/master/vision/classification/vgg</a></p>

<p>また、ビルドしてテストする場合。<br>
ビルド済みフォルダのDataフォルダにMLModelフォルダごと移してください。</p>
</div></div></section><div class="css-1yzj1fm"><div class="css-1uv1qiv"><span class="fa fa-twitter"></span></div><div class="css-1uv1qiv"><span class="fa fa-facebook"></span></div></div><div class="apm-Content"><div class="apm-Content_title">Why not register and get more from Qiita?</div><ol class="apm-Content_list"><li>We will deliver articles that match you<div class="description">By following users and tags, you can catch up information on technical fields that you are interested in as a whole</div></li><li>you can read useful information later efficiently<div class="description">By &quot;stocking&quot; the articles you like, you can search right away</div></li><div><a class="apm-Content_help" href="https://help.qiita.com/ja/articles/qiita-login-user" target="_blank"><i class="fa fa-fw fa-arrow-circle-right"></i>What you can do with signing up</a></div></ol><a href="/signup?callback_action=login_or_signup&amp;redirect_to=%2Fhighno_RQ%2Fitems%2F7a33e7eaa6d77c93968f&amp;realm=qiita" class="apm-Content_button apm-Content_button-signup">Sign up</a><a href="/login?callback_action=login_or_signup&amp;redirect_to=%2Fhighno_RQ%2Fitems%2F7a33e7eaa6d77c93968f&amp;realm=qiita" class="apm-Content_button apm-Content_button-signin">Login</a></div></div><div class="css-helsa7"></div></div></div></div><div class="css-109dbrr"><div class="css-5jpx49"><div class="css-mnxgyc"><button class=" css-1vlpknv"><svg size="20" xmlns="http://www.w3.org/2000/svg" viewBox="0 0 392.81 429" color="#55C500" class="css-1g4cku8 e31pr5q0"><path d="M14.19 5.4h53.86v149.45h90.05v44.87H14.19zM288.4 93.77h100.79q1.29 25-5.66 45.39a96.79 96.79 0 01-20.33 34.89 92 92 0 01-32.13 22.45 104 104 0 01-40.95 7.93 109.71 109.71 0 01-76-29.92 104.05 104.05 0 01-23-32.56 95.46 95.46 0 01-8.47-39.88 94.78 94.78 0 018.47-39.87 104.38 104.38 0 0123-32.42A107.71 107.71 0 01248.23 8a110.79 110.79 0 01118.48 22.49l-35.07 35.08a51.25 51.25 0 00-17.75-15 52.83 52.83 0 00-44.67-1.23 52.92 52.92 0 00-17 12 57.07 57.07 0 00-11.45 18.11 60 60 0 00-4.23 22.77 60 60 0 004.23 22.68 56.57 56.57 0 0011.45 18.19 52.62 52.62 0 0017 12 50.5 50.5 0 0020.9 4.36q20.19 0 31.07-7.51a35.75 35.75 0 0014.46-20.55h-47.39zM51.29 279.55H0v-44.86h156v44.86h-51.13V429H51.29zM283.36 381.71l-41.72-62V429h-53.86V234.69h47.47L290 312l55.9-77.29h46.9V429h-53.86V320.27l-42.43 61.44z"></path></svg></button><a href="/highno_RQ/items/7a33e7eaa6d77c93968f/likers" class="css-1iupg5d">102</a></div><div class="css-fsjkhv"><button class=" css-15cocm3"><svg size="16" xmlns="http://www.w3.org/2000/svg" viewBox="0 0 353.02 398" color="#6E6F70" class="css-12rp90f e11v00bf0"><path d="M176.72 398c-67.52 0-130.16-29-171.84-79.69l-4.46-5.42V78.05H353v234.84l-4.45 5.42C306.88 369 244.24 398 176.72 398zm-137.2-99.34c34.17 38.37 83.78 60.25 137.2 60.25s103-21.88 137.21-60.25V117.14H39.52zM0 0h351.12v40.94H0z"></path></svg></button><span class="css-1b17vb0">94</span></div><div class="css-79elbk"><button class="css-16hhh7b"><i class="fa fa-ellipsis-h"></i></button><div class="css-7i7f4d"><div class="css-1gj7nt">Improve article</div><a href="/drafts/7a33e7eaa6d77c93968f/edit" class="css-154zy0m"><span class="css-yikrym"><i class="fa fa-code-fork css-1jqivyb" aria-hidden="true"></i></span>Send edit request</a><div class="css-1ode1bp"></div><div class="css-1gj7nt">Article information</div><a href="/highno_RQ/items/7a33e7eaa6d77c93968f/revisions" class="css-154zy0m"><span class="css-yikrym"><i class="fa fa-history css-1jqivyb" aria-hidden="true"></i></span>Revisions</a><a href="/highno_RQ/items/7a33e7eaa6d77c93968f/patches" class="css-154zy0m"><span class="css-yikrym"><i class="fa fa-inbox css-1jqivyb" aria-hidden="true"></i></span>Edit Requests</a><a href="/highno_RQ/items/7a33e7eaa6d77c93968f/likers" class="css-154zy0m"><span class="css-yikrym"><svg size="13" xmlns="http://www.w3.org/2000/svg" viewBox="0 0 392.81 429" color="rgba(0, 0, 0, 0.6)" class="css-le4d8r e31pr5q0"><path d="M14.19 5.4h53.86v149.45h90.05v44.87H14.19zM288.4 93.77h100.79q1.29 25-5.66 45.39a96.79 96.79 0 01-20.33 34.89 92 92 0 01-32.13 22.45 104 104 0 01-40.95 7.93 109.71 109.71 0 01-76-29.92 104.05 104.05 0 01-23-32.56 95.46 95.46 0 01-8.47-39.88 94.78 94.78 0 018.47-39.87 104.38 104.38 0 0123-32.42A107.71 107.71 0 01248.23 8a110.79 110.79 0 01118.48 22.49l-35.07 35.08a51.25 51.25 0 00-17.75-15 52.83 52.83 0 00-44.67-1.23 52.92 52.92 0 00-17 12 57.07 57.07 0 00-11.45 18.11 60 60 0 00-4.23 22.77 60 60 0 004.23 22.68 56.57 56.57 0 0011.45 18.19 52.62 52.62 0 0017 12 50.5 50.5 0 0020.9 4.36q20.19 0 31.07-7.51a35.75 35.75 0 0014.46-20.55h-47.39zM51.29 279.55H0v-44.86h156v44.86h-51.13V429H51.29zM283.36 381.71l-41.72-62V429h-53.86V234.69h47.47L290 312l55.9-77.29h46.9V429h-53.86V320.27l-42.43 61.44z"></path></svg></span>Show all likers</a><a href="/highno_RQ/items/7a33e7eaa6d77c93968f.md" class="css-154zy0m"><span class="css-yikrym"><i class="fa fa-file-text-o css-1jqivyb" aria-hidden="true"></i></span>Show article in Markdown</a><div class="css-1ode1bp"></div><div class="css-154zy0m"><span class="css-yikrym"><i class="fa fa-flag css-1jqivyb" aria-hidden="true"></i></span>Report article</div></div><div class="st-Modal"><div class="st-Modal_backdrop"></div><div class="st-Modal_body"><form><div class="st-Form"><span class="st-Form_label">Help us understand the problem. What is going on with this article?</span></div><div class="st-Form"><label><input type="radio" name="reason" value="illegal" required=""/>It&#x27;s illegal (copyright infringement, privacy infringement, libel, etc.)</label></div><div class="st-Form"><label><input type="radio" name="reason" value="inappropriate_content" required=""/>It&#x27;s socially inappropriate (offensive to public order and morals)</label></div><div class="st-Form"><label><input type="radio" name="reason" value="advertising" required=""/>It&#x27;s advertising</label></div><div class="st-Form"><label><input type="radio" name="reason" value="spam" required=""/>It&#x27;s spam</label></div><div class="st-Form"><label><input type="radio" name="reason" value="guideline_violation" required=""/>Other than the above, but not suitable for the Qiita community (violation of guidelines)</label></div><div class="st-Form st-Form-right"><input type="submit" class="st-Form_submit" value="Submit"/></div></form></div></div></div></div></div></div>
      <script type="application/json" class="js-react-on-rails-component" data-component-name="PersonalArticlePage" data-dom-id="PersonalArticlePage-react-component-39c5d909-6029-48a7-bc6a-c577e131fed2">{"authorAnalyticsTrackingId":null,"organizationAnalyticsTrackingId":null}</script>
      
<footer id="globalFooter" class="st-Footer"><div class="st-Footer_container"><div class="st-Footer_start"><div class="st-Footer_logo"><svg viewbox="0 0 426.57 130" xmlns="http://www.w3.org/2000/svg"><circle cx="167.08" cy="21.4" r="12.28" /><path d="M250.81 29.66h23.48v18.9h-23.48z" /><path d="M300.76 105.26a22.23 22.23 0 01-6.26-.86 12.68 12.68 0 01-5.17-3 14.41 14.41 0 01-3.56-5.76 28 28 0 01-1.3-9.22V48.56h29.61v-18.9h-29.52V3.29h-20.17v83.34q0 11.16 2.83 18.27a27.71 27.71 0 007.7 11.2 26.86 26.86 0 0011.43 5.62 47.56 47.56 0 0012.34 1.53h15.16v-18zM0 61.7a58.6 58.6 0 015-24.21A62.26 62.26 0 0118.73 17.9 63.72 63.72 0 0139 4.78 64.93 64.93 0 0164 0a65 65 0 0124.85 4.78 64.24 64.24 0 0120.38 13.12A62 62 0 01123 37.49a58.6 58.6 0 015 24.21 58.34 58.34 0 01-4 21.46 62.8 62.8 0 01-10.91 18.16l11.1 11.1a10.3 10.3 0 010 14.52 10.29 10.29 0 01-14.64 0l-12.22-12.41a65 65 0 01-15.78 6.65 66.32 66.32 0 01-17.55 2.3 64.63 64.63 0 01-45.23-18A62.82 62.82 0 015 85.81 58.3 58.3 0 010 61.7zm21.64.08a43.13 43.13 0 0012.42 30.63 42.23 42.23 0 0013.43 9.09A41.31 41.31 0 0064 104.8a42 42 0 0030-12.39 42.37 42.37 0 009-13.64 43.43 43.43 0 003.3-17 43.77 43.77 0 00-3.3-17A41.7 41.7 0 0080.55 22 41.78 41.78 0 0064 18.68 41.31 41.31 0 0047.49 22a42.37 42.37 0 00-13.43 9.08 43.37 43.37 0 00-12.42 30.7zM331.89 78a47.59 47.59 0 013.3-17.73 43.22 43.22 0 019.34-14.47A44.25 44.25 0 01359 36a47.82 47.82 0 0118.81-3.58 42.72 42.72 0 019.26 1 46.5 46.5 0 018.22 2.58 40 40 0 017 3.84 44.39 44.39 0 015.71 4.63l1.22-9.47h17.35v85.83h-17.35l-1.17-9.42a42.54 42.54 0 01-5.84 4.67 43.11 43.11 0 01-7 3.79 44.86 44.86 0 01-8.17 2.59 43 43 0 01-9.22 1A47.94 47.94 0 01359 119.9a43.3 43.3 0 01-14.47-9.71 44.17 44.17 0 01-9.34-14.47 47 47 0 01-3.3-17.72zm20.27-.08a29.16 29.16 0 002.17 11.34 27 27 0 005.92 8.88 26.69 26.69 0 008.76 5.76 29.19 29.19 0 0021.44 0 26.11 26.11 0 008.72-5.76 27.57 27.57 0 005.88-8.84 29 29 0 002.16-11.38 28.62 28.62 0 00-2.16-11.22 26.57 26.57 0 00-5.93-8.8 27.68 27.68 0 00-19.51-7.9 28.29 28.29 0 00-10.77 2.05 26.19 26.19 0 00-8.71 5.75 27.08 27.08 0 00-5.84 8.8 28.94 28.94 0 00-2.13 11.31zm-194.97-30.5h19.78v73.54h-19.78zm49.25 0h19.78v73.54h-19.78z" /><circle cx="216.33" cy="21.4" r="12.28" /></svg></div><div class="st-Footer_catchcopy">How developers code is here.</div><div class="st-Footer_socials"><a class="fa fa-twitter" href="https://twitter.com/qiita"></a><a class="fa fa-facebook-square" href="https://www.facebook.com/qiita/"></a></div></div><div class="st-Footer_end"><div class="st-Footer_qiita"><div class="st-Footer_label">Qiita</div><div class="st-Footer_list"><div class="st-Footer_column"><a href="/about">About</a><a href="/terms">Terms</a><a href="/privacy">Privacy</a><a target="_blank" href="http://help.qiita.com/ja/articles/qiita-community-guideline">Guideline</a><a target="_blank" href="https://help.qiita.com/ja/articles/others-brand-guideline">Design Guideline</a></div><div class="st-Footer_column"><a href="/release-notes">Release</a><a href="/api/v2/docs">API</a><a href="/feedback/new">ご意見</a><a href="https://help.qiita.com">Help</a><a target="_blank" href="https://qiita.com/ads?utm_source=qiita&amp;utm_medium=referral&amp;utm_content=footer">Advertisement</a></div></div></div><div class="st-Footer_increments"><div class="st-Footer_label">Increments</div><div class="st-Footer_list"><div class="st-Footer_column"><a href="https://increments.co.jp/company/">About</a><a href="https://increments.co.jp/jobs/">採用情報</a><a href="https://blog.qiita.com">Blog</a></div><div class="st-Footer_column"><a href="https://teams.qiita.com/">Qiita Team</a><a href="https://jobs.qiita.com?utm_source=qiita&amp;utm_medium=referral&amp;utm_content=footer">Qiita Jobs</a><a href="https://zine.qiita.com?utm_source=qiita&amp;utm_medium=referral&amp;utm_content=footer">Qiita Zine</a></div></div></div></div></div><div class="st-Footer_copyright">© 2011-2021 Increments Inc.</div></footer><div id="Snackbar-react-component-a75710f3-ea2d-4e60-9e52-9554fa67c06f"></div>
      <script type="application/json" class="js-react-on-rails-component" data-component-name="Snackbar" data-dom-id="Snackbar-react-component-a75710f3-ea2d-4e60-9e52-9554fa67c06f">{}</script>
      
<div id="LoginModal-react-component-574adab7-1bb6-4839-a90a-389a453a9114"></div>
      <script type="application/json" class="js-react-on-rails-component" data-component-name="LoginModal" data-dom-id="LoginModal-react-component-574adab7-1bb6-4839-a90a-389a453a9114">{}</script>
      
<div id="StockModal-react-component-1f8532b2-cc5f-4813-ae52-9e20ffaa77ba"></div>
      <script type="application/json" class="js-react-on-rails-component" data-component-name="StockModal" data-dom-id="StockModal-react-component-1f8532b2-cc5f-4813-ae52-9e20ffaa77ba">{}</script>
      
</div><div id="dataContainer" style="display: none;" data-config="{&quot;actionPath&quot;:&quot;public/items#show&quot;,&quot;settings&quot;:{&quot;analyticsTrackingId&quot;:&quot;UA-24675221-12&quot;,&quot;assetsMap&quot;:{},&quot;csrfToken&quot;:&quot;E6ay3938j/bcpKYyfQ/kLmvtLXAXEnySTExh1kHD/nj0tH9yoWH0ej5/6tsIreC7QPg8832A1C5/Q4IXcEePMQ==&quot;,&quot;locale&quot;:&quot;en&quot;},&quot;currentUser&quot;:null}" /></body></html><script type="application/json" data-js-react-on-rails-store="AppStoreWithReactOnRails">{"snackbar":{"type":"","body":"","isActive":false},"article":{"article":{"body":"\n\u003ch1\u003e\n\u003cspan id=\"202072追記\" class=\"fragment\"\u003e\u003c/span\u003e\u003ca href=\"#202072%E8%BF%BD%E8%A8%98\"\u003e\u003ci class=\"fa fa-link\"\u003e\u003c/i\u003e\u003c/a\u003e2020/7/2追記\u003c/h1\u003e\n\n\u003cp\u003eBarracudaの記事を書きました\u003cbr\u003e\nこの記事の手法より圧倒的に簡単なので、こっち利用推奨です！！\u003cbr\u003e\n\u003cstrong\u003e\u003ca href=\"https://qiita.com/highno_RQ/items/478e1145f0eb868c0f2e\" id=\"reference-e9987542498d7c5c7318\"\u003eUnity Technologies製推論エンジン Barracudaがスゴイという話\u003c/a\u003e\u003c/strong\u003e\u003c/p\u003e\n\n\u003ch1\u003e\n\u003cspan id=\"2020512追記\" class=\"fragment\"\u003e\u003c/span\u003e\u003ca href=\"#2020512%E8%BF%BD%E8%A8%98\"\u003e\u003ci class=\"fa fa-link\"\u003e\u003c/i\u003e\u003c/a\u003e2020/5/12追記\u003c/h1\u003e\n\n\u003cp\u003eUnity製推論エンジンのBarracudaがonnxからモデルを直接読み込めるようになっていたようです。\u003cbr\u003e\nこの記事で書いた程度の内容であればこんな面倒な手順を踏まなくてもいけそうです。\u003cbr\u003e\n\u003cdel\u003eBarracudaのテストについては後日実施します(おそらく6月中)\u003c/del\u003e\u003c/p\u003e\n\n\u003cp\u003e※参考\u003cbr\u003e\n\u003ca href=\"https://docs.unity3d.com/Packages/com.unity.barracuda@0.7/manual/index.html\" rel=\"nofollow noopener\" target=\"_blank\"\u003e公式ドキュメント ver0.70\u003c/a\u003e\u003cbr\u003e\n\u003ca href=\"https://note.com/npaka/n/nf7f76871af8f\" rel=\"nofollow noopener\" target=\"_blank\"\u003eUnity Barracuda - ニューラルネットワーク推論ライブラリ\u003c/a\u003e\u003c/p\u003e\n\n\u003ch1\u003e\n\u003cspan id=\"概要\" class=\"fragment\"\u003e\u003c/span\u003e\u003ca href=\"#%E6%A6%82%E8%A6%81\"\u003e\u003ci class=\"fa fa-link\"\u003e\u003c/i\u003e\u003c/a\u003e概要\u003c/h1\u003e\n\n\u003cp\u003eARで深度推定とかするのにml-agentsだけだと足りなかったので\u003cbr\u003e\nUnityで学習済みモデルを扱う方法を調べて試してみました。\u003c/p\u003e\n\n\u003ch1\u003e\n\u003cspan id=\"実施内容\" class=\"fragment\"\u003e\u003c/span\u003e\u003ca href=\"#%E5%AE%9F%E6%96%BD%E5%86%85%E5%AE%B9\"\u003e\u003ci class=\"fa fa-link\"\u003e\u003c/i\u003e\u003c/a\u003e実施内容\u003c/h1\u003e\n\n\u003cp\u003e学習済みモデルVGG19を利用してUnity内に表示した画像の分類を行う。\u003cbr\u003e\n\u003ca href=\"https://camo.qiitausercontent.com/13921305ae9dbdf05abe82482997f2c90f27dd14/68747470733a2f2f71696974612d696d6167652d73746f72652e73332e61702d6e6f727468656173742d312e616d617a6f6e6177732e636f6d2f302f3434303330392f64636565383635322d363832382d626638342d666664322d3863313966313230303730392e706e67\" target=\"_blank\" rel=\"nofollow noopener\"\u003e\u003cimg src=\"https://qiita-user-contents.imgix.net/https%3A%2F%2Fqiita-image-store.s3.ap-northeast-1.amazonaws.com%2F0%2F440309%2Fdcee8652-6828-bf84-ffd2-8c19f1200709.png?ixlib=rb-4.0.0\u0026amp;auto=format\u0026amp;gif-q=60\u0026amp;q=75\u0026amp;s=f37962d877c4a385893cf0b3aa1069fa\" alt=\"キャプチャ.PNG\" data-canonical-src=\"https://qiita-image-store.s3.ap-northeast-1.amazonaws.com/0/440309/dcee8652-6828-bf84-ffd2-8c19f1200709.png\" srcset=\"https://qiita-user-contents.imgix.net/https%3A%2F%2Fqiita-image-store.s3.ap-northeast-1.amazonaws.com%2F0%2F440309%2Fdcee8652-6828-bf84-ffd2-8c19f1200709.png?ixlib=rb-4.0.0\u0026amp;auto=format\u0026amp;gif-q=60\u0026amp;q=75\u0026amp;w=1400\u0026amp;fit=max\u0026amp;s=7fa5e57312a3518b3d9118e5c5d77d20 1x\" loading=\"lazy\"\u003e\u003c/a\u003e\u003c/p\u003e\n\n\u003ch2\u003e\n\u003cspan id=\"モデルの形式\" class=\"fragment\"\u003e\u003c/span\u003e\u003ca href=\"#%E3%83%A2%E3%83%87%E3%83%AB%E3%81%AE%E5%BD%A2%E5%BC%8F\"\u003e\u003ci class=\"fa fa-link\"\u003e\u003c/i\u003e\u003c/a\u003eモデルの形式\u003c/h2\u003e\n\n\u003ctable\u003e\n\u003cthead\u003e\n\u003ctr\u003e\n\u003cth style=\"text-align: left\"\u003e\u003c/th\u003e\n\u003cth style=\"text-align: left\"\u003e\u003c/th\u003e\n\u003c/tr\u003e\n\u003c/thead\u003e\n\u003ctbody\u003e\n\u003ctr\u003e\n\u003ctd style=\"text-align: left\"\u003eモデル名\u003c/td\u003e\n\u003ctd style=\"text-align: left\"\u003eVGG-19\u003c/td\u003e\n\u003c/tr\u003e\n\u003ctr\u003e\n\u003ctd style=\"text-align: left\"\u003e入力\u003c/td\u003e\n\u003ctd style=\"text-align: left\"\u003e224×224×3\u003c/td\u003e\n\u003c/tr\u003e\n\u003ctr\u003e\n\u003ctd style=\"text-align: left\"\u003e出力\u003c/td\u003e\n\u003ctd style=\"text-align: left\"\u003e1000\u003c/td\u003e\n\u003c/tr\u003e\n\u003ctr\u003e\n\u003ctd style=\"text-align: left\"\u003eデータセット\u003c/td\u003e\n\u003ctd style=\"text-align: left\"\u003eImageNet (ILSVRCA2012)\u003c/td\u003e\n\u003c/tr\u003e\n\u003c/tbody\u003e\n\u003c/table\u003e\n\n\u003ch2\u003e\n\u003cspan id=\"前処理\" class=\"fragment\"\u003e\u003c/span\u003e\u003ca href=\"#%E5%89%8D%E5%87%A6%E7%90%86\"\u003e\u003ci class=\"fa fa-link\"\u003e\u003c/i\u003e\u003c/a\u003e前処理\u003c/h2\u003e\n\n\u003col\u003e\n\u003cli\u003e8bitで表された画像を0-1範囲に変換　= pixel / 255\u003c/li\u003e\n\u003cli\u003eRGBの各値を平均{ 0.485f, 0.456f, 0.406f } 標準偏差　{ 0.229f, 0.224f, 0.225f }で正規化\u003c/li\u003e\n\u003c/ol\u003e\n\n\u003ch1\u003e\n\u003cspan id=\"環境\" class=\"fragment\"\u003e\u003c/span\u003e\u003ca href=\"#%E7%92%B0%E5%A2%83\"\u003e\u003ci class=\"fa fa-link\"\u003e\u003c/i\u003e\u003c/a\u003e環境\u003c/h1\u003e\n\n\u003ctable\u003e\n\u003cthead\u003e\n\u003ctr\u003e\n\u003cth style=\"text-align: left\"\u003e\u003c/th\u003e\n\u003cth style=\"text-align: left\"\u003e\u003c/th\u003e\n\u003c/tr\u003e\n\u003c/thead\u003e\n\u003ctbody\u003e\n\u003ctr\u003e\n\u003ctd style=\"text-align: left\"\u003eOS\u003c/td\u003e\n\u003ctd style=\"text-align: left\"\u003eWindows10 Home\u003c/td\u003e\n\u003c/tr\u003e\n\u003ctr\u003e\n\u003ctd style=\"text-align: left\"\u003eCPU\u003c/td\u003e\n\u003ctd style=\"text-align: left\"\u003eIntel(R) Core(TM) i7-7700K\u003c/td\u003e\n\u003c/tr\u003e\n\u003ctr\u003e\n\u003ctd style=\"text-align: left\"\u003eUnity\u003c/td\u003e\n\u003ctd style=\"text-align: left\"\u003e2019.1.0f2(64-bit)\u003c/td\u003e\n\u003c/tr\u003e\n\u003ctr\u003e\n\u003ctd style=\"text-align: left\"\u003etensorflow\u003c/td\u003e\n\u003ctd style=\"text-align: left\"\u003e1.14.0\u003c/td\u003e\n\u003c/tr\u003e\n\u003c/tbody\u003e\n\u003c/table\u003e\n\n\u003cp\u003e今回、ライブラリの導入に\u003ca href=\"https://www.nuget.org\" rel=\"nofollow noopener\" target=\"_blank\"\u003eNuGet\u003c/a\u003eを利用しました。\u003cbr\u003e\nUnityではVisualStudioのNuGetパッケージマネージャーは使えないので以下の2手法を利用します。\u003c/p\u003e\n\n\u003col\u003e\n\u003cli\u003eNugetForUnity Assetの利用\nAssetStoreから\u003ca href=\"https://assetstore.unity.com/packages/tools/utilities/nuget-for-unity-104640\" rel=\"nofollow noopener\" target=\"_blank\"\u003eNuget For Unity\u003c/a\u003eをインストールして利用します。\u003c/li\u003e\n\u003cli\u003e公式ドキュメント記載の方法\n公式ドキュメント 「\u003ca href=\"https://docs.microsoft.com/ja-jp/visualstudio/cross-platform/unity-scripting-upgrade?view=vs-2019#add-packages-from-nuget-to-a-unity-project\" rel=\"nofollow noopener\" target=\"_blank\"\u003eNuGetからUnity プロジェクトにパッケージを追加する\u003c/a\u003e」を参照してください。\u003c/li\u003e\n\u003c/ol\u003e\n\n\u003cp\u003eどちらの手法でもパッケージ同士の依存関係は手動で特定する必要があります。\u003c/p\u003e\n\n\u003cp\u003e参考：\u003cbr\u003e\nUnity であらゆる C#(.NET) パッケージを使う（例：opencv）\u003cbr\u003e\n　\u003ca href=\"https://qiita.com/kingyo222/items/11100e8f7be396b98453\" class=\"autolink\" id=\"reference-bd62df36b1b8f3bdbcb2\"\u003ehttps://qiita.com/kingyo222/items/11100e8f7be396b98453\u003c/a\u003e\u003c/p\u003e\n\n\u003ch1\u003e\n\u003cspan id=\"手法一覧\" class=\"fragment\"\u003e\u003c/span\u003e\u003ca href=\"#%E6%89%8B%E6%B3%95%E4%B8%80%E8%A6%A7\"\u003e\u003ci class=\"fa fa-link\"\u003e\u003c/i\u003e\u003c/a\u003e手法一覧\u003c/h1\u003e\n\n\u003col\u003e\n\u003cli\u003eOnnx形式で学習済みモデルを扱う\n\n\u003cul\u003e\n\u003cli\u003eOnnx Runtime\u003c/li\u003e\n\u003cli\u003eOpenCVSharp\u003c/li\u003e\n\u003c/ul\u003e\n\u003c/li\u003e\n\u003cli\u003eC#から直接tensorflowを扱う\n\n\u003cul\u003e\n\u003cli\u003eTensorFlowSharp\u003c/li\u003e\n\u003c/ul\u003e\n\u003c/li\u003e\n\u003cli\u003e今回実施できなかった手法\n\n\u003cul\u003e\n\u003cli\u003eWindowsML\u003c/li\u003e\n\u003cli\u003eML. Net\u003c/li\u003e\n\u003cli\u003eCoreML\u003c/li\u003e\n\u003cli\u003eIronPython\u003c/li\u003e\n\u003c/ul\u003e\n\u003c/li\u003e\n\u003cli\u003eおまけ\n\n\u003cul\u003e\n\u003cli\u003eml-agents\u003c/li\u003e\n\u003cli\u003eThe Unity Inference Engine\u003c/li\u003e\n\u003c/ul\u003e\n\u003c/li\u003e\n\u003c/ol\u003e\n\n\u003ch1\u003e\n\u003cspan id=\"1-onnx形式で学習済みモデルを扱う\" class=\"fragment\"\u003e\u003c/span\u003e\u003ca href=\"#1-onnx%E5%BD%A2%E5%BC%8F%E3%81%A7%E5%AD%A6%E7%BF%92%E6%B8%88%E3%81%BF%E3%83%A2%E3%83%87%E3%83%AB%E3%82%92%E6%89%B1%E3%81%86\"\u003e\u003ci class=\"fa fa-link\"\u003e\u003c/i\u003e\u003c/a\u003e1. Onnx形式で学習済みモデルを扱う\u003c/h1\u003e\n\n\u003ch2\u003e\n\u003cspan id=\"概要-1\" class=\"fragment\"\u003e\u003c/span\u003e\u003ca href=\"#%E6%A6%82%E8%A6%81-1\"\u003e\u003ci class=\"fa fa-link\"\u003e\u003c/i\u003e\u003c/a\u003e概要\u003c/h2\u003e\n\n\u003cp\u003e機械学習モデルの標準形式であるOnnx(Open Neural Network Exchange)形式を利用します。\u003cbr\u003e\nこの形式に変換することで、TensorflowやPytorch,Chainerといった\u003cbr\u003e\n主要な深層学習ライブラリで学習したモデルを色々なライブラリで利用できるようになります。\u003c/p\u003e\n\n\u003cp\u003e参考：\u003cbr\u003e\nONNX Model Zoo(訓練済みモデルの配布場所)\u003cbr\u003e\n　\u003ca href=\"https://github.com/onnx/models\" class=\"autolink\" rel=\"nofollow noopener\" target=\"_blank\"\u003ehttps://github.com/onnx/models\u003c/a\u003e\u003c/p\u003e\n\n\u003ch2\u003e\n\u003cspan id=\"--onnx-runtime\" class=\"fragment\"\u003e\u003c/span\u003e\u003ca href=\"#--onnx-runtime\"\u003e\u003ci class=\"fa fa-link\"\u003e\u003c/i\u003e\u003c/a\u003e- Onnx Runtime\u003c/h2\u003e\n\n\u003cp\u003eOnnx形式を扱う事に特化した推論エンジンです。\u003cbr\u003e\n2018/12/04にOSS(MITライセンス)化されたこともあり、使い所は色々とありそうです。\u003c/p\u003e\n\n\u003cdiv class=\"code-frame\" data-lang=\"C#\"\u003e\u003cdiv class=\"highlight\"\u003e\u003cpre class=\"with-code\"\u003e\u003ccode\u003e    \u003cspan class=\"k\"\u003eprivate\u003c/span\u003e \u003cspan class=\"n\"\u003eInferenceSession\u003c/span\u003e \u003cspan class=\"n\"\u003esession\u003c/span\u003e\u003cspan class=\"p\"\u003e;\u003c/span\u003e\n    \u003cspan class=\"k\"\u003epublic\u003c/span\u003e \u003cspan class=\"k\"\u003evoid\u003c/span\u003e \u003cspan class=\"nf\"\u003eLoadModel\u003c/span\u003e \u003cspan class=\"p\"\u003e(\u003c/span\u003e\u003cspan class=\"kt\"\u003estring\u003c/span\u003e \u003cspan class=\"n\"\u003emodel_name\u003c/span\u003e\u003cspan class=\"p\"\u003e)\u003c/span\u003e \u003cspan class=\"p\"\u003e{\u003c/span\u003e\n        \u003cspan class=\"kt\"\u003estring\u003c/span\u003e \u003cspan class=\"n\"\u003emodel_path\u003c/span\u003e \u003cspan class=\"p\"\u003e=\u003c/span\u003e \u003cspan class=\"n\"\u003eApplication\u003c/span\u003e\u003cspan class=\"p\"\u003e.\u003c/span\u003e\u003cspan class=\"n\"\u003edataPath\u003c/span\u003e \u003cspan class=\"p\"\u003e+\u003c/span\u003e \u003cspan class=\"s\"\u003e@\"/MLModel/\"\u003c/span\u003e \u003cspan class=\"p\"\u003e+\u003c/span\u003e \u003cspan class=\"n\"\u003emodel_name\u003c/span\u003e \u003cspan class=\"p\"\u003e+\u003c/span\u003e \u003cspan class=\"s\"\u003e\".onnx\"\u003c/span\u003e\u003cspan class=\"p\"\u003e;\u003c/span\u003e\n\n        \u003cspan class=\"kt\"\u003evar\u003c/span\u003e \u003cspan class=\"n\"\u003eopitions\u003c/span\u003e \u003cspan class=\"p\"\u003e=\u003c/span\u003e \u003cspan class=\"k\"\u003enew\u003c/span\u003e \u003cspan class=\"nf\"\u003eSessionOptions\u003c/span\u003e \u003cspan class=\"p\"\u003e();\u003c/span\u003e\n        \u003cspan class=\"n\"\u003esession\u003c/span\u003e \u003cspan class=\"p\"\u003e=\u003c/span\u003e \u003cspan class=\"k\"\u003enew\u003c/span\u003e \u003cspan class=\"nf\"\u003eInferenceSession\u003c/span\u003e \u003cspan class=\"p\"\u003e(\u003c/span\u003e\u003cspan class=\"n\"\u003emodel_path\u003c/span\u003e\u003cspan class=\"p\"\u003e);\u003c/span\u003e\n    \u003cspan class=\"p\"\u003e}\u003c/span\u003e\n    \u003cspan class=\"k\"\u003epublic\u003c/span\u003e \u003cspan class=\"kt\"\u003eint\u003c/span\u003e \u003cspan class=\"nf\"\u003eInference\u003c/span\u003e \u003cspan class=\"p\"\u003e(\u003c/span\u003e\u003cspan class=\"n\"\u003eTexture2D\u003c/span\u003e \u003cspan class=\"n\"\u003einput\u003c/span\u003e\u003cspan class=\"p\"\u003e)\u003c/span\u003e \u003cspan class=\"p\"\u003e{\u003c/span\u003e\n        \u003cspan class=\"kt\"\u003evar\u003c/span\u003e \u003cspan class=\"n\"\u003einput_nodes_name\u003c/span\u003e \u003cspan class=\"p\"\u003e=\u003c/span\u003e \u003cspan class=\"n\"\u003esession\u003c/span\u003e\u003cspan class=\"p\"\u003e.\u003c/span\u003e\u003cspan class=\"n\"\u003eInputMetadata\u003c/span\u003e\u003cspan class=\"p\"\u003e.\u003c/span\u003e\u003cspan class=\"nf\"\u003eFirst\u003c/span\u003e\u003cspan class=\"p\"\u003e().\u003c/span\u003e\u003cspan class=\"n\"\u003eKey\u003c/span\u003e\u003cspan class=\"p\"\u003e;\u003c/span\u003e\n        \u003cspan class=\"kt\"\u003evar\u003c/span\u003e \u003cspan class=\"n\"\u003einput_nodes_dim\u003c/span\u003e \u003cspan class=\"p\"\u003e=\u003c/span\u003e \u003cspan class=\"n\"\u003esession\u003c/span\u003e\u003cspan class=\"p\"\u003e.\u003c/span\u003e\u003cspan class=\"n\"\u003eInputMetadata\u003c/span\u003e\u003cspan class=\"p\"\u003e.\u003c/span\u003e\u003cspan class=\"nf\"\u003eFirst\u003c/span\u003e\u003cspan class=\"p\"\u003e().\u003c/span\u003e\u003cspan class=\"n\"\u003eValue\u003c/span\u003e\u003cspan class=\"p\"\u003e.\u003c/span\u003e\u003cspan class=\"n\"\u003eDimensions\u003c/span\u003e\u003cspan class=\"p\"\u003e;\u003c/span\u003e\n\n        \u003cspan class=\"c1\"\u003e// Texture2Dをモデルの入力に合った形に整形、正規化する。\u003c/span\u003e\n        \u003cspan class=\"kt\"\u003evar\u003c/span\u003e \u003cspan class=\"n\"\u003einput_floats\u003c/span\u003e \u003cspan class=\"p\"\u003e=\u003c/span\u003e \u003cspan class=\"nf\"\u003eGetFloatFromTex2DWithFlip\u003c/span\u003e \u003cspan class=\"p\"\u003e(\u003c/span\u003e\u003cspan class=\"n\"\u003einput\u003c/span\u003e\u003cspan class=\"p\"\u003e);\u003c/span\u003e\n        \u003cspan class=\"kt\"\u003evar\u003c/span\u003e \u003cspan class=\"n\"\u003einput_tensor\u003c/span\u003e \u003cspan class=\"p\"\u003e=\u003c/span\u003e \u003cspan class=\"k\"\u003enew\u003c/span\u003e \u003cspan class=\"n\"\u003eDenseTensor\u003c/span\u003e\u003cspan class=\"p\"\u003e\u0026lt;\u003c/span\u003e\u003cspan class=\"kt\"\u003efloat\u003c/span\u003e\u003cspan class=\"p\"\u003e\u0026gt;\u003c/span\u003e \u003cspan class=\"p\"\u003e(\u003c/span\u003e\u003cspan class=\"n\"\u003einput_floats\u003c/span\u003e\u003cspan class=\"p\"\u003e,\u003c/span\u003e \u003cspan class=\"n\"\u003einput_nodes_dim\u003c/span\u003e\u003cspan class=\"p\"\u003e);\u003c/span\u003e\n\n        \u003cspan class=\"c1\"\u003e// OnnxRuntimeでの入力形式であるNamedOnnxValueを作成する\u003c/span\u003e\n        \u003cspan class=\"kt\"\u003evar\u003c/span\u003e \u003cspan class=\"n\"\u003einput_onnx_values\u003c/span\u003e \u003cspan class=\"p\"\u003e=\u003c/span\u003e \u003cspan class=\"k\"\u003enew\u003c/span\u003e \u003cspan class=\"n\"\u003eList\u003c/span\u003e\u003cspan class=\"p\"\u003e\u0026lt;\u003c/span\u003e\u003cspan class=\"n\"\u003eNamedOnnxValue\u003c/span\u003e\u003cspan class=\"p\"\u003e\u0026gt;\u003c/span\u003e \u003cspan class=\"p\"\u003e{\u003c/span\u003e\n            \u003cspan class=\"n\"\u003eNamedOnnxValue\u003c/span\u003e\u003cspan class=\"p\"\u003e.\u003c/span\u003e\u003cspan class=\"nf\"\u003eCreateFromTensor\u003c/span\u003e \u003cspan class=\"p\"\u003e(\u003c/span\u003e\u003cspan class=\"n\"\u003einput_nodes_name\u003c/span\u003e\u003cspan class=\"p\"\u003e,\u003c/span\u003e \u003cspan class=\"n\"\u003einput_tensor\u003c/span\u003e\u003cspan class=\"p\"\u003e)\u003c/span\u003e\n        \u003cspan class=\"p\"\u003e};\u003c/span\u003e\n\n        \u003cspan class=\"c1\"\u003e// 推論を実行\u003c/span\u003e\n        \u003cspan class=\"kt\"\u003evar\u003c/span\u003e \u003cspan class=\"n\"\u003eresults\u003c/span\u003e \u003cspan class=\"p\"\u003e=\u003c/span\u003e \u003cspan class=\"n\"\u003esession\u003c/span\u003e\u003cspan class=\"p\"\u003e.\u003c/span\u003e\u003cspan class=\"nf\"\u003eRun\u003c/span\u003e \u003cspan class=\"p\"\u003e(\u003c/span\u003e\u003cspan class=\"n\"\u003einput_onnx_values\u003c/span\u003e\u003cspan class=\"p\"\u003e);\u003c/span\u003e\n        \u003cspan class=\"kt\"\u003evar\u003c/span\u003e \u003cspan class=\"n\"\u003escores\u003c/span\u003e \u003cspan class=\"p\"\u003e=\u003c/span\u003e \u003cspan class=\"n\"\u003eresults\u003c/span\u003e\u003cspan class=\"p\"\u003e.\u003c/span\u003e\u003cspan class=\"nf\"\u003eFirst\u003c/span\u003e\u003cspan class=\"p\"\u003e().\u003c/span\u003e\u003cspan class=\"n\"\u003eAsTensor\u003c/span\u003e\u003cspan class=\"p\"\u003e\u0026lt;\u003c/span\u003e\u003cspan class=\"kt\"\u003efloat\u003c/span\u003e\u003cspan class=\"p\"\u003e\u0026gt;().\u003c/span\u003e\u003cspan class=\"nf\"\u003eToArray\u003c/span\u003e \u003cspan class=\"p\"\u003e();\u003c/span\u003e\n\n        \u003cspan class=\"k\"\u003ereturn\u003c/span\u003e \u003cspan class=\"nf\"\u003eGetBestScorePos\u003c/span\u003e\u003cspan class=\"p\"\u003e(\u003c/span\u003e\u003cspan class=\"n\"\u003escores\u003c/span\u003e\u003cspan class=\"p\"\u003e);\u003c/span\u003e\n    \u003cspan class=\"p\"\u003e}\u003c/span\u003e\n\u003c/code\u003e\u003c/pre\u003e\u003c/div\u003e\u003c/div\u003e\n\n\u003cul\u003e\n\u003cli\u003e利点\n\n\u003cul\u003e\n\u003cli\u003eモデルの扱いが単純\u003c/li\u003e\n\u003cli\u003e前処理など数値の扱いが容易\u003c/li\u003e\n\u003c/ul\u003e\n\u003c/li\u003e\n\u003cli\u003e欠点\n\n\u003cul\u003e\n\u003cli\u003e依存関係のあるパッケージの特定に時間がかかる\u003c/li\u003e\n\u003cli\u003eモデルの読み込みが不安定(環境依存？)\u003c/li\u003e\n\u003c/ul\u003e\n\u003c/li\u003e\n\u003c/ul\u003e\n\n\u003cp\u003e総評：\u003cbr\u003e\n入力・モデルの扱い共に非常に簡単です。\u003cbr\u003e\nモデルの読み込みが最初の数秒不安定ですが、それ以降は特に問題なく実行できます。\u003c/p\u003e\n\n\u003cp\u003e参考：\u003cbr\u003e\nC#でONNXファイルを利用して手書き数字を認識する方法：\u003cbr\u003e\n　\u003ca href=\"https://kagasu.hatenablog.com/entry/2019/05/21/162445\" class=\"autolink\" rel=\"nofollow noopener\" target=\"_blank\"\u003ehttps://kagasu.hatenablog.com/entry/2019/05/21/162445\u003c/a\u003e\u003cbr\u003e\nonnx runtime 公式 github：\u003cbr\u003e\n　\u003ca href=\"https://github.com/microsoft/onnxruntime\" class=\"autolink\" rel=\"nofollow noopener\" target=\"_blank\"\u003ehttps://github.com/microsoft/onnxruntime\u003c/a\u003e\u003c/p\u003e\n\n\u003ch2\u003e\n\u003cspan id=\"--opencvsharp\" class=\"fragment\"\u003e\u003c/span\u003e\u003ca href=\"#--opencvsharp\"\u003e\u003ci class=\"fa fa-link\"\u003e\u003c/i\u003e\u003c/a\u003e- OpenCVSharp\u003c/h2\u003e\n\n\u003cp\u003e画像処理ライブラリとして有名ですが、Onnxを読み込む機能がサポートされているので使ってみました。\u003c/p\u003e\n\n\u003cdiv class=\"code-frame\" data-lang=\"C#\"\u003e\u003cdiv class=\"highlight\"\u003e\u003cpre class=\"with-code\"\u003e\u003ccode\u003e    \u003cspan class=\"k\"\u003eprivate\u003c/span\u003e \u003cspan class=\"n\"\u003eNet\u003c/span\u003e \u003cspan class=\"n\"\u003emodel\u003c/span\u003e\u003cspan class=\"p\"\u003e;\u003c/span\u003e\n    \u003cspan class=\"k\"\u003epublic\u003c/span\u003e \u003cspan class=\"k\"\u003evoid\u003c/span\u003e \u003cspan class=\"nf\"\u003eLoadModel\u003c/span\u003e \u003cspan class=\"p\"\u003e(\u003c/span\u003e\u003cspan class=\"kt\"\u003estring\u003c/span\u003e \u003cspan class=\"n\"\u003emodel_name\u003c/span\u003e\u003cspan class=\"p\"\u003e)\u003c/span\u003e \u003cspan class=\"p\"\u003e{\u003c/span\u003e\n        \u003cspan class=\"kt\"\u003estring\u003c/span\u003e \u003cspan class=\"n\"\u003emodel_path\u003c/span\u003e \u003cspan class=\"p\"\u003e=\u003c/span\u003e \u003cspan class=\"n\"\u003eApplication\u003c/span\u003e\u003cspan class=\"p\"\u003e.\u003c/span\u003e\u003cspan class=\"n\"\u003edataPath\u003c/span\u003e \u003cspan class=\"p\"\u003e+\u003c/span\u003e \u003cspan class=\"s\"\u003e@\"/MLModel/\"\u003c/span\u003e \u003cspan class=\"p\"\u003e+\u003c/span\u003e \u003cspan class=\"n\"\u003emodel_name\u003c/span\u003e \u003cspan class=\"p\"\u003e+\u003c/span\u003e \u003cspan class=\"s\"\u003e\".onnx\"\u003c/span\u003e\u003cspan class=\"p\"\u003e;\u003c/span\u003e\n        \u003cspan class=\"n\"\u003emodel\u003c/span\u003e \u003cspan class=\"p\"\u003e=\u003c/span\u003e \u003cspan class=\"n\"\u003eNet\u003c/span\u003e\u003cspan class=\"p\"\u003e.\u003c/span\u003e\u003cspan class=\"nf\"\u003eReadNetFromONNX\u003c/span\u003e \u003cspan class=\"p\"\u003e(\u003c/span\u003e\u003cspan class=\"n\"\u003emodel_path\u003c/span\u003e\u003cspan class=\"p\"\u003e);\u003c/span\u003e\n    \u003cspan class=\"p\"\u003e}\u003c/span\u003e\n    \u003cspan class=\"k\"\u003epublic\u003c/span\u003e \u003cspan class=\"kt\"\u003eint\u003c/span\u003e \u003cspan class=\"nf\"\u003eInference\u003c/span\u003e \u003cspan class=\"p\"\u003e(\u003c/span\u003e\u003cspan class=\"n\"\u003eTexture2D\u003c/span\u003e \u003cspan class=\"n\"\u003einput\u003c/span\u003e\u003cspan class=\"p\"\u003e)\u003c/span\u003e \u003cspan class=\"p\"\u003e{\u003c/span\u003e\n        \u003cspan class=\"c1\"\u003e// Texture2Dをモデルの入力に合った形に整形する\u003c/span\u003e\n        \u003cspan class=\"n\"\u003eMat\u003c/span\u003e \u003cspan class=\"n\"\u003eimg\u003c/span\u003e \u003cspan class=\"p\"\u003e=\u003c/span\u003e \u003cspan class=\"nf\"\u003eFixTexture2Input\u003c/span\u003e \u003cspan class=\"p\"\u003e(\u003c/span\u003e\u003cspan class=\"n\"\u003einput\u003c/span\u003e\u003cspan class=\"p\"\u003e);\u003c/span\u003e\n\n        \u003cspan class=\"c1\"\u003e// OpenCVSharpでの入力形式であるblobを作成、同時に正規化を行う。\u003c/span\u003e\n        \u003cspan class=\"n\"\u003eScalar\u003c/span\u003e \u003cspan class=\"n\"\u003emean\u003c/span\u003e \u003cspan class=\"p\"\u003e=\u003c/span\u003e \u003cspan class=\"k\"\u003enew\u003c/span\u003e \u003cspan class=\"nf\"\u003eScalar\u003c/span\u003e \u003cspan class=\"p\"\u003e(\u003c/span\u003e\u003cspan class=\"m\"\u003e0.485f\u003c/span\u003e\u003cspan class=\"p\"\u003e,\u003c/span\u003e \u003cspan class=\"m\"\u003e0.456f\u003c/span\u003e\u003cspan class=\"p\"\u003e,\u003c/span\u003e \u003cspan class=\"m\"\u003e0.406f\u003c/span\u003e\u003cspan class=\"p\"\u003e);\u003c/span\u003e\n        \u003cspan class=\"kt\"\u003evar\u003c/span\u003e \u003cspan class=\"n\"\u003eblob\u003c/span\u003e \u003cspan class=\"p\"\u003e=\u003c/span\u003e \u003cspan class=\"n\"\u003eCvDnn\u003c/span\u003e\u003cspan class=\"p\"\u003e.\u003c/span\u003e\u003cspan class=\"nf\"\u003eBlobFromImage\u003c/span\u003e \u003cspan class=\"p\"\u003e(\u003c/span\u003e\u003cspan class=\"n\"\u003eimg\u003c/span\u003e\u003cspan class=\"p\"\u003e,\u003c/span\u003e \u003cspan class=\"m\"\u003e1.0\u003c/span\u003e \u003cspan class=\"p\"\u003e/\u003c/span\u003e \u003cspan class=\"m\"\u003e255.0\u003c/span\u003e \u003cspan class=\"p\"\u003e/\u003c/span\u003e \u003cspan class=\"m\"\u003e0.225f\u003c/span\u003e\u003cspan class=\"p\"\u003e,\u003c/span\u003e \n                                        \u003cspan class=\"k\"\u003enew\u003c/span\u003e \u003cspan class=\"nf\"\u003eSize\u003c/span\u003e \u003cspan class=\"p\"\u003e(\u003c/span\u003e\u003cspan class=\"n\"\u003einput\u003c/span\u003e\u003cspan class=\"p\"\u003e.\u003c/span\u003e\u003cspan class=\"n\"\u003ewidth\u003c/span\u003e\u003cspan class=\"p\"\u003e,\u003c/span\u003e \u003cspan class=\"n\"\u003einput\u003c/span\u003e\u003cspan class=\"p\"\u003e.\u003c/span\u003e\u003cspan class=\"n\"\u003eheight\u003c/span\u003e\u003cspan class=\"p\"\u003e),\u003c/span\u003e \n                                        \u003cspan class=\"n\"\u003emean\u003c/span\u003e\u003cspan class=\"p\"\u003e,\u003c/span\u003e \u003cspan class=\"n\"\u003eswapRB\u003c/span\u003e \u003cspan class=\"p\"\u003e:\u003c/span\u003e \u003cspan class=\"k\"\u003etrue\u003c/span\u003e\u003cspan class=\"p\"\u003e,\u003c/span\u003e \u003cspan class=\"n\"\u003ecrop\u003c/span\u003e \u003cspan class=\"p\"\u003e:\u003c/span\u003e \u003cspan class=\"k\"\u003efalse\u003c/span\u003e\u003cspan class=\"p\"\u003e);\u003c/span\u003e\n        \u003cspan class=\"n\"\u003emodel\u003c/span\u003e\u003cspan class=\"p\"\u003e.\u003c/span\u003e\u003cspan class=\"nf\"\u003eSetInput\u003c/span\u003e \u003cspan class=\"p\"\u003e(\u003c/span\u003e\u003cspan class=\"n\"\u003eblob\u003c/span\u003e\u003cspan class=\"p\"\u003e);\u003c/span\u003e\n\n        \u003cspan class=\"c1\"\u003e// 推論を実行\u003c/span\u003e\n        \u003cspan class=\"n\"\u003eMat\u003c/span\u003e \u003cspan class=\"n\"\u003escores\u003c/span\u003e \u003cspan class=\"p\"\u003e=\u003c/span\u003e \u003cspan class=\"n\"\u003emodel\u003c/span\u003e\u003cspan class=\"p\"\u003e.\u003c/span\u003e\u003cspan class=\"nf\"\u003eForward\u003c/span\u003e \u003cspan class=\"p\"\u003e();\u003c/span\u003e\n\n        \u003cspan class=\"k\"\u003ereturn\u003c/span\u003e \u003cspan class=\"nf\"\u003eGetBestScorePos\u003c/span\u003e\u003cspan class=\"p\"\u003e(\u003c/span\u003e\u003cspan class=\"n\"\u003escores\u003c/span\u003e\u003cspan class=\"p\"\u003e);\u003c/span\u003e\n    \u003cspan class=\"p\"\u003e}\u003c/span\u003e\n\u003c/code\u003e\u003c/pre\u003e\u003c/div\u003e\u003c/div\u003e\n\n\u003cul\u003e\n\u003cli\u003e利点\n\n\u003cul\u003e\n\u003cli\u003eモデルの扱いが単純\u003c/li\u003e\n\u003cli\u003eNuget For Unityで簡単にプラグインを導入できる\u003c/li\u003e\n\u003c/ul\u003e\n\u003c/li\u003e\n\u003cli\u003e欠点\n\n\u003cul\u003e\n\u003cli\u003e画像をMat形式で扱うため、指定の前処理を行えないことがある\u003c/li\u003e\n\u003c/ul\u003e\n\u003c/li\u003e\n\u003c/ul\u003e\n\n\u003cp\u003eOpenCVSharpに関してはこちらの記事で今回の問題点を解決してくれてます。\u003cbr\u003e\n\u003ca href=\"https://qiita.com/utibenkei/items/adc7f526d629bdfc41fc\" id=\"reference-b3395fb394c8becba400\"\u003eUnityでディープラーニング推論をする方法（マルチプラットフォーム対応）\u003c/a\u003e\u003c/p\u003e\n\n\u003cp\u003e総評：\u003cbr\u003e\n読み込みが早く、動作も安定しています。\u003cbr\u003e\n画像関係のモデルを扱うならばこれだけで十分そうです。\u003c/p\u003e\n\n\u003cp\u003e参考：\u003cbr\u003e\n3Dの姿勢推定のOnnxのモデルでUnityちゃんを動かしてみた\u003cbr\u003e\n　\u003ca href=\"https://qiita.com/yukihiko_a/items/386e3a86a5e523757707\" class=\"autolink\" id=\"reference-9b9e9a1704d1888fc8a7\"\u003ehttps://qiita.com/yukihiko_a/items/386e3a86a5e523757707\u003c/a\u003e\u003c/p\u003e\n\n\u003ch1\u003e\n\u003cspan id=\"2-unityから直接tensorflowを扱う\" class=\"fragment\"\u003e\u003c/span\u003e\u003ca href=\"#2-unity%E3%81%8B%E3%82%89%E7%9B%B4%E6%8E%A5tensorflow%E3%82%92%E6%89%B1%E3%81%86\"\u003e\u003ci class=\"fa fa-link\"\u003e\u003c/i\u003e\u003c/a\u003e2. Unityから直接tensorflowを扱う\u003c/h1\u003e\n\n\u003ch2\u003e\n\u003cspan id=\"tensorflowsharp\" class=\"fragment\"\u003e\u003c/span\u003e\u003ca href=\"#tensorflowsharp\"\u003e\u003ci class=\"fa fa-link\"\u003e\u003c/i\u003e\u003c/a\u003eTensorFlowSharp\u003c/h2\u003e\n\n\u003cp\u003eC#から直接Tensorflowを利用するためのライブラリです。\u003cbr\u003e\nといっても学習済みモデルの利用が主な機能なので実質推論環境みたいなものです。\u003c/p\u003e\n\n\u003cdiv class=\"code-frame\" data-lang=\"C#\"\u003e\u003cdiv class=\"highlight\"\u003e\u003cpre class=\"with-code\"\u003e\u003ccode\u003e\n    \u003cspan class=\"k\"\u003eprivate\u003c/span\u003e \u003cspan class=\"n\"\u003eTFGraph\u003c/span\u003e \u003cspan class=\"n\"\u003emodel\u003c/span\u003e\u003cspan class=\"p\"\u003e;\u003c/span\u003e\n    \u003cspan class=\"k\"\u003epublic\u003c/span\u003e \u003cspan class=\"k\"\u003evoid\u003c/span\u003e \u003cspan class=\"nf\"\u003eLoadModel\u003c/span\u003e\u003cspan class=\"p\"\u003e(\u003c/span\u003e\u003cspan class=\"kt\"\u003estring\u003c/span\u003e \u003cspan class=\"n\"\u003emodel_name\u003c/span\u003e\u003cspan class=\"p\"\u003e)\u003c/span\u003e\n    \u003cspan class=\"p\"\u003e{\u003c/span\u003e\n        \u003cspan class=\"kt\"\u003estring\u003c/span\u003e \u003cspan class=\"n\"\u003emodel_path\u003c/span\u003e \u003cspan class=\"p\"\u003e=\u003c/span\u003e \u003cspan class=\"n\"\u003eApplication\u003c/span\u003e\u003cspan class=\"p\"\u003e.\u003c/span\u003e\u003cspan class=\"n\"\u003edataPath\u003c/span\u003e \u003cspan class=\"p\"\u003e+\u003c/span\u003e \u003cspan class=\"s\"\u003e@\"/MLModel/\"\u003c/span\u003e \u003cspan class=\"p\"\u003e+\u003c/span\u003e \u003cspan class=\"n\"\u003emodel_name\u003c/span\u003e \u003cspan class=\"p\"\u003e+\u003c/span\u003e \u003cspan class=\"s\"\u003e\".pb\"\u003c/span\u003e\u003cspan class=\"p\"\u003e;\u003c/span\u003e\n\n        \u003cspan class=\"kt\"\u003evar\u003c/span\u003e \u003cspan class=\"n\"\u003emodel_input\u003c/span\u003e \u003cspan class=\"p\"\u003e=\u003c/span\u003e \u003cspan class=\"n\"\u003eFile\u003c/span\u003e\u003cspan class=\"p\"\u003e.\u003c/span\u003e\u003cspan class=\"nf\"\u003eReadAllBytes\u003c/span\u003e\u003cspan class=\"p\"\u003e(\u003c/span\u003e\u003cspan class=\"n\"\u003emodel_path\u003c/span\u003e\u003cspan class=\"p\"\u003e);\u003c/span\u003e\n        \u003cspan class=\"n\"\u003emodel\u003c/span\u003e \u003cspan class=\"p\"\u003e=\u003c/span\u003e \u003cspan class=\"k\"\u003enew\u003c/span\u003e \u003cspan class=\"nf\"\u003eTFGraph\u003c/span\u003e\u003cspan class=\"p\"\u003e();\u003c/span\u003e\n        \u003cspan class=\"n\"\u003emodel\u003c/span\u003e\u003cspan class=\"p\"\u003e.\u003c/span\u003e\u003cspan class=\"nf\"\u003eImport\u003c/span\u003e\u003cspan class=\"p\"\u003e(\u003c/span\u003e\u003cspan class=\"n\"\u003emodel_input\u003c/span\u003e\u003cspan class=\"p\"\u003e);\u003c/span\u003e\n\n        \u003cspan class=\"n\"\u003eloaded\u003c/span\u003e \u003cspan class=\"p\"\u003e=\u003c/span\u003e \u003cspan class=\"k\"\u003etrue\u003c/span\u003e\u003cspan class=\"p\"\u003e;\u003c/span\u003e\n    \u003cspan class=\"p\"\u003e}\u003c/span\u003e\n    \u003cspan class=\"k\"\u003eprivate\u003c/span\u003e \u003cspan class=\"kt\"\u003ebool\u003c/span\u003e \u003cspan class=\"n\"\u003eloaded\u003c/span\u003e \u003cspan class=\"p\"\u003e=\u003c/span\u003e \u003cspan class=\"k\"\u003efalse\u003c/span\u003e\u003cspan class=\"p\"\u003e;\u003c/span\u003e\n    \u003cspan class=\"k\"\u003epublic\u003c/span\u003e \u003cspan class=\"kt\"\u003ebool\u003c/span\u003e \u003cspan class=\"nf\"\u003eGetLoaded\u003c/span\u003e\u003cspan class=\"p\"\u003e()\u003c/span\u003e \u003cspan class=\"p\"\u003e{\u003c/span\u003e \u003cspan class=\"k\"\u003ereturn\u003c/span\u003e \u003cspan class=\"n\"\u003eloaded\u003c/span\u003e\u003cspan class=\"p\"\u003e;\u003c/span\u003e \u003cspan class=\"p\"\u003e}\u003c/span\u003e\n\n    \u003cspan class=\"k\"\u003epublic\u003c/span\u003e \u003cspan class=\"kt\"\u003eint\u003c/span\u003e \u003cspan class=\"nf\"\u003eInference\u003c/span\u003e\u003cspan class=\"p\"\u003e(\u003c/span\u003e\u003cspan class=\"n\"\u003eTexture2D\u003c/span\u003e \u003cspan class=\"n\"\u003einput\u003c/span\u003e\u003cspan class=\"p\"\u003e)\u003c/span\u003e\n    \u003cspan class=\"p\"\u003e{\u003c/span\u003e\n        \u003cspan class=\"kt\"\u003evar\u003c/span\u003e \u003cspan class=\"n\"\u003esession\u003c/span\u003e \u003cspan class=\"p\"\u003e=\u003c/span\u003e \u003cspan class=\"k\"\u003enew\u003c/span\u003e \u003cspan class=\"nf\"\u003eTFSession\u003c/span\u003e\u003cspan class=\"p\"\u003e(\u003c/span\u003e\u003cspan class=\"n\"\u003emodel\u003c/span\u003e\u003cspan class=\"p\"\u003e);\u003c/span\u003e\n\n        \u003cspan class=\"c1\"\u003e// Texture2Dをモデルの入力に合った形に整形、正規化する。        \u003c/span\u003e\n        \u003cspan class=\"kt\"\u003evar\u003c/span\u003e \u003cspan class=\"n\"\u003efloat_values\u003c/span\u003e \u003cspan class=\"p\"\u003e=\u003c/span\u003e \u003cspan class=\"nf\"\u003eGetFloatFromTex2DWithFlip\u003c/span\u003e\u003cspan class=\"p\"\u003e(\u003c/span\u003e\u003cspan class=\"n\"\u003einput\u003c/span\u003e\u003cspan class=\"p\"\u003e);\u003c/span\u003e\n\n        \u003cspan class=\"c1\"\u003e// TensorFlowSharpでの入力形式であるTFTensorを作成する\u003c/span\u003e\n        \u003cspan class=\"kt\"\u003evar\u003c/span\u003e \u003cspan class=\"n\"\u003eshape\u003c/span\u003e \u003cspan class=\"p\"\u003e=\u003c/span\u003e \u003cspan class=\"k\"\u003enew\u003c/span\u003e \u003cspan class=\"nf\"\u003eTFShape\u003c/span\u003e\u003cspan class=\"p\"\u003e(\u003c/span\u003e\u003cspan class=\"m\"\u003e1\u003c/span\u003e\u003cspan class=\"p\"\u003e,\u003c/span\u003e \u003cspan class=\"n\"\u003einput\u003c/span\u003e\u003cspan class=\"p\"\u003e.\u003c/span\u003e\u003cspan class=\"n\"\u003ewidth\u003c/span\u003e\u003cspan class=\"p\"\u003e,\u003c/span\u003e \u003cspan class=\"n\"\u003einput\u003c/span\u003e\u003cspan class=\"p\"\u003e.\u003c/span\u003e\u003cspan class=\"n\"\u003eheight\u003c/span\u003e\u003cspan class=\"p\"\u003e,\u003c/span\u003e \u003cspan class=\"m\"\u003e3\u003c/span\u003e\u003cspan class=\"p\"\u003e);\u003c/span\u003e\n        \u003cspan class=\"kt\"\u003evar\u003c/span\u003e \u003cspan class=\"n\"\u003einput_tensor\u003c/span\u003e \u003cspan class=\"p\"\u003e=\u003c/span\u003e \u003cspan class=\"n\"\u003eTFTensor\u003c/span\u003e\u003cspan class=\"p\"\u003e.\u003c/span\u003e\u003cspan class=\"nf\"\u003eFromBuffer\u003c/span\u003e\u003cspan class=\"p\"\u003e(\u003c/span\u003e\u003cspan class=\"n\"\u003eshape\u003c/span\u003e\u003cspan class=\"p\"\u003e,\u003c/span\u003e \u003cspan class=\"n\"\u003efloat_values\u003c/span\u003e\u003cspan class=\"p\"\u003e,\u003c/span\u003e \u003cspan class=\"m\"\u003e0\u003c/span\u003e\u003cspan class=\"p\"\u003e,\u003c/span\u003e \u003cspan class=\"n\"\u003efloat_values\u003c/span\u003e\u003cspan class=\"p\"\u003e.\u003c/span\u003e\u003cspan class=\"n\"\u003eLength\u003c/span\u003e\u003cspan class=\"p\"\u003e);\u003c/span\u003e\n\n        \u003cspan class=\"c1\"\u003e//  データの入力・推論\u003c/span\u003e\n        \u003cspan class=\"c1\"\u003e//  input_2:0およびoutput_node0:0はpbファイル作成時につけた入力ノードと出力ノードの名前。\u003c/span\u003e\n        \u003cspan class=\"kt\"\u003evar\u003c/span\u003e \u003cspan class=\"n\"\u003erunner\u003c/span\u003e \u003cspan class=\"p\"\u003e=\u003c/span\u003e \u003cspan class=\"n\"\u003esession\u003c/span\u003e\u003cspan class=\"p\"\u003e.\u003c/span\u003e\u003cspan class=\"nf\"\u003eGetRunner\u003c/span\u003e\u003cspan class=\"p\"\u003e();\u003c/span\u003e\n        \u003cspan class=\"n\"\u003erunner\u003c/span\u003e\u003cspan class=\"p\"\u003e.\u003c/span\u003e\u003cspan class=\"nf\"\u003eAddInput\u003c/span\u003e\u003cspan class=\"p\"\u003e(\u003c/span\u003e\u003cspan class=\"n\"\u003emodel\u003c/span\u003e\u003cspan class=\"p\"\u003e[\u003c/span\u003e\u003cspan class=\"s\"\u003e\"input\"\u003c/span\u003e\u003cspan class=\"p\"\u003e][\u003c/span\u003e\u003cspan class=\"m\"\u003e0\u003c/span\u003e\u003cspan class=\"p\"\u003e],\u003c/span\u003e \u003cspan class=\"n\"\u003einput_tensor\u003c/span\u003e\u003cspan class=\"p\"\u003e);\u003c/span\u003e\n        \u003cspan class=\"n\"\u003erunner\u003c/span\u003e\u003cspan class=\"p\"\u003e.\u003c/span\u003e\u003cspan class=\"nf\"\u003eFetch\u003c/span\u003e\u003cspan class=\"p\"\u003e(\u003c/span\u003e\u003cspan class=\"n\"\u003emodel\u003c/span\u003e\u003cspan class=\"p\"\u003e[\u003c/span\u003e\u003cspan class=\"s\"\u003e\"content_vgg/prob\"\u003c/span\u003e\u003cspan class=\"p\"\u003e][\u003c/span\u003e\u003cspan class=\"m\"\u003e0\u003c/span\u003e\u003cspan class=\"p\"\u003e]);\u003c/span\u003e\n\n        \u003cspan class=\"c1\"\u003e// 推論を実行\u003c/span\u003e\n        \u003cspan class=\"kt\"\u003evar\u003c/span\u003e \u003cspan class=\"n\"\u003eoutput\u003c/span\u003e \u003cspan class=\"p\"\u003e=\u003c/span\u003e \u003cspan class=\"n\"\u003erunner\u003c/span\u003e\u003cspan class=\"p\"\u003e.\u003c/span\u003e\u003cspan class=\"nf\"\u003eRun\u003c/span\u003e\u003cspan class=\"p\"\u003e();\u003c/span\u003e\n        \u003cspan class=\"kt\"\u003evar\u003c/span\u003e \u003cspan class=\"n\"\u003escores\u003c/span\u003e \u003cspan class=\"p\"\u003e=\u003c/span\u003e \u003cspan class=\"p\"\u003e((\u003c/span\u003e\u003cspan class=\"kt\"\u003efloat\u003c/span\u003e\u003cspan class=\"p\"\u003e[][])\u003c/span\u003e\u003cspan class=\"n\"\u003eoutput\u003c/span\u003e\u003cspan class=\"p\"\u003e[\u003c/span\u003e\u003cspan class=\"m\"\u003e0\u003c/span\u003e\u003cspan class=\"p\"\u003e].\u003c/span\u003e\u003cspan class=\"nf\"\u003eGetValue\u003c/span\u003e\u003cspan class=\"p\"\u003e(\u003c/span\u003e\u003cspan class=\"k\"\u003etrue\u003c/span\u003e\u003cspan class=\"p\"\u003e))[\u003c/span\u003e\u003cspan class=\"m\"\u003e0\u003c/span\u003e\u003cspan class=\"p\"\u003e];\u003c/span\u003e\n\n        \u003cspan class=\"k\"\u003ereturn\u003c/span\u003e \u003cspan class=\"nf\"\u003eGetBestScorePos\u003c/span\u003e\u003cspan class=\"p\"\u003e(\u003c/span\u003e\u003cspan class=\"n\"\u003escores\u003c/span\u003e\u003cspan class=\"p\"\u003e);\u003c/span\u003e\n    \u003cspan class=\"p\"\u003e}\u003c/span\u003e\n\n\u003c/code\u003e\u003c/pre\u003e\u003c/div\u003e\u003c/div\u003e\n\n\u003cul\u003e\n\u003cli\u003e利点\n\n\u003cul\u003e\n\u003cli\u003eTensorFlowでサポートされているモデルを全て実行できる(onnxでは一部のパーツが非対応)\u003c/li\u003e\n\u003cli\u003e参考になるドキュメントが多い\u003c/li\u003e\n\u003cli\u003e前処理など数値の扱いが容易\u003c/li\u003e\n\u003c/ul\u003e\n\u003c/li\u003e\n\u003cli\u003e欠点\n\n\u003cul\u003e\n\u003cli\u003e推論が非常に遅い(OnnxRuntime 0.2[sec]に対し16.8[sec]) \u003c/li\u003e\n\u003cli\u003ePython・TensorFlowが使えることが前提となる\u003c/li\u003e\n\u003cli\u003eモデルの保存・読み込み方法が複雑\u003c/li\u003e\n\u003c/ul\u003e\n\u003c/li\u003e\n\u003c/ul\u003e\n\n\u003cp\u003e総評：\u003cbr\u003e\nバージョン変更によるAPIの変化やpbファイル(TensorFlowのモデル出力形式)の扱いづらさに加え、\u003cbr\u003e\n推論速度も今回試した手法の中で圧倒的に遅いです。\u003cbr\u003e\nUnityでディープラーニングをしたいだけなら他を選んだ方がよいと思います。\u003cbr\u003e\nTensorFlowを使い慣れていて、複雑なモデルを利用したい場合には使えるかもしれません。\u003c/p\u003e\n\n\u003cp\u003e参考：\u003cbr\u003e\nC#でTensorFlowのCNNを動かす。\u003cbr\u003e\n　\u003ca href=\"https://qiita.com/Tama_maru/items/6e50edfd8f8dea184d18\" class=\"autolink\" id=\"reference-5a3a25cc477978cf4e25\"\u003ehttps://qiita.com/Tama_maru/items/6e50edfd8f8dea184d18\u003c/a\u003e \u003cbr\u003e\nUnityで学習済みのVGG16による1000クラスの画像識別\u003cbr\u003e\n　\u003ca href=\"https://qiita.com/dipmizu/items/e46591bd4ab03b02ad38\" class=\"autolink\" id=\"reference-1382ef96fb9093121d29\"\u003ehttps://qiita.com/dipmizu/items/e46591bd4ab03b02ad38\u003c/a\u003e\u003c/p\u003e\n\n\u003ch1\u003e\n\u003cspan id=\"3-今回実施できなかった手法\" class=\"fragment\"\u003e\u003c/span\u003e\u003ca href=\"#3-%E4%BB%8A%E5%9B%9E%E5%AE%9F%E6%96%BD%E3%81%A7%E3%81%8D%E3%81%AA%E3%81%8B%E3%81%A3%E3%81%9F%E6%89%8B%E6%B3%95\"\u003e\u003ci class=\"fa fa-link\"\u003e\u003c/i\u003e\u003c/a\u003e3. 今回実施できなかった手法\u003c/h1\u003e\n\n\u003cp\u003e自分の能力不足や環境の問題で扱えなかった手法です。\u003c/p\u003e\n\n\u003ch2\u003e\n\u003cspan id=\"--windowsml\" class=\"fragment\"\u003e\u003c/span\u003e\u003ca href=\"#--windowsml\"\u003e\u003ci class=\"fa fa-link\"\u003e\u003c/i\u003e\u003c/a\u003e- WindowsML\u003c/h2\u003e\n\n\u003cp\u003eWindows 10に搭載されている機械学習のための実行環境です。\u003cbr\u003e\nOnnx形式をUWP(Universal Windows Platform)アプリで使うのに最適化されています。\u003c/p\u003e\n\n\u003cul\u003e\n\u003cli\u003e利点\n\n\u003cul\u003e\n\u003cli\u003eパッケージの導入を必要としない\u003c/li\u003e\n\u003cli\u003e参考となるドキュメントがやや多い\u003c/li\u003e\n\u003c/ul\u003e\n\u003c/li\u003e\n\u003cli\u003e欠点\n\n\u003cul\u003e\n\u003cli\u003eテストの度にビルドしなくてはならない\u003c/li\u003e\n\u003cli\u003e非同期処理が基本となっており、扱いにある程度知識が必要\u003c/li\u003e\n\u003c/ul\u003e\n\u003c/li\u003e\n\u003c/ul\u003e\n\n\u003cp\u003e総評：\u003cbr\u003e\n一応試してはみましたが、UWPアプリが良くわからなかったので一旦保留にしました。\u003cbr\u003e\nHoloLens界隈でよく使われているようです。\u003cbr\u003e\n非同期処理が分かるようになったらまた挑戦します。\u003c/p\u003e\n\n\u003cp\u003e参考：\u003cbr\u003e\n公式GitHub\u003cbr\u003e\n　\u003ca href=\"https://github.com/Microsoft/Windows-Machine-Learning/tree/master\" class=\"autolink\" rel=\"nofollow noopener\" target=\"_blank\"\u003ehttps://github.com/Microsoft/Windows-Machine-Learning/tree/master\u003c/a\u003e\u003cbr\u003e\nHoloLensでWindowsMLを試してみる(Unity利用編)\u003cbr\u003e\n　\u003ca href=\"http://akihiro-document.azurewebsites.net/post/hololens_windowsmachinelearningunity/\" class=\"autolink\" rel=\"nofollow noopener\" target=\"_blank\"\u003ehttp://akihiro-document.azurewebsites.net/post/hololens_windowsmachinelearningunity/\u003c/a\u003e\u003c/p\u003e\n\n\u003ch2\u003e\n\u003cspan id=\"--ml-net\" class=\"fragment\"\u003e\u003c/span\u003e\u003ca href=\"#--ml-net\"\u003e\u003ci class=\"fa fa-link\"\u003e\u003c/i\u003e\u003c/a\u003e- ML. Net\u003c/h2\u003e\n\n\u003cp\u003eC#上で機械学習を行うためのフレームワークです。\u003cbr\u003e\nモデル構築・訓練まで行えるので、Unityでの入力で直接学習を進めたりできそうです。\u003cbr\u003e\n今回はランタイムエラーがどうしても解決できなかったため未実施です。\u003cbr\u003e\nネット上のドキュメントも少ないのでしばらくは保留にすると思います。\u003c/p\u003e\n\n\u003cp\u003e参考：\u003cbr\u003e\n公式ドキュメント・チュートリアル\u003cbr\u003e\n　\u003ca href=\"https://docs.microsoft.com/ja-jp/dotnet/machine-learning/\" class=\"autolink\" rel=\"nofollow noopener\" target=\"_blank\"\u003ehttps://docs.microsoft.com/ja-jp/dotnet/machine-learning/\u003c/a\u003e\u003c/p\u003e\n\n\u003ch2\u003e\n\u003cspan id=\"--coreml\" class=\"fragment\"\u003e\u003c/span\u003e\u003ca href=\"#--coreml\"\u003e\u003ci class=\"fa fa-link\"\u003e\u003c/i\u003e\u003c/a\u003e- CoreML\u003c/h2\u003e\n\n\u003cp\u003eWindowsMLを紹介したのでこちらも一応。\u003cbr\u003e\niOS向けの推論環境で、こちらはOnnxを専用形式に変換してから利用します。\u003cbr\u003e\n今回はメイン環境がWindowsだったので未実施です。\u003cbr\u003e\n一応実施例が一つだけありましたが、中でswiftを使っているみたいなので\u003cbr\u003e\n素のUnityでCoreMLが使えるのかどうかは不明です。\u003cbr\u003e\n(少なくともNugetにはCoreMLはありませんでした)\u003c/p\u003e\n\n\u003cp\u003e参考：\u003cbr\u003e\nUnityVision-iOS\u003cbr\u003e\n　\u003ca href=\"https://github.com/possiblecee/UnityVision-iOS\" class=\"autolink\" rel=\"nofollow noopener\" target=\"_blank\"\u003ehttps://github.com/possiblecee/UnityVision-iOS\u003c/a\u003e\u003c/p\u003e\n\n\u003ch2\u003e\n\u003cspan id=\"--ironpython\" class=\"fragment\"\u003e\u003c/span\u003e\u003ca href=\"#--ironpython\"\u003e\u003ci class=\"fa fa-link\"\u003e\u003c/i\u003e\u003c/a\u003e- IronPython\u003c/h2\u003e\n\n\u003cp\u003eUnity上でpythonのコードを扱えるようになるライブラリです。\u003cbr\u003e\npython3.x系がサポートされておらず、自分の環境ではtensorflowが動かないので諦めました。\u003cbr\u003e\nmacやLinuxを使っていて、どうしてもPythonしか書きたくない場合には使えるかもしれません。\u003cbr\u003e\n(Unityでそんな状況が起こるのかは知りませんが)\u003c/p\u003e\n\n\u003cp\u003e参考：\u003cbr\u003e\n公式ページ\u003cbr\u003e\n　\u003ca href=\"https://ironpython.net\" class=\"autolink\" rel=\"nofollow noopener\" target=\"_blank\"\u003ehttps://ironpython.net\u003c/a\u003e\u003c/p\u003e\n\n\u003ch1\u003e\n\u003cspan id=\"4-おまけ\" class=\"fragment\"\u003e\u003c/span\u003e\u003ca href=\"#4-%E3%81%8A%E3%81%BE%E3%81%91\"\u003e\u003ci class=\"fa fa-link\"\u003e\u003c/i\u003e\u003c/a\u003e4. おまけ\u003c/h1\u003e\n\n\u003ch2\u003e\n\u003cspan id=\"--ml-agents-toolkit\" class=\"fragment\"\u003e\u003c/span\u003e\u003ca href=\"#--ml-agents-toolkit\"\u003e\u003ci class=\"fa fa-link\"\u003e\u003c/i\u003e\u003c/a\u003e- ML-Agents Toolkit\u003c/h2\u003e\n\n\u003cp\u003eUnity Technorogyが公式に提供している機械学習用アセットです。\u003cbr\u003e\n機械学習用アセットとは言うものの、サポートしている機能は強化学習だけなので\u003cbr\u003e\n画像や音声、文章を扱うモデルには適用できません。\u003cbr\u003e\n大変面白い機能ですが、今回の目的とは合わないのでおまけ扱いに。  \u003c/p\u003e\n\n\u003cp\u003e参考：\u003cbr\u003e\n公式ドキュメント\u003cbr\u003e\nML-Agents Installation\u003cbr\u003e\n　\u003ca href=\"https://github.com/Unity-Technologies/ml-agents/blob/master/docs/Installation.md\" class=\"autolink\" rel=\"nofollow noopener\" target=\"_blank\"\u003ehttps://github.com/Unity-Technologies/ml-agents/blob/master/docs/Installation.md\u003c/a\u003e\u003cbr\u003e\nBasic Guide\u003cbr\u003e\n　\u003ca href=\"https://github.com/Unity-Technologies/ml-agents/blob/master/docs/Basic-Guide.md\" class=\"autolink\" rel=\"nofollow noopener\" target=\"_blank\"\u003ehttps://github.com/Unity-Technologies/ml-agents/blob/master/docs/Basic-Guide.md\u003c/a\u003e\u003c/p\u003e\n\n\u003ch2\u003e\n\u003cspan id=\"--the-unity-inference-enginebarracuda\" class=\"fragment\"\u003e\u003c/span\u003e\u003ca href=\"#--the-unity-inference-enginebarracuda\"\u003e\u003ci class=\"fa fa-link\"\u003e\u003c/i\u003e\u003c/a\u003e- The Unity Inference Engine(Barracuda)\u003c/h2\u003e\n\n\u003cp\u003eml-agents v0.7から導入されたUnity Labs謹製推論エンジンです。\u003cbr\u003e\nUnityに対応する全てのプラットフォームで推論できる事を目的に開発されているようですが、\u003cbr\u003e\n現段階ではml-agentsを経由しないと利用できない様子です。 \u003cbr\u003e\n(多分そんなことはないのだけれど、コードを読んでもよくわかりませんでした)\u003cbr\u003e\n将来的には独立したUnityパッケージとして\u003cbr\u003e\n他の深層学習アプリケーションにも対応する予定らしいので、\u003cbr\u003e\nリリースされたらまた調べてみようと思います。\u003c/p\u003e\n\n\u003cp\u003e参考：\u003cbr\u003e\n公式ブログの該当記事\u003cbr\u003e\n　\u003ca href=\"https://blogs.unity3d.com/jp/2019/03/01/unity-ml-agents-toolkit-v0-7-a-leap-towards-cross-platform-inference/\" class=\"autolink\" rel=\"nofollow noopener\" target=\"_blank\"\u003ehttps://blogs.unity3d.com/jp/2019/03/01/unity-ml-agents-toolkit-v0-7-a-leap-towards-cross-platform-inference/\u003c/a\u003e\u003c/p\u003e\n\n\u003ch1\u003e\n\u003cspan id=\"おわりに\" class=\"fragment\"\u003e\u003c/span\u003e\u003ca href=\"#%E3%81%8A%E3%82%8F%E3%82%8A%E3%81%AB\"\u003e\u003ci class=\"fa fa-link\"\u003e\u003c/i\u003e\u003c/a\u003eおわりに\u003c/h1\u003e\n\n\u003cp\u003eあまり時間をかけすぎるのも効率が悪いので一旦打ち切りましたが、\u003cbr\u003e\nWindowsMLやCoreMLあたりはベースになる言語を勉強した上で再挑戦したいと思います。\u003c/p\u003e\n\n\u003ch1\u003e\n\u003cspan id=\"今回の内容githubにあげました\" class=\"fragment\"\u003e\u003c/span\u003e\u003ca href=\"#%E4%BB%8A%E5%9B%9E%E3%81%AE%E5%86%85%E5%AE%B9github%E3%81%AB%E3%81%82%E3%81%92%E3%81%BE%E3%81%97%E3%81%9F\"\u003e\u003ci class=\"fa fa-link\"\u003e\u003c/i\u003e\u003c/a\u003e今回の内容(GitHubにあげました)\u003c/h1\u003e\n\n\u003cp\u003eプロジェクトファイル\u003cbr\u003e\n　\u003ca href=\"https://github.com/highnoRQ/Unity_ML_Test\" class=\"autolink\" rel=\"nofollow noopener\" target=\"_blank\"\u003ehttps://github.com/highnoRQ/Unity_ML_Test\u003c/a\u003e\u003c/p\u003e\n\n\u003cp\u003eAssets/MLModel下に以下のモデルファイルを置いてください\u003cbr\u003e\nVGG19　tensorflow pbファイル\u003cbr\u003e\n　\u003ca href=\"https://drive.google.com/file/d/10XFhmr43zIZD4dPKP55hUg9YxKUZ3cBV/view?usp=sharing\" class=\"autolink\" rel=\"nofollow noopener\" target=\"_blank\"\u003ehttps://drive.google.com/file/d/10XFhmr43zIZD4dPKP55hUg9YxKUZ3cBV/view?usp=sharing\u003c/a\u003e\u003cbr\u003e\nVGG19 onnxファイル\u003cbr\u003e\n　\u003ca href=\"https://github.com/onnx/models/tree/master/vision/classification/vgg\" class=\"autolink\" rel=\"nofollow noopener\" target=\"_blank\"\u003ehttps://github.com/onnx/models/tree/master/vision/classification/vgg\u003c/a\u003e\u003c/p\u003e\n\n\u003cp\u003eまた、ビルドしてテストする場合。\u003cbr\u003e\nビルド済みフォルダのDataフォルダにMLModelフォルダごと移してください。\u003c/p\u003e\n","createdAt":"2019-08-08T23:19:35Z","elapsedYearsFromLastModifiedAt":1,"encryptedId":"XmhRDKEk3xxKtt0cH58vNd+klpCrnY5+--BjHgNwJ4VEGyWYhP--pkvlpbdVKDFsR0s9gR9tig==","isBanned":false,"isDeprecated":true,"isDestroyableByViewer":false,"isEditRequestReadableByViewer":true,"isEditRequestSendableByViewer":true,"isLikableByViewer":true,"isLikedByViewer":false,"isPublic":true,"isSlide":false,"isStockableByViewer":true,"isStockedByViewer":false,"isSubscribableByViewer":false,"isSubscribedByViewer":false,"isUpdatableByViewer":false,"isUpdated":true,"lastModifiedAt":"2020-07-02T12:29:40Z","likesCount":102,"linkUrl":"https://qiita.com/highno_RQ/items/7a33e7eaa6d77c93968f","organization":null,"originalId":996432,"stockedCount":94,"title":"Unityでディープラーニングする方法調べた","toc":"\u003cul\u003e\n\u003cli\u003e\n\u003ca href=\"#202072%E8%BF%BD%E8%A8%98\"\u003e2020/7/2追記\u003c/a\u003e\n\u003c/li\u003e\n\u003cli\u003e\n\u003ca href=\"#2020512%E8%BF%BD%E8%A8%98\"\u003e2020/5/12追記\u003c/a\u003e\n\u003c/li\u003e\n\u003cli\u003e\n\u003ca href=\"#%E6%A6%82%E8%A6%81\"\u003e概要\u003c/a\u003e\n\u003c/li\u003e\n\u003cli\u003e\n\u003ca href=\"#%E5%AE%9F%E6%96%BD%E5%86%85%E5%AE%B9\"\u003e実施内容\u003c/a\u003e\n\u003cul\u003e\n\u003cli\u003e\n\u003ca href=\"#%E3%83%A2%E3%83%87%E3%83%AB%E3%81%AE%E5%BD%A2%E5%BC%8F\"\u003eモデルの形式\u003c/a\u003e\n\u003c/li\u003e\n\u003cli\u003e\n\u003ca href=\"#%E5%89%8D%E5%87%A6%E7%90%86\"\u003e前処理\u003c/a\u003e\n\u003c/li\u003e\n\u003c/ul\u003e\n\u003c/li\u003e\n\u003cli\u003e\n\u003ca href=\"#%E7%92%B0%E5%A2%83\"\u003e環境\u003c/a\u003e\n\u003c/li\u003e\n\u003cli\u003e\n\u003ca href=\"#%E6%89%8B%E6%B3%95%E4%B8%80%E8%A6%A7\"\u003e手法一覧\u003c/a\u003e\n\u003c/li\u003e\n\u003cli\u003e\n\u003ca href=\"#1-onnx%E5%BD%A2%E5%BC%8F%E3%81%A7%E5%AD%A6%E7%BF%92%E6%B8%88%E3%81%BF%E3%83%A2%E3%83%87%E3%83%AB%E3%82%92%E6%89%B1%E3%81%86\"\u003e1. Onnx形式で学習済みモデルを扱う\u003c/a\u003e\n\u003cul\u003e\n\u003cli\u003e\n\u003ca href=\"#%E6%A6%82%E8%A6%81-1\"\u003e概要\u003c/a\u003e\n\u003c/li\u003e\n\u003cli\u003e\n\u003ca href=\"#--onnx-runtime\"\u003e- Onnx Runtime\u003c/a\u003e\n\u003c/li\u003e\n\u003cli\u003e\n\u003ca href=\"#--opencvsharp\"\u003e- OpenCVSharp\u003c/a\u003e\n\u003c/li\u003e\n\u003c/ul\u003e\n\u003c/li\u003e\n\u003cli\u003e\n\u003ca href=\"#2-unity%E3%81%8B%E3%82%89%E7%9B%B4%E6%8E%A5tensorflow%E3%82%92%E6%89%B1%E3%81%86\"\u003e2. Unityから直接tensorflowを扱う\u003c/a\u003e\n\u003cul\u003e\n\u003cli\u003e\n\u003ca href=\"#tensorflowsharp\"\u003eTensorFlowSharp\u003c/a\u003e\n\u003c/li\u003e\n\u003c/ul\u003e\n\u003c/li\u003e\n\u003cli\u003e\n\u003ca href=\"#3-%E4%BB%8A%E5%9B%9E%E5%AE%9F%E6%96%BD%E3%81%A7%E3%81%8D%E3%81%AA%E3%81%8B%E3%81%A3%E3%81%9F%E6%89%8B%E6%B3%95\"\u003e3. 今回実施できなかった手法\u003c/a\u003e\n\u003cul\u003e\n\u003cli\u003e\n\u003ca href=\"#--windowsml\"\u003e- WindowsML\u003c/a\u003e\n\u003c/li\u003e\n\u003cli\u003e\n\u003ca href=\"#--ml-net\"\u003e- ML. Net\u003c/a\u003e\n\u003c/li\u003e\n\u003cli\u003e\n\u003ca href=\"#--coreml\"\u003e- CoreML\u003c/a\u003e\n\u003c/li\u003e\n\u003cli\u003e\n\u003ca href=\"#--ironpython\"\u003e- IronPython\u003c/a\u003e\n\u003c/li\u003e\n\u003c/ul\u003e\n\u003c/li\u003e\n\u003cli\u003e\n\u003ca href=\"#4-%E3%81%8A%E3%81%BE%E3%81%91\"\u003e4. おまけ\u003c/a\u003e\n\u003cul\u003e\n\u003cli\u003e\n\u003ca href=\"#--ml-agents-toolkit\"\u003e- ML-Agents Toolkit\u003c/a\u003e\n\u003c/li\u003e\n\u003cli\u003e\n\u003ca href=\"#--the-unity-inference-enginebarracuda\"\u003e- The Unity Inference Engine(Barracuda)\u003c/a\u003e\n\u003c/li\u003e\n\u003c/ul\u003e\n\u003c/li\u003e\n\u003cli\u003e\n\u003ca href=\"#%E3%81%8A%E3%82%8F%E3%82%8A%E3%81%AB\"\u003eおわりに\u003c/a\u003e\n\u003c/li\u003e\n\u003cli\u003e\n\u003ca href=\"#%E4%BB%8A%E5%9B%9E%E3%81%AE%E5%86%85%E5%AE%B9github%E3%81%AB%E3%81%82%E3%81%92%E3%81%BE%E3%81%97%E3%81%9F\"\u003e今回の内容(GitHubにあげました)\u003c/a\u003e\n\u003c/li\u003e\n\u003c/ul\u003e\n","totalPv":18347,"uuid":"7a33e7eaa6d77c93968f","banReason":null,"adventCalendarItem":null,"author":{"encryptedId":"iOnYQMs5l5tcN1Ilk8V8oFqIJ1ec--fLz2TTveOhw7zEmV--7OQgLaja2+BA313k2979tg==","originalId":440309,"description":"1の方です。\r\nこのサイトの掲載内容は私個人の見解であり、\r\n必ずしも私が所属する会社、組織、団体の立場、戦略、意見を代表するものではありません。","facebookUrl":null,"githubUrl":"https://github.com/highnoRQ","isBlockingViewer":false,"isFollowableByViewer":true,"isFollowedByViewer":false,"isTweetWebNotificationReceivable":true,"linkedinUrl":null,"name":"Yukihiro  Azuma","profileImageUrl":"https://qiita-image-store.s3.ap-northeast-1.amazonaws.com/0/440309/profile-images/1565306650","profileImageUrlW48":"https://qiita-user-profile-images.imgix.net/https%3A%2F%2Fqiita-image-store.s3.ap-northeast-1.amazonaws.com%2F0%2F440309%2Fprofile-images%2F1565306650?ixlib=rb-4.0.0\u0026auto=compress%2Cformat\u0026lossless=0\u0026w=48\u0026s=5ec37de6fa8124a59023010a9820479b","profileImageUrlW75":"https://qiita-user-profile-images.imgix.net/https%3A%2F%2Fqiita-image-store.s3.ap-northeast-1.amazonaws.com%2F0%2F440309%2Fprofile-images%2F1565306650?ixlib=rb-4.0.0\u0026auto=compress%2Cformat\u0026lossless=0\u0026w=75\u0026s=fa0d9a94a3e3518975d82d7562d24329","urlName":"highno_RQ","websiteUrl":"","twitterUrl":"https://twitter.com/highno_RQ","twitterUrlName":"highno_RQ","revealedOrganizations":{"edges":[]}},"tags":[{"name":"C#","urlName":"csharp"},{"name":"Unity","urlName":"unity"},{"name":"DeepLearning","urlName":"deeplearning"},{"name":"TensorFlow","urlName":"tensorflow"},{"name":"ONNX","urlName":"onnx"}],"followingLikers":{"edges":[]},"comments":{"totalCount":0}},"comments":[],"client":null,"ads_event_emitter":null}}</script>
