音声波形の解析はせず口パク用データを作成し、それを再生する方法 になります。
ですので、マイクの音を拾ってリアルタイムで口パクさせたい方とかが使える内容ではありません。"lipsync" とは何かについては、他に良い記事を書かれている方が沢山いらっしゃいますので割愛します。"lipsync" を Google先生で検索すると、音声波形を解析して口パクさせている方法が多くヒットするのですが（そして、普通は当然それを使うと思います。）、自身が開発している「ACUAH β」というアプリでは、音声波形の解析はせず、こんなアナログな方法でやってますという話になります。（逆に、音声波形を解析しない口パクはどうやって実装されているのか、まったく分かりません...アニメーションにキーを打っている？）また、アプリ開発経験なしの素人プログラマの記事です。口パク品質もそれなりですので諸々ご容赦ください。(2021/7/16時点）でしょうか。どれも凄いライブラリです。主に以下の理由でした。「ACUAH β」　は決まった音声データしか使わないので、事前に口パクデータが用意できていればOK。悩んで、以下の方法にする事にしました。　「ACUAH β」の音声データは短い文しかないので、この方法で1音声ファイルについてだいたい5分～10分でデータが作れます。  ただそれでも数が多いのと波形データをにらめっこするので精神的にきつい作業ではあります。　できたデータの再生はこんな感じにやります。（すみません、このままコピペでは動きません。）英語の音声データに対するリップシンクは更に悩みました。
日本語と同じ処理方式でやるためにはという事で、発音記号を使う というアイデアになりました。　日本語と違って発音記号毎の時間を取得するのは流石に作業大変すぎる...
　どうしようと悩んだところで、かなり大雑把なやり方を思いつきました。　ここまでできたら、Adobe Auditionで 音声データの波形から hello の発音時間を調べて・・・

　やっぱり面倒。　そこで、更にUnity上で音声データを聴きながら、単語（音節）に合わせてリズムを取る方法を思いつきました。
　これならデータを作る人が音を聞き取るタイミングはほぼ一定と思われるし、それほどタイミングずれないはず。　例えば、"What can I do for you?" という音声データなら、リズム良く、"What" "can" "I" "do" "for" "you" と聞こえてくるタイミングで 画面上のボタン を押して　ボタンの押された間隔（秒）をTime.time で取得すれば、各単語の発音時間とおよそ同じになるはず。
　とは言え、再生速度は落とさないと無理なので、ピッチ（再生速度）をスライダーで調整できるようにしました。
　また、1音節しかない場合には、音声データの再生時間と同じになります。　実際の発音時間と、取得した発音時間にズレは生じるものの違和感を感じるほどではありませんでした。
　この方法だと、音声を1、2回聴くだけでデータを作成できるので 1データ数分で作成できます。音節に合わせてボタンポチポチするだけでリップシンクデータを作ってくれるようにしたら割と有能。（音声データはAh-ya（@aya_voicer）様）#ACUAH β pic.twitter.com/H1DOL0KibF口パクデータを事前に用意しておける程々実装でよければ。


