macOS, Windows両方で動く、Unityで対話型の音声認識アプリがすぐに作れるフレームワークを作りました。リポジトリへのリンク：https://github.com/HassoPlattnerInstituteHCI/SpeechIOForUnity
Unity Package: https://github.com/HassoPlattnerInstituteHCI/SpeechIOForUnity/releases/tag/v1.0SpeechOut/SpeechInを宣言し、以下のように書くだけ。macOS, Windowsを使う学生のために、Unityを扱う授業で音声認識を使った対話型アプリを作るためのフレームワークがあればいいなと思ったので作りました。
主に非同期処理のasync/awaitベースで、逐次処理をわかりやすく書けるように、また速度や”間”などのチューニングも簡単にできるようにしました。授業ではmac, Windowsそれぞれを使う学生を同様に対応しないといけないのですが、Unityそのものは双方で動作する一方、ネイティブOSの機能を使おうとすると相互にコードを共有できない＝他のチームやグループが作ったアプリを動作させられないことになるので、このようなコードを作りました。授業のためとはいえ、一般的に使いやすいツールだと思います。macOSネイティブのオフライン音声認識ツールとしては、NSSpeechRecognizerが、音声合成ツールとしては、コマンドラインツールとしてsayがあります。このうち、sayに関してはUnity上のSystem.Diagnostics.Processから呼び出すことで実行できます（この点、Argsをいじるだけで声質とか速度を変えられるのでサイコー）一方のNSSpeechRecognizerはOSネイティブAPIとして提供されている機能なので、実行するためにはObjective-CもしくはSwiftで記述されたコードから実行する必要があります。今回のフレームワークでは別途NSSpeechForUnityとして、NSSpeechRecognizerを呼び出すObjective-Cコードを書き、外部ライブラリとして.bundleファイルを書き出し、それをUnityのPluginとすることで実行しています。逆にWindows側ではUnityEngine.Windows.Speechなるモジュールがあり、音声認識に関してはUnityから直接実行できる一方、音声合成に関しては別途WindowsのネイティブAPI＝SAPIを叩く必要がありました。コード内で別途WindowsVoiceProjectとして、Visual Studioから.dllをビルドできるプロジェクトを作り、その.dllをUnityのPluginにします。以上２つのOSに関して独自にライブラリを書き出すことによって実現しましたが、Unity上で非同期の動作を実現させるため、それぞれの処理がきちんと終わったかどうかを常に監視するためのコードを書く必要があります。たとえばこのように、内部のStateを逐一変えて、Unity側から監視することで、UnityのAsync/Awaitが進行するように若干HardCodedな感じではありますが、リアルタイムアプリケーションのための非同期処理を実現しています。対話型アプリケーションのためとは言いましたが、VRアプリケーションなどのための目と手が離せない際のデバッグ（イベントが起こったときにどのフラグが立ったか喋って教えてくれる）や、視覚障害者向けアプリなど、様々な用途が考えられます。「デバッグのときになにか喋ってくれたら便利だな」とか、「アクセシビリティ機能の充実したアプリを作りたいな」などというときにはぜひ使ってみてください。


